[{"authors":["admin"],"categories":null,"content":"Kyle Jones is a Principal Solutions Architect for Amazon Web Services (AWS) specializing in the Energy Sector. Prior to joining AWS, Kyle was the lead architect for ExxonMobil\u0026rsquo;s upstream data science and analytics platforms. He taught weekly classes at ExxonMobil on Python, R, and data science.\nOutside of AWS, Kyle is a lecturer in business analytics at Sam Houston State University. He is based in Houston, Texas.\n","date":1587340800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1587081600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/kyle-jones/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kyle-jones/","section":"authors","summary":"Kyle Jones is a Principal Solutions Architect for Amazon Web Services (AWS) specializing in the Energy Sector. Prior to joining AWS, Kyle was the lead architect for ExxonMobil\u0026rsquo;s upstream data science and analytics platforms.","tags":null,"title":"Kyle Jones","type":"authors"},{"authors":null,"categories":null,"content":"BANA 3363 ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"d7d711bf4aaa16e572b074d12685341a","permalink":"/courses/bana3363/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/bana3363/","section":"courses","summary":"This course is a continuation of BANA 2372 and is designed to introduce the use of statistics as a business tool in the face of incomplete knowledge. Students will learn estimation, hypothesis testing, analysis of variance, goodness-of-fit measures, correlation, and regression.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"What is statistics? Statistics is the science of collecting, organizing, analyzing, and interpreting data in order to make decisions.\nStatistics is all around us. We hear statsitics on the news and we use statistics to make buisnes decisions.\nThere are five general things you can do with statistics:1\n Describe ‚Äî characterizing populations and samples using descriptive statistics, statistical intervals, correlation coefficients, graphics, and maps. Compare or Test ‚Äî detecting differences between statistical populations or reference values using simple hypothesis tests, and analysis of variance and covariance. Identify or Classify ‚Äî classifying and identifying a known or hypothesized entity or group of entities using descriptive statistics; statistical intervals and tests, graphics, and multivariate techniques such as cluster analysis. Predict ‚Äî predicting measurements using regression and neural networks, forecasting using time-series modeling techniques, and interpolating spatial data. Explain ‚Äî Explaining latent aspects of phenomena using regression, cluster analysis, discriminant analysis, factor analysis, and other data mining techniques.  How do we do this? This table shows some statitical methods that can be used to accomplish different goals.\n   Objective Common Tools Example Applications     Describe graphs; Descriptive Statistics Opinion Surveys; Demographic Surveys   Compare or Test Statistical Tests Program Evaluation; Pharma/Education Effectiveness   Identify or Classify Classification Trees; Data Mining Customer Churn   Predict Regression Credit Score; Cost for House   Explain Regression; ANOVA Research; Root cause analysis    Why do we need statistics? The world is full of uncertainy. Variability arises from two primary sources: variability in the natural process (inherent variability) and variability due to our imperfect measurement devices (measurement variability).\n Summary Statistics  Once we have data set (sample), want to determine basic summary statistics Quantitative summaries of data, give rough contours Useful for giving you a feel for what data actually look like Common summary statistics: mean, median, mode, quartiles, max, min Note: n = number of observations in our sample  Variance and Standard Deviation  Standard Deviation (like variance but better) However, variance is a squared term Difficult to understand ‚Äì squared ages? Squared income? (Difficult substantive meaning) Standard deviation: Square root of variance Another measure of spread Reverts back to original scale  Summary  Samples are data sampled from an underlying population Use summary statistics to get a sense of the data Common examples: mean, median, variance, standard deviation Describe different properties, useful in different contexts  Measures of Central Tendency Sample mean The sample mean (also called the arithmetic mean or sample average), is found by adding up all observations and dividing by the total number of observations.\nThe symbol $\\bar{x}$ is read as ‚Äúx-bar‚Äù\n We don‚Äôt always have to use the letter X. We can use any letter we want to indicate a variable (ùëã, ùëå, ùëä, ùëâ, etc.). Whatever we call the variable, a letter with a bar over it indicates the sample mean. $\\bar{x}$ is a statistic, specifically, a point estimate for the population mean ‚Äúmu‚Äù: $\\mu$  Some key terms   Mean: The Average - The mean of a data set is the sum of the data entries divided by the number of entries.\n  Median: The Middle - The median of a data set is the value that lies in the middle of the data when the data set is ordered from least to greatest. How do you find it? 1. Order the data entries. 2. Find the middle data entry. 3. Interpret the results in the context of the data.\n  Mode: The Most - The mode of the data set is the data entry that occurs with the greatest frequency. A data set can have one mode, more than one mode, or no mode. How do you find it? 1. Write the data in order. 2. Identify the entry, or entries, that occur with the greatest frequency. 3. Interpret the results in the context of the data.\n  Range: Smallest-Largest\n  Outlier: A data entry that is far removed from the other entries in the data set.\n  Descriptive Statistics: The branch of statistics that involves the organization, summarization, and display of data. Example: \u0026ldquo;For unmarried men, approximately 70% were alive at age 65\u0026rdquo; and \u0026ldquo;For married men, 90% were alive at 65.\u0026rdquo;\n  Inferential Statistics: The branch of statistics that involves using a sample to draw conclusions about a population. A basic tool in the study of inferential statistics is probability. Inferential statistics is a collection of techniques designed to use sample data to make generally-applicable statements about the natural process or population from which the data arose. Example: Possible Inference: Being married is associated with a longer life for men.\n  Probability: We typically aren‚Äôt interested in just the sample that we have (although the sample is still interesting).\n  Example: An insurance company sends out a survey to 100 policy holders asking them to rate their satisfaction with the service.\n Managers aren‚Äôt just interested in only those 100 people. They want to be able to say something about the process (or population of customers) that is behind those customer satisfaction ratings That is, they want to infer (make a conclusion about) the process, so they can (perhaps) make adjustments to their website, coverage, customer service, etc. Probability helps us make the leap from a sample to population (or process) in an objective, mathematically justified way       Population Mean Sample Mean     $\\mu = \\frac{\\sum x}{N}$ $\\bar{x} = \\frac{\\sum x}{n}$   $N$ is the number of items in the population $n$ is the number of items in the sample (and the sample is drawn from the population)   It has higher accuracy It has lower accuracy   The population is very large and not known, the population mean is an unknown constant It is an efficient and unbiased estimator of the population mean. The expected value of the sample mean is the population mean.    Measures of Central Tendency Estimators and Estimates: a mathematical function of the sample that tell us how to calculate an estimate of a parameter form a sample. Smaller the variance, most efficient the estimator. Hence, we require to find what are the \u0026ldquo;good\u0026rdquo; estimators.\nFew vital criterial for goodness of an estimator are based on these properties:\nEstimator, Parameter, and Excel Command    Term Estimator Parameter Excel Command     Mean $\\bar{x}$ $\\mu$ =AVERAGE(A1:A10)   Variance $s^2$ $\\sigma^2$ =STDEV(A1:A:10)   Median   =Median(A1:A:10)   Mode   =Mode(A1:A:10)   Correlation R $\\rho$     Sample Mean and Standard Deviation Given a sample of $n$ numerical data values $x_{1}, x_{2},\\ldots ,x_{n},$ the general first step is to calculate summary measures (or **statistics)** to get a feel for the population or process from which the data arises. Many summary measures exist, but the two most important are the sample mean and the sample standard deviation, which we now review.\nThe sample mean of a sample of $n$ data values $x_{1},x_{2},\u0026hellip;,x_{n}$ is a measure of the center of a set of data and is defined as\n$$\\overline{x}=\\frac{\\sum x_{i}}{n}$$\nwhere $\\overline{x}$ is read as \u0026ldquo;x bar.\u0026rdquo;\nThe sample standard deviation of a sample of $n$ data values $x_{1},x_{2},\u0026hellip;,x_{n}$ is a measure of variability in the data and is defined as\n$$s=\\sqrt{\\frac{\\sum (x_{i}-\\overline{x})^{2}}{n-1}}$$\nA shortcut formula is\n$$s=\\sqrt{\\frac{\\sum x_{i}^{2}-n\\overline{x}^{2}}{n-1}}$$\nThe sample variance of a sample of $n$ data points $x_{1},x_{2},\u0026hellip;,x_{n}$ is the square of the standard deviation $s$. That is\n$$\\text{Sample Variance }=\\text{ }s^{2}$$\n Researchers working for a hospitality trade publication surveyed a random sample of $10$ restaurant servers, asking them to report their best guess at their hourly wage (including tips). The data are given below.     Tips     $13$   $9.29$   $11.88$   $10.04$   $12.15$   $6.77$   $8.86$   $11.49$   $11.39$   $10.41$    a). Calculate the sample mean hourly wage for the 10 restaurant servers.\nb). Calculate the sample standard deviation of the hourly wages.\n A researcher is interested in the number of years that individuals employed in the financial services industry spend with their current employer. A sample of $11$ individuals ages $25-30$ who worked in the industry was collected. The data are shown below.     Years     $6.6$   $6.2$   $3.9$   $6.5$   $6.8$   $5.2$   $5.4$   $5.1$   $4.7$   $8.4$   $4.4$    ¬†a). Calculate the sample mean number of years spent with the current employer.\nb). Calculate the sample standard deviation of years spent with the current employer.\nNumerical Graphical Techniques Another early step in the analysis of data is the construction of graphical summaries, which represent the data using a picture designed to highlight key attributes. One of the most important tools for analyzing numerical data is a histogram. A histogram is a type of bar chart that divides the total range of the data into a number of \u0026ldquo;bins\u0026rdquo; of equal width and then sorts the data into the bins based upon those ranges. It answers the questions about 1). center (Where do the numbers tend to concentrate?), 2). spread (How variable is the data?) and 3). shape (In what pattern do the data tend to fall?)\nFigure [calls]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;calls\u0026rdquo;} below is an example of a histogram made from a set of waiting times (in minutes) for a technical support call center during a specific shift. From this chart, a supervisor could learn quite a lot about the customer service process. It seems most of the time, people are being served fairly quickly and the general pattern is that very short waiting times are common (notice that the tallest bar is above the $0-2$ range, and the bars decrease in height systematically from left to right). This pattern is precisely what a customer service supervisor wants to see! Were the pattern reversed (with small bars over small waiting times and large bars over large waiting times), the call center would need to be investigated to determine why customers are having to wait so long for service.\nAnother graphical tool for numerical data is the box plot. This plot typically shows five numbers:¬†the minimum value, the $25^{th}$ percentile, the median (or $50^{th}$ percentile), the $75^{th}$ percentile, and the maximum value1. The $25^{th}$ percentile is the number $x_{0.25}$ such that (approximately) $25%$ of the data falls below it and (approximately) $75%$ of the data falls above it. In general, the $p^{th}$ percentile is a number $x_{p}$ that splits the data such that (approximately) $p%$ of the data falls below it and $(100-p)%$ of the data falls above it. **Outliers** , data values that are extremely small or large compared to the rest of the data, are typically plotted separately. Figure [boxplot]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;boxplot\u0026rdquo;} displays the waiting time data. From this plot, we can see that about $25%$ of the data falls below $0.75$ minutes$,$ about $50%$ falls below $1.5$ minutes, and about $75%$ falls below $4$ minutes. The largest non-outlier waiting time was about $9$ minutes, and one unlucky soul had to wait over $12$ minutes!\nProbability Distributions The mathematical concept of probability theory forms the basis of all statistical inference. It allows us to \u0026ldquo;make the leap\u0026rdquo; from the sample we have to the population or process that we are trying to study. In order for us to be able to talk about statistics in an efficient way, we must agree upon some definitions. In this section, we will review some concepts that you should be familiar with from earlier classes.\nA random variable $X$ is a numerical or categorical characteristic of a population or process whose range of possible values is known or assumed but for which the values are subject to random (or \u0026ldquo;chance\u0026rdquo;) variation.\nAny letter of the alphabet can be used to indicate a random variable. The most common choices are $X$, $Z$, and $Y$. The name of the random variable is written in capital letters, while a specific value that the random variable takes on is written in lowercase.\n[[5k]]{#5k label=\u0026quot;5k\u0026rdquo;}Let $X$ be number of people out of $30$ who finish a $5$K race. This is a random variable before the race occurs because we cannot say with certainty how many people will ultimately finish. We know that the possible values are ${0,1,2,\\ldots ,30}$. If it turns out that $25$ people finish, then $x=25$.\nExample: Accodents Let $V$ be the number of accidents that occur on a particular length of highway on a Tuesday between 5 p.m. and 9 p.m. Before this time period occurs, $V$ is a random variable because we cannot say with certainty how many accidents will occur. We do know that the least number of accidents is $0,$ so we would say the possible values of $V$ are ${0,1,2,\\ldots }$\nExample: Coffee Let $Y$ be the time (in seconds) you spend in line at a coffee shop on a Wednesday morning. This is a random variable before you visit the coffee shop. The smallest possible wait time is $0,$ while the largest possible time is not clearly defined without more assumptions. If you decide that you will leave, with coffee or without, if the wait is more than $15$ minutes, then the range of possible values is $0\\leq y\\leq 900$. If on this particular trip you wait $350.96$ seconds, then $y=350.96$, or about $5$ minutes and $51$ seconds.\nExample: Weight Let $W$ be the number of pounds lost for an adult male over a six-week period of following a particular exercise program. This is a random variable before the six-week period is over because we cannot say for certain what the weight loss will be. We do know that it is possible to gain weight (through building muscle, for example) or to lose weight. Here it is a bit more difficult to assign a range to the random variable without knowing more about the study, but one reasonable choice would be $-35\\leq y\\leq 55$ ($-35$ implies that it is possible to gain $35$ pounds). If a particular man loses $35.4$ pounds, then $w=35.4$.\nHere are some additional important definitions.\nA probability, in practical (i.e., non-mathematically formal) terms can be thought of as a chance of something happening. The range of a valid probability for an event is $0$ (no chance of the event occurring) to $1$ (guaranteed for the event to occur). Probabilities cannot be negative.\nA discrete random variable is a random variable whose entire set of values and associated probabilities can be completely listed or at least counted.\nExamples [5k]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;5k\u0026rdquo;} and [accidents]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;accidents\u0026rdquo;} above involve discrete random variables because the possibilities can be completely listed (example [5k]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;5k\u0026rdquo;}) or counted [accidents]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;accidents\u0026rdquo;}.\nA continuous random variable is a random variable whose entire set of values cannot be listed because the possible values extend, in theory, to an infinite number of decimal places.\nExamples [coffee]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;coffee\u0026rdquo;} and [weight]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;weight\u0026rdquo;} above involve continuous random variables because time and weight are measurements that can take on, in theory, an infinite number of decimal places depending upon how precise our measurement instruments are.\nOnce we have defined a random variable, we need some way to assign the probabilities to its values. In general, how we do this depends on whether the random variable is discrete or continuous. There is some highly technical mathematics involved in that we are going to skip right over in favor of the following definition:\nA probability distribution is a specification of the possible values of a random variable and their associated probabilities, made either by listing the possible values and their probabilities or by providing a function that gives probabilities for a collection of possible values of the random variable.\nWhen we write $P(X=x),$ this stands for \u0026ldquo;the probability that the random variable $X$ equals $x,\u0026ldquo;$ or, more simply, \u0026ldquo;the probability that $X$ equals $x.\u0026ldquo;$ For example, $P(X=4),$ referring to example [5k]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;5k\u0026rdquo;} above, would be read as \u0026ldquo;the probability that $X=4,\u0026ldquo;$ which in context is the probability that $4$ runners finish the $5K$ run.\nIn general, how we assign probabilities depends on whether the random variable is discrete or continuous. Discrete probability distributions can be specified as two-column tables listing the possible values of the random variable and their associated probabilities, or by writing a function that outputs probabilities. Continuous random variables must be specified by writing a function $f(y)$ and the outputs of these functions are not probabilities, but \u0026ldquo;relative likelihoods\u0026rdquo; of various values.\nA discrete probability distribution for a random variable $X$ must meet the following criteria:\n  $\\sum_{all\\text{ }x}P(X=x)=1$ [probabilities must sum to $1$]\n  $0\\leq P(X=x)\\leq 1$ for all $x$ [probabilities are between $0$ and $1]$\n  A continuous distribution for a random variable $Y$ must meet the following criteria:\n  $f(y)\\geq 0$ for all possible $y$ [graph of the function is above the horizontal axis]\n  Total area under the curve $f(y)=1$\n  Referring to example [5k]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;5k\u0026rdquo;}, one way to define the probability distribution of $X$ would be to list the values of $x$ and their probabilities as in Table [TableKey]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;TableKey\u0026rdquo;} below (the three dots notation, $\\vdots$ ,means that the rows between $3$ and $29$ have been left out for space reasons)$$. Note that the probabilities listed are just examples.\n   $x$ $P(X=x)$     $0$ $0.000001$   $1$ $0.000003$   $2$ $0.00004$   $3$ $0.00005$   $\\vdots$ $\\vdots$   $29$ $0.10$   $30$ $0.05$    All of the probabilities above must be between $0$¬†and $1$ , and they must add up to exactly $1$. Another way to define this distribution would be to use a function. If we assume that each runner has the same probability of \u0026ldquo;success\u0026rdquo; $p$¬†(finishing the race) and that they perform independently of other runners, we might use the binomial distribution to describe the probabilities of $x$¬†number of runners finishing:\n$$P(X=x)=\\frac{30!}{(30-x)!x!}p^{x}(1-p)^{30-x}$$\nwhere the symbol $\u0026rdquo;!\u0026ldquo;$¬†denotes the factorial function (for example $5!=5\\times 4\\times 3\\times 2\\times 1)$.\nReferring to example [weight]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;weight\u0026rdquo;}, we might assume that the amount of weight lost could be described by a bell-shaped curve with a peak value at $10$ pounds, such as the one in Figure [wt_dist]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;wt_dist\u0026rdquo;}.\nThe particular function that produces this graph is\n$$f(y)=\\frac{1}{15\\sqrt{2\\pi }}e^{\\frac{-(y-10)^{2}}{450}}$$\nThe graph meets the criterion that it must lie above the horizontal axis. Further, if you were to use calculus to find the area under the curve, it would be exactly $1$.¬†The distribution shown in Figure [wt_dist]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;wt_dist\u0026rdquo;} is known as the normal distribution, and we will be seeing it again and again.\nMany times, a histogram of the data can suggest a particular probability distribution. Look again at Figure [calls]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;calls\u0026rdquo;}, which shows the distribution of waiting times at a call center. This \u0026ldquo;downstairs\u0026rdquo; pattern suggests another widely used continuous probability distribution called the exponential distribution. Figure [exp_dist]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;exp_dist\u0026rdquo;} shows the exponential distribution drawn over the histogram.\nOnce we have a random variable and its distribution, we can talk about its properties. The two most common properties to examine are its mean ( expected or average) value and its degree of variability (variance or standard deviation). We will only look at the definitions of these quantities for the discrete case because integral calculus is required to define them for continuous random variables.\nThe expected value (or mean) of a discrete random variable $X,$ denoted by either $E(X)$ or $\\mu$ (and pronounced \u0026ldquo;mu\u0026rdquo;)$,$ is defined as\n$$\\mu =E(X)=\\sum_{all\\text{ }x}xP(X=x)$$\nThat is, it is a weighted (by the probabilities) sum of the possible values of the random variable.\nThe variance of a discrete random variable $X,$ denoted by either $V(X)$ or $\\sigma ^{2}$ (and pronounced \u0026ldquo;sigma squared\u0026rdquo;) is defined as\n$$V(X)=\\sum_{all\\text{ }x}(x-\\mu )^{2}P(X=x)$$\nA shortcut formula is $$E(X^{2})-[E(X)]^{2}$$\nThe standard deviation, denoted as either $StDev(X)$ or $\\sigma$ (read as \u0026ldquo;sigma\u0026rdquo;)$,$ is defined as $\\sqrt{V(X)}$.\nBasically, $E(X)$ is the population version of $\\overline{x}$ and $V(X)$ is the population version of $s^{2}$.\nThe following facts will come in handy later.\n(math fact) If $X$ and $Y$ are random variables and $a$ and $b$ are any constants (fixed values), then all of the following statements about expected value and variance are true:\n$$\\begin{gathered} E(aX+b)=aE(X)+b \\ E(aX+bY)=aE(X)+bE(Y) \\ V(aX+b)=a^{2}V(X) \\ V(aX+bY)=a^{2}V(X)+b^{2}V(Y)+2ab\\rho StDev(X)StDev(Y)\\end{gathered}$$\nThat is, you get to pull the constants out in front of the mean and variance operators, but with variance you square the constant while with expected value, you just pull it out. The mean of a constant is just the constant, while the variance of a constant is $0$.\nThe Greek letter $\\rho$ is pronounced \u0026ldquo;row\u0026rdquo; and describes the correlation (or linear relationship) between two numerical variables. We will return to this concept toward the end of the course.\nThe following questions address the topics above. Most, if not all, of these questions are review from BANA 2372.\n The General Social Survey asked the following question : In the last 12 months, how many times have you been injured on the job?  a). What must $P(X=12)$ be for this to be a valid probability distribution?\nb). Find $\\mu =E(X),$ the mean of the distribution of $X$.\nc). Find $\\sigma ^{2}=V(X),$ the variance of the distribution of $X$.\ne). Find $\\sigma =StDev(X),$ the standard deviation of the distribution of $X$.\nf). Find $E(10X+2)$ using the rules of expected value.\ng). Find $V(9X-7)$ using the rules of variance.\n (Real-world application). A potential customer for a $50,000$ fire insurance policy has a home in an area that, according to past experience, may sustain a total loss in a given year with a probability of $0.001$ and a $50%$ loss with a probability of $0.01$. There is a $0.989$ chance that the customer will make no claim in the coverage year. The company also offers renter\u0026rsquo;s insurance. This same potential customer wants a $20,000$ policy. Based on historical data the company has for the area, the probability of a total loss due to fire is $0.005,$ the probability of a $50%$ loss is $0.015,$ and the probability of no loss is $0.98$. Let $X$ be the company\u0026rsquo;s loss on the fire insurance policy and $Y$ be the company\u0026rsquo;s loss on the renter\u0026rsquo;s insurance policy.  a). Find the expected loss for the fire policy and for the renter\u0026rsquo;s insurance policy individually. That is, find $E(X)$ and $E(Y)$.\nb). Find the combined expected loss for the company on the two policies. That is, find $E(X+Y)$.\nc). Suppose the company combines the premiums for the two policies and charges $600$ per year. Based on your answer to (b), is this a good business decision for the company?\nd). Suppose the company offers the fire policy to $20$ customers and the renter\u0026rsquo;s insurance to $10$ customers in the same area. Find the combined expected loss for the company, assuming each customer has identical loss probabilities. That is, Find $E(20X+10Y)$.\nBinomial Distribution In this section, we review in more detail one of the most important discrete distributions.\nThe binomial distribution is used to model the situation in which there are $n$ identical trials or experiments and you want to know the chance of getting $x$ \u0026ldquo;successes\u0026rdquo; or \u0026ldquo;events\u0026rdquo; out of those $n$ trials when the probability of success is known to be $p$. If $X$ has the binomial distribution, then\n$$P(X=x)=\\frac{n!}{(n-x)!x!}p^{x}(1-p)^{n-x}$$\nwhere $!$ is the factorial operation. For example $5!=5\\times 4\\times 3\\times 2\\times 1$.\nThis can also be written as\n$$P(X=x)=\\binom{n}{x}p^{x}(1-p)^{n-x}$$\nwhere $\\binom{n}{x}=\\frac{n!}{(n-x)!x!}$ and is read as \u0026ldquo;$n$ choose $x$\u0026rdquo;\n (Real-world application). An operations manager for Umbrella Corporation, a consumer products company, is in charge of determining if one of its current suppliers should be dropped because of an unacceptably high defect rate in the chemical it is sending to the company. The supplier provides the chemical in bottles. Management personnel from both companies have agreed that $1$ bottle out of $100$ (a defect rate of $0.01)$ is an acceptable risk. Suppose the company receives a batch of $50$ bottles and finds that $2$ bottles contain defective product.  a). What is the probability that Umbrella would find exactly $2$ defective bottles in a shipment of $50$, if the real defect rate is $0.01$ as promised?\nb). What is the probability that Umbrella would find more than $1$ defect?\nc). Using your answer to (b), as a supply chain manager, what is your recommendation for keeping or dropping the supplier?\n  Suppose you are investigating whether there is a difference in taste between organic milk and regular milk. You recruit $10$ random volunteers and ask each one individually after tasting to tell you which of two unmarked glasses contains the organic milk. Let X be the number of correct guesses. If there really were no difference in taste between organic milk and regular milk, what would be the probability of $8$ or more correct guesses, to three decimal places?\n  The marketing department of Low-Brau, a microbrewery that makes what it considers an excellent light beer, asked $7$ volunteers, individually, to taste $10$ types of light beer (one of which was the company\u0026rsquo;s brand and the others being competitor brands) and report which one they liked the best. Out of those $7$ people, $3$ said they liked Low-Brau\u0026rsquo;s light beer the best. The taste test was \u0026ldquo;blind\u0026rdquo; in that the volunteers did not know beforehand which of the $7$ beers they tasted was Low-Brau\u0026rsquo;s. If there is no difference in the taste of the light beers, what is the chance that $3$ out of $7$ people would be able to rate Low-Brau\u0026rsquo;s as the best?\n  Descriptive Statistics Definition: Descriptive statistics is a collection of techniques for organizing, summarizing, and presenting a sample of data in a convenient, informative way.\n  If you follow sports, you know this definition:\n Completed passes, yards gained by passing, rushing touchdowns Turnovers, rebounds, free-throws made Batting average, on-base percentage, runs produced    Business/Economics\n Average time customers spend on hold in a call center Proportion of prospects at a car dealership who end up purchasing Variability in stock returns for the Dow Jones for the past year Average amount of debt students carry after graduation Average market value of homes in a neighborhood Proportion of members of a focus group who think favorably about a new product       Bias Variance Mean Square Error  Histograms A histogram is a bar graph that represents the frequency distribution of a dataset. It is a way to visualize a pattern in a numerical data set. It answers the questions about\n Center (where the numbers tend to fall) Spread (the degree of variability in the data) and Shape (the pattern in which the numbers tend to fall)   A histogram can be used to estimate continuous probability distributions  Properties of a histogram:\n Horizontal scale measures the data values Vertical scale measures the frequencies of the bins Consecutive bars touch   Histogram Example: Call Durations Suppose you work at a call center for a cable company. Your job is to help customers troubleshoot their technology problems. Many people are bad with technology, and so the call center stays busy. Your manager wants to make sure that people aren‚Äôt on hold for too long; otherwise they might bail and go to another provider. Your department keeps track of the hold times for customers. Each week, your manager reviews the data to see if changes need to be made. To do this, she makes a histogram of the duration times, which we will call Y.\nHere we see that most customers get served pretty quickly (the times between 0 and 2 are far more frequent). However, some customers have to wait a lot longer.\nSuppose we want to find the probability that a customer has to wait between 1 and 4 minutes for service. How can we do that?\nIt looks like we might be able to fit a smooth curve to the histogram. For N = 100 calls, it doesn‚Äôt fit so well, but for n = 10,000 calls, it gets better.\nThis smooth curve is an example of a continuous distribution function, and using it, we can find the probabilities that a caller has to wait between 1 and 4 minutes.\nEstimating Probabilities Using a Histogram  Find Relative Frequencies Estimate the Density Probabilities Are Estimated as (width of the rectangle) * density  This is an estimate of the probability without resorting to calculus, which gives exact answers.\nWhat is a scatter plot? A way to graph paired data sets where the ordered pairs are graphed as points in a coordinate plane. Scatter plots show the relationship between two quantitative variables.\n Central limit theorem In the central limit theorem:\n No matter the distribution* the more samples, closer to normal the bigger the samples, the closer to normal    Measures of dispersion Population variance: The mean of the squares of the deviations. Population standard deviation: The square root of the population variance.\n Find the mean and each deviation. Square each deviation. Find the sum of the squares. Divide by the number of data entries. Take the square root of the population variance.  Standard Deviation: $\\sigma = \\sqrt{\\frac{\\Sigma(x-\\bar{x})^2}{n}}$\n Empirical Rule(Or 68-95-99.7 Rule) For data with a (symmetric) bell-shaped distribution, the standard deviation has the following characteristics.\n About 68% of the data lie within one standard deviation of the mean. About 95% of the data lie within two standard deviations of the mean. About 99.7% of the data lie within three standard deviations of the mean.   Quartiles of a Data Set What are quartiles?\nThe three quartiles, $Q_1$ ,$Q_2$ ,and $Q_3$ , approximately divide an ordered data set into four equal parts. About one quarter of the data fall on or below the first quartile $Q_1$.About one half of the data fall on or below the second quartile $Q_2$.About three quarters of the data fall on or below the third quartile $Q_3$.\n Interquartile Range (IQR) The interquartile range (IQR) of a data set is a measure of variation that gives the range of the middle 50% of the data. It is the difference between the third and first quartiles.\n Box-and-Whisker Plot  Find the five-number summary of the data set. (minimum entry, first quartile, median, third quartile, maximum entry) Construct a horizontal scale that spans the range of the data. Plot the five numbers above the horizontal scale. Draw a box above the horizontal scale from $Q_1$ to $Q_3$ and draw a vertical line in the box at $Q_2$. Draw whiskers from the box to the minimum and maximum entries.   The Shapes of Frequency Distributions  Skewed Distributions    Skew Neg/Positive Meaning     Left Negatively Skewed The tail of the graph extends more to the left. More of the data entries are clumped on the right.   Right Positively Skewed The tail of the graph extends more to the right. More of the data entries are clumped on the left.       Standard normal distribution It is a normal distribution with a mean of 0 and a standard deviation of 1.\nEvery normal distribution can be standardized using the following formula:\n$$\\alpha = \\frac{x-/mu}{\\sigma}$$\n Discrete Probability Distributions Definition: A discrete probability distribution is a listing or specification of the possible values of a random variable and their associated probabilities Notation:\n An upper-case letter (like X) will represent the name of the random variable, Its lower-case counterpart (like x) will represent the value of the random variable. The probability that the random variable X will equal x is P(X = x)     Distribution Definition     Normal distribution Also known as Gaussian distribution or Bell Curve. it is mostly used in regression analysis. When data is normally distributed then distribution is symmetric and mean = median = mode.   Uniform or Rectangular All entries in the distribution have approximately equal frequencies.   Symmetric If a vertical line is drawn through the middle of the graph, then the two halves would be mirror images.   Continuous Distributions A distribution for a continuous random variable, say, Y, is written as f(y). It is a smooth function with points on the curve indicating ‚Äúrelative likelihoods‚Äù of observations. It must meet two criteria: 1. f(y) ‚â• 0 for all possible values that the random variable can take on. 2. The total area under the curve is exactly 1. Probabilities are calculated as areas under the curve.      There are many ways of calculating percentiles from data, and no method is truly the standard. You can read more about this issue here: http://www.dummies.com/how-to/content/how-to-calculate-percentiles-in-statistics.html \u0026#x21a9;\u0026#xfe0e;\n   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"70776a7ce71db5bbc67eea505a478c29","permalink":"/courses/bana3363/1-basic-statistics-concepts/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/bana3363/1-basic-statistics-concepts/","section":"courses","summary":"What is statistics? Statistics is the science of collecting, organizing, analyzing, and interpreting data in order to make decisions.\nStatistics is all around us. We hear statsitics on the news and we use statistics to make buisnes decisions.","tags":null,"title":"Basic Statistics Concepts","type":"docs"},{"authors":null,"categories":null,"content":"Sampling Distributions Let\u0026rsquo;s get a definition out of the way. This comes very close to the \u0026ldquo;official\u0026rdquo;¬†Google definition:\n A sample is a small collection of objects whose properties are thought to correspond closely with those of a large (or even infinite) collection of items or quantities that is the main interest of the researcher.   This large collection of items or quantities about which we would like to make reasonable statements is called a population, or, more accurately, a statistical process, or just process.\nYou will see the word \u0026ldquo;population\u0026rdquo; used more often than \u0026ldquo;process\u0026rdquo; in most of the material you read despite \u0026ldquo;process\u0026rdquo; being a more accurate description of how a statistician views the natural world. The word \u0026ldquo;population\u0026rdquo; evokes images of a single, fixed collection of objects that we sample from. In much of statistics, and especially in forecasting and time series, the population view is simply wrong. In many cases, what a researcher is trying to study is an ever-changing process with many factors that can influence the actual data the researcher can see. For these situations, the process view is better 1.\nFor example, suppose wish to study income of beginning accounting professionals in the U. S. Many factors interact to produce the income population: labor markets, the government, regional events, education, etc.. You take a sample of $300$ professionals and invite them to complete a survey about income. Your goal might be to estimate the \u0026ldquo;population\u0026rdquo; mean beginning accounting salary, $\\mu$, using the sample of size $300$. There are two main problems. First, the population you want to make a conclusion about (all beginning accountants) is not the population you drew from (all beginning accountants who don\u0026rsquo;t mind filling out a survey). Furthermore, some respondents no doubt will (unintentionally or intentionally) give you incorrect responses, so the \u0026ldquo;population\u0026rdquo; you are sampling from is again not quite what you want. Taking the process view, though, you can view $\\mu$ as the mean of the process that produces the actual accounting salary data you see, and you can use inference to make conclusions about that process. If your study is designed properly, your results will be a reasonable representation of the \u0026ldquo;population\u0026rdquo; you are really interested in, beginning accounting graduates.\nFor purposes of doing the work in this class, however, you can use phrases such as \u0026ldquo;the population mean\u0026rdquo; (I probably will, just out of habit) and be fine. Now we need to be more specific about what a sample is.\nA random sample is a collection of $n$ independent random variables $Y_{1},Y_{2},\\ldots,Y_{n}$ that are assumed to have been \u0026ldquo;drawn\u0026rdquo; from the same probability distribution $f(y|\\mathbf{\\theta ),}$where $\\mathbf{\\theta }$ is a generic collection of parameters. For example, if we are taking a sample from a normal distribution $\\mathbf{\\theta ={}\\mu,\\sigma }$.\nFigure [sampling]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;sampling\u0026rdquo;} illustrates this idea. Essentially, before the researcher gathers data, he or she imagines that some (usually unknown) probability distribution produces data from which he or she will draw a single sample of size, say, $300$. When the data are actually gathered, the researcher takes the view that he or she is observing one possible realization of those $300$ random variables.\nThe key to understanding statistical inference is that different samples produce different values of calculated statistics such as the sample mean and sample standard deviation, but these values behave in a predictable way. Although we only work with one sample, we use the properties, we use the fact that the samples behave in a predictable way to make the leap from any one sample to the population or process of interest.\nA sampling distribution is the distribution of a calculated statistic (for example, the sample mean) when we imagine taking repeated samples from the same process and calculating the statistic over and over.\nThere is an entire distribution of values because each random sample of size $n$ will have different data in it. So, a statistic is actually a random variable too. Therefore, when we talk about a statistic like the sample mean in general terms, we will use a capital letter, $\\overline{Y}$, and when we talk about a specific value of the statistic, like $2.3$, we will use lowercase, as in $\\overline{y}=2.3$. A good illustration of this concept is found here http://onlinestatbook.com/stat_sim/sampling_dist/, courtesy of our friends at Rice University.\nWe can summarize the distribution like we have before using expected value and variance. It turns out that if we choose the right statistic, we can make some powerful conclusions about the population. Here are some cool facts.\n[[samp_dist]]{#samp_dist label=\u0026quot;samp_dist\u0026rdquo;} For a random sample of size $n$ from a process (or population), the sample mean, $\\overline{Y}$, is a random variable with mean $=E(\\overline{Y})=% %TCIMACRO{\\U{3bc} }% %BeginExpansion \\mu %EndExpansion$ (the process/population mean) and variance $V(\\overline{Y})=$ $\\frac{% \\sigma ^{2}}{n}$, the population variance divided by $n$.\nStand by for the most important fact in statistics.¬†It\u0026rsquo;s so important that it deserves its own section.\nSampling Distribution of the Sample Mean The Central Limit Theorem The sampling distribution of the sample mean $\\overline{Y}$ is approximately a normal distribution with mean $=\\mu$ and variance $\\frac{\\sigma ^{2}}{n}$, no matter what the original distribution looks like, as the sample size $n$ gets large.\n\u0026ldquo;Large\u0026rdquo; here technically refers to letting the sample size go to infinity, but in most practical cases, the Central Limit Theorem (CLT) applies when $n$ is as small as $30$.\nIf we assume the sample is drawn from a normal distribution, then the distribution of $\\overline{Y}$ is normal regardless of sample size.\nThe CLT is the reason we don\u0026rsquo;t really have to worry about what the population distribution looks like in Figure #sampling. It says that the distribution can take any shape and the distribution of the sample average will still be approximately normal. The CLT is what makes most of statistics work for practical problems.\nWe can use the distribution of the sample mean to make conclusions about a sample. For example, we can ask, what is $P(\\overline{Y}\u0026gt;4)?$ To answer this question, we can use our old friend the $z$-score. Remember that a $z$-score is, in general,\n$$\\frac{\\text{observation }-\\text{ mean}}{\\text{standard deviation}}$$.\nZ-scores for sample averages are found by subtracting the mean and dividing by the standard deviation, which are given in Theorem [samp_dist]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;samp_dist\u0026rdquo;}:\n$$Z=\\frac{\\overline{Y}-\\mu }{\\sigma /\\sqrt{n}}$$\nHere are some examples to try out.\nThe foreman of a bottling plant has observed that the amount of soda in each \u0026ldquo;$32$-ounce\u0026rdquo;¬†bottle is actually a normally distributed random variable, with a mean of $32$ ounces and a standard deviation of $0.3$ ounce. If the foreman is doing quality control by taking samples of $n$ $=4$ bottles from the line and calculating the average fill, would a reading of $32.5$ be unusually high? Answer by finding $P(\\overline{Y}\u0026gt;32.5)$. How would your answer change if we took away the assumption that the amount of soda in each bottle comes from a normal distribution?\nYou take a random sample of size $50$ from a population with mean $\\mu =16$ and standard deviation $\\sigma = 8$.\na). What are the mean and standard deviation of $\\overline{Y}$, the sample mean?\nb). Sketch the distribution, indicating the intervals within which $68%$, $% 95%$, and $99.7%$ of the observations are expected to lie.\nc). Find $P(\\overline{Y}\u0026gt;18)$\nd). Find $P(17\u0026lt;\\overline{Y}\u0026lt;19.5)$\ne). Explain why all of these probabilities are approximate.\nIn a random sample of $20$ college students, the average reported working time was $15.4$ hours per week. A researcher claims that the true working time of college students is normally distributed with a mean of $16$ hours and a standard deviation of $2.5$ hours. Let $\\overline{Y}$ denote the sample mean.\na). What is $P(\\overline{Y}\u0026lt;15.4)$?\nb). Using your answer to (a), how reasonable do you think the researcher\u0026rsquo;s claim is?\nc). Using the $68-95-99.7$ rule, find a set of sample averages that would be unusual to see (either \u0026ldquo;too large\u0026rdquo; or \u0026ldquo;too small\u0026rdquo;) if the true mean and standard deviation of college student working hours were as the researcher claims.\nIn the next section, we will discuss how we can use the sampling distribution of the sample mean to make inferences about $\\mu $.\nInterval Inference about a Single Population (Process) Mean Basic Concepts Our ultimate goal is to obtain a reasonable \u0026quot; guess\u0026rdquo;¬†at the true value of a process parameter (in this section, $\\mu$) using a sample of data. Before we do that, we have to introduce some terms.\nA**¬†point estimator** for a parameter is a particular function of the random sample that gives a single number that we hope is \u0026quot; close\u0026rdquo;¬†to the true value of the parameter.\nAn interval estimator is a range of values, constructed using observed data, within which the parameter is believed to fall.\nThis sounds very technical, but it really is not so bad. An example of a point estimator is the sample mean: $\\overline{Y}=\\frac{\\sum Y_{i}}{n}$. Notice how it is a function of the data (the values of $Y_{i})$. In practice, we typically use the point estimator to construct an interval estimator.\nIn theory, there are an infinite number of point estimators we could pick to estimate $\\mu$, but it turns out that only one, $\\overline{Y}$, is \u0026ldquo;the best.\u0026rdquo; The following criteria are used to judge whether a point estimator is \u0026ldquo;good:\u0026rdquo;\n  Unbiasedness \u0026ndash; The average of the estimator is equal to the population parameter.\n  Consistency \u0026ndash; The estimator gets closer and closer to the true population parameter as the sample size increases\n  Relative efficiency \u0026ndash;Given two different estimators with the same sample size, we choose the one with the smaller variance.\n  We know from Theorem [samp_dist]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;samp_dist\u0026rdquo;} that the sample mean $% \\overline{Y}$ is unbiased because the theorem states that $E(\\overline{Y})=% %TCIMACRO{\\U{3bc} }% %BeginExpansion \\mu %EndExpansion $.\nThe other two properties are also true, but we won\u0026rsquo;t prove it; just take my word for it. Also, it can be shown that $\\overline{Y}$ is has the smallest variability among all unbiased estimators, so when we use it, we are using \u0026ldquo;the best\u0026rdquo; guess of $\\mu $.\nA Confidence Interval for the Mean When $\\sigma$ Is Known By the Central Limit Theorem, we know that the distribution of $\\overline{Y}$ is approximately $N(\\mu,\\sigma /\\sqrt{n})$. This implies that we can predict something about the behavior of any given sample average $\\overline{x}$. Specifically, we know that a sample average will have about a $68%$ chance of being within one standard deviation of $% \\mu$ about a $95%$ chance of being within two standard deviations of $\\mu$, and about a $99.7%$ chance of being within three standard deviations of $% \\mu $.\nConfidence intervals flip this logic around and use the fact that if $% \\overline{Y}$ has a $68%$ chance of being within one standard deviation of $% \\mu$, then we can also say that $\\mu$ has a $68%$ chance of being within one standard deviation of $\\overline{Y}$. This means that\n$$P(\\mu -2\\sigma /\\sqrt{n}\u0026lt;\\overline{Y}\u0026lt;\\mu +2\\sigma /\\sqrt{n})$$\nand\n$$P(\\overline{Y}-2\\sigma /\\sqrt{n}\u0026lt;\\mu \u0026lt;\\overline{Y}+2\\sigma /\\sqrt{n})$$\nare equal. One statement can be made from the other simply by rearranging what goes in the middle using algebra.\nHere\u0026rsquo;s a picture that might make things clearer.\nWe don\u0026rsquo;t know what $\\mu$ is, but we know that $\\overline{Y}$ should fall around it in a predictable way. So we use this idea to \u0026ldquo;guess\u0026rdquo; at the location of $\\mu$. If we were to take the range of numbers $% \\overline{x}-2\\sigma /\\sqrt{n}$ to $\\overline{x}+2\\sigma /\\sqrt{n}$ as our interval estimator, about $95%$ of the time, the true value of $\\mu$ would fall in the interval; about $5%$ of the time it wouldn\u0026rsquo;t. So we could say we were $95%$ confident in the interval.\nWhat if we want a different level of confidence than $68,95$, or $99.7%?$¬†Suppose we wanted to be $98%$ confident, for example? Recall from Handout 2 that $P(-z_{\\alpha /2}\u0026lt;Z\u0026lt;z_{\\alpha /2})=1-\\alpha$. If we want $98%$ confidence, we¬†begin by setting $1-\\alpha =0.98$, which means $\\alpha =0.02$. Then $z_{\\alpha /2}=z_{0.02/2}=z_{0.01}=2.33$. Then we substitute $Z=\\frac{% \\overline{Y}-\\mu }{\\sigma /\\sqrt{n}}$ in the middle and solve for $\\mu$:\n$$\\begin{aligned} P(-2.33 \u0026amp;\u0026lt;\u0026amp;Z\u0026lt;2.33)=0.98 \\ \u0026amp;\\Rightarrow \u0026amp;P(-2.33\u0026lt;\\frac{\\overline{Y}-\\mu }{\\sigma /\\sqrt{n}}\u0026lt;2.33)=0.98 \\ \u0026amp;\\Rightarrow \u0026amp;P(-2.33\\sigma /\\sqrt{n}\u0026lt;\\overline{Y}-\\mu \u0026lt;2.33\\sigma /\\sqrt{n}% )=0.98 \\ \u0026amp;\\Rightarrow \u0026amp;P(-2.33\\sigma /\\sqrt{n}+\\overline{Y}\u0026lt;-\\mu \u0026lt;2.33\\sigma /\\sqrt{n}% -\\overline{Y})=0.98 \\ \u0026amp;\u0026amp;\\text{Multiply by }-1\\text{ and rearrange to get} \\ P(\\overline{Y}-2.33\\sigma /\\sqrt{n} \u0026amp;\u0026lt;\u0026amp;\\mu \u0026lt;\\overline{Y}+2.33\\sigma /\\sqrt{n}% )=0.98\\end{aligned}$$\nYou can apply these steps for any confidence level you want. Therefore, we have the following:\n A $(1-\\alpha )100%$ confidence interval for $\\mu$ when $\\sigma$ is known (or when $n$ is large) is given by\n$$\\overline{x}\\pm z_{\\alpha /2}\\frac{\\sigma }{\\sqrt{n}}$$\n  Calculation is easy with software. Interpretation is key. We view the mean $\\mu$ as a fixed but unknown quantity. Once we gather data and compute the sample mean $\\overline{x}$ and the associated confidence interval, the interval either contains $\\mu$ or it does not (see Figure [Figure_c_int]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;Figure_c_int\u0026rdquo;} above). In the long run, however, if you took repeated samples of size $n$ and computed the interval each time, $(1-\\U{3b1} )100%$ of them would contain $\\mu$.Thus, the \u0026ldquo;confidence\u0026rdquo;¬†you have is in the method used to construct an interval, not in the particular interval you have constructed.\nRarely do we know $\\sigma$, but at times we have fairly large samples, so we can use Definition [CI_sig_known]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;CI_sig_known\u0026rdquo;} anyway by just replacing $\\sigma$ with the sample standard deviation $s$. If we have a small sample and do not know $\\sigma$, we must use some other multiplier other than $z_{\\alpha /2}$. For this, we use values from Student\u0026rsquo;s t distribution.\nA Confidence Interval for the Mean When $\\sigma$ Is Unknown Student\u0026rsquo;s $t$ distribution with $\\nu$ degrees of freedom is the probability distribution of the quantity\n$$t=\\frac{\\overline{Y}-\\mu }{s/\\sqrt{n}}$$\nAs with the standard normal distribution, it is symmetric, bell-shaped, and centered at $0$. The degrees of freedom $\\nu$ is a parameter that governs the width of the distribution. The notation $t_{\\nu }$ refers to a random variable from a $t$ distribution with $\\nu$ degrees of freedom.\nHere\u0026rsquo;s a picture. As you can see in Figure [studt]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;studt\u0026rdquo;}, Student\u0026rsquo;s $t$ distribution looks a lot like the standard normal. In fact, it can be shown that as $\\nu$ gets very large, the the $t$ distribution \u0026ldquo;becomes\u0026rdquo; the standard normal distribution. That\u0026rsquo;s why we can use values of $z$ when $n$ is large in a confidence interval.\nUsing the $t$ distribution, we can define a confidence interval as follows:\n$(1-\\alpha )100%$ confidence interval for $\\mu$ when $\\sigma$ is unknown (or when $n$ is small) is given by\n$$\\overline{x}\\pm t_{n-1,\\alpha /2}\\frac{s}{\\sqrt{n}}$$\nwith $t_{n-1,\\alpha /2}$, being the value of a $t$ distribution with $n-1$ degrees of freedom that has probability $\\alpha /2$ to its right.\nHere are some examples.\nA recent survey asked, \u0026ldquo;What is the ideal number of children for a person to have?\u0026rdquo;¬†A sample of $15$ American adults aged $20$ to $30$ reported the following numbers:\n$1$ $2$ $0$ $3$ $4$ $2$ $0$ $1$ $3$ $4$ $2$ $2$ $2$ $3$ $8$.\nFind a $93%$ confidence interval for the mean number of children American adults in this age group believe is ideal.\nIn a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8$, respectively. Calculate $95%$ confidence intervals for the amount of time spent studying per week for both accounting and general business majors. Interpret the intervals in context.\nIn the next subsection, we address the situation in which you wish examine two population/process means.\nInterval Inference about Two Population/Process Means Many times we are not interested only in a single group, but multiple $(\\geq 2)$ groups. For example, we might compare the amount of weight lost for two groups to determine the effect of a new drug on adult men $45-60$. Let Group $1$ be men 45-60 who follow a specific diet and let Group $2$ be men $45-60$ who use the same diet but also take a weight loss drug. In the language of experimental design, Group $1$ is a control group, the group that receives no treatment (i.e., a specific manipulation of the members of the group), or no treatment above a reasonable standard. Group $2$ is an experimental or treatment group.\nBy the fundamental assumption of statistics (that variability is inescapable), any one person in Group $1$ could lose more weight than any one person in Group $2$, even if the drug is effective, simply by random chance. The question is not whether the drug works for everyone, but if it \u0026ldquo;works on average.\u0026rdquo;¬†The average for Group $1$ we can specify as $\\mu_{1}$ and the average for Group 2 we can specify as $\\mu_{2}$. Then the question is whether $\\mu_{1}=\\mu_{2}$, or, equivalently, whether $\\mu_{1}-\\mu_{2}=0$. We can address this question either through a confidence interval or a hypothesis test. We will discuss the first of these here. There are two cases to consider:\n  You know (or assume) that the two population standard deviations are the same, that is, $\\sigma_{1}=\\sigma_{2}$\n  You don\u0026rsquo;t know (or don\u0026rsquo;t wish to assume) that the two population standard deviations are the same, that is $\\sigma_{1}\\neq \\sigma_{2}$\n  Case 1: Confidence Interval for Difference in Means $\\sigma_{1}=\\sigma_{2}$ A $(1-\\alpha )100%$ confidence interval for $\\mu_{1}-\\mu_{2}$ is given by $\\overline{x}_{1}-\\overline{x}_{2}\\pm t_{n_{1}+n_{2}-2,\\alpha /2}s_{p}\\sqrt{% \\frac{1}{n_{1}}+\\frac{1}{n_{2}}}$, where $s_{p}=\\sqrt{\\frac{% (n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2}}$ is the \u0026ldquo;pooled\u0026rdquo; standard deviation from the two samples.\nCase 2: Confidence Interval for Difference in Means $\\sigma_{1}\\neq \\sigma_{2}$ A $(1-\\alpha )100%$ confidence interval for $\\mu_{1}-\\mu_{2}$ is given by $\\overline{x}_{1}-\\overline{x}_{2}\\pm t_{\\nu,\\alpha /2}\\sqrt{\\frac{s_{1}^{2}% }{n_{1}}+\\frac{s_{2}^{2}}{n_{2}}}$, where $\\nu =\\frac{\\left( \\frac{s_{1}^{2}% }{n_{1}}+\\frac{s_{2}^{2}}{n_{2}}\\right) ^{2}}{\\frac{\\left( \\frac{s_{1}^{2}}{% n_{1}}\\right) ^{2}}{n_{1}-1}+\\frac{\\left( \\frac{s_{2}^{2}}{n_{2}}\\right) ^{2}% }{n_{2}-1}}$.\nThe Case 2 interval makes one less assumption than Case 1, so it is closer to the truth (in reality, it\u0026rsquo;s unlikely that the two standard deviations will be exactly equal). On the other hand, it is known that the Case 2 interval is only an approximate interval, although the approximation is quite good. As with most things in life, there is a trade-off. In general, however, Case 2 is the safer bet. The degrees of freedom parameter, $\\nu$, is easily calculated using software.\nHere\u0026rsquo;s an example:\nIn a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8$, respectively. Calculate a $90%$ confidence interval for mean difference in the amount of time spent studying per week between accounting and general business majors. Interpret the interval in context. Assume the population standard deviations are equal.\nIn the next section, we\u0026rsquo;ll address another form of inference for means, hypothesis testing.\nHypothesis Testing Inference for a Single Population/Process Mean Basic Concepts Let\u0026rsquo;s begin with a definition.\nA hypothesis test (also called a significance test) is a method of using data to evaluate the evidence about a specified value of a parameter.\nEvery statistical test has two mutually exclusive hypotheses, or claims about the value of a parameter.\nThe null hypothesis, denoted $H_{0}$ and pronounced \u0026ldquo;$H$ not\u0026rdquo;¬†or \u0026ldquo;$H$ sub-zero,\u0026rdquo;¬†is typically a statement of \u0026quot; no effect,\u0026rdquo;¬†\u0026ldquo;no difference,\u0026rdquo;¬†\u0026ldquo;no special ability beyond random chance,\u0026rdquo;¬†\u0026quot; equality,\u0026rdquo;¬†etc. This is usually what the researcher wants to disprove, or reject.\nThe alternative hypothesis, denoted $H_{a}$ or $H_{1}$, is the complement (\u0026ldquo;opposite\u0026rdquo;) of the null, and usually what the researcher is trying to establish.\nHypotheses always come in pairs.\nWhen performing a hypothesis test, it is possible to be wrong, or, more formally, to commit an error. Here are the two types of errors we\u0026rsquo;ll be concerned with (there is a third type as well but we won\u0026rsquo;t worry about that one).\nA Type I error occurs when we reject a true null hypothesis. This is akin to a \u0026ldquo;false positive\u0026rdquo; (claiming there is an effect or difference when there isn\u0026rsquo;t).\nA Type II error occurs when we don\u0026rsquo;t reject a false null hypothesis. This is akin to a \u0026ldquo;false negative\u0026rdquo; (claiming there is not an effect or difference when there is).\nFor any given test, we don\u0026rsquo;t know if we have committed a Type I error because we don\u0026rsquo;t know what the true value of the parameter is (if we did, why do a test at all?). What we can do is specify that we want to construct our test such that the $P($Type¬†I error$)$ $=\\alpha$ is \u0026ldquo;small.\u0026rdquo; That is, if we could imagine doing the test repeatedly for different samples from the same population, in the long run, we would only want to reject $H_{0}$ $% \\alpha (100)%$ of the time. A similar criterion exists for a Type II error: we want $P($Type II error) $=\\beta$ to be small as well.\nThe two types of errors cannot be minimized at the same time; lowering one error probability raises the other. What is done is to find tests with a small $\\alpha$, and from that set, find the test that also has a small $% \\beta$. For our purposes, we will only consider Type I error. It can be shown that the hypothesis tests presented next also have small $\\beta $.\nHypothesis testing can be thought of as a process with a number of steps. Here they are:\n  Specify the hypothesis\n  Calculate the test statistic\n  Compute the p-value or rejection region\n  Make a conclusion\n  We already talked about (1) in Figure [hyps]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;hyps\u0026rdquo;}. Here are some more definitions:\nA test statistic is a specific function of data and the parameter value specified by the null hypothesis.\nThe p-value of a test is the probability, assuming the null is true, that we would observe a value of the test statistic as extreme or more extreme than what we actually got.\nWe make a conclusion based on the $p$-value or the rejection region. $P$ -values are more commonly used in practice. The judgement is simple: if the $p$-value $\\leq$ $\\alpha$ (i.e., our Type I¬†error probability),then we reject the null hypothesis; if the $p$-value $\u0026gt;\\alpha$, we fail to reject the null hypothesis. Most software programs report $p$-values automatically.\nAnother approach is to specify, before examining the data, the $% \\alpha$ (Type I error probability) that we want and determine a critical value, a value of test statistic distribution under the null hypothesis such that if the test statistic falls within that region, we reject the null hypothesis. A visual of this idea is shown in Figure [rr]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;rr\u0026rdquo;} for a test of a mean. If the calculated value of the test statistic falls in the shaded region, we would reject the null hypothesis.\n If a $(1-\\alpha )100%$ confidence interval contains the value of $% H_{0}$, then a test of $H_{0}:\\mu =\\mu_{0}$ conducted at the $\\alpha$ level will **fail to reject** the null hypothesis.   This fact can be proven but the intuition is simple: if the interval contains the most reasonable values of the parameter (based on the data and the confidence level) then it\u0026rsquo;s sensible that the test should not reject one of the values that lies inside the interval. This logic extends to the situation of testing the difference of two means, described below.\nA Hypothesis Test for a Single Population/Process Mean Let\u0026rsquo;s now address a specific test. In every equation that follows, $\\mu_{0}$ is the value specified value of $\\mu$ that we are testing.\n  Specify the hypotheses\n$$\\begin{aligned} \\text{(a) }H_{0} \u0026amp;:\u0026amp;\\mu \\leq \\mu_{0}\\text{ or} \\ \\text{(b) }H_{0} \u0026amp;:\u0026amp;\\mu \\geq \\mu_{0}\\text{ or } \\ \\text{(c) }H_{0} \u0026amp;:\u0026amp;\\mu =\\mu_{0}\\end{aligned}$$\n  The alternative hypothesis is the complement of the null.\n  Calculate the test statistic\nFor a test of a single mean, the test statistic is\n$$t_{0}=\\frac{\\overline{y}-\\mu_{0}}{s/\\sqrt{n}}$$\n  Compute the rejection region\nThe the rejection region (i.e., the \u0026ldquo;critical values\u0026rdquo;) for a test with the $% \\alpha$ level of significance would be computed as follows:\n$$\\begin{aligned} \\text{(a)}{\\text{ }t_{n-1} \u0026amp;:\u0026amp;t_{n-1}\u0026gt;\\text{ }t_{\\alpha,n-1}} \\ \\text{(b) }{t_{n-1} \u0026amp;:\u0026amp;t_{n-1}\u0026lt;-t_{\\alpha,n-1}} \\ \\text{(c) }{t_{n-1} \u0026amp;:\u0026amp;|t_{n-1}|\u0026gt;\\text{ }t_{\\alpha /2,n-1}}\\end{aligned}$$\nwhere $t_{n-1}$is a Student\u0026rsquo;s $t$ random variable with $n_{d}-1$ degrees of freedom. Alternatively, we can calculate the $p$-value as\n  $$\\begin{aligned} \\text{(a) }p \u0026amp;=\u0026amp;P(t_{n-1}\u0026lt;t_{0}) \\ \\text{(b) }p \u0026amp;=\u0026amp;P(t_{n-1}\u0026gt;t_{0}) \\ \\text{(c) }p \u0026amp;=\u0026amp;2\\min [P(t_{n-1}\u0026gt;t_{0}),P(t_{n-1}\u0026lt;t_{0})]\\end{aligned}$$\nCalculating the $p$-value for a test involving the $t$ distribution requires software.\n Make a conclusion.  Here are some examples.\nA machine produces ball bearings is calibrated to produce diameters of $0.5$ inches. A sample of $10$ ball bearings from the process was taken. The average diameter was $0.493$ inches, with a standard deviation of $0.022$ inches. A quality control inspector wants to determine if the machine is not calibrated properly. Conduct a hypothesis test at the $\\alpha =0.05$ level to answer his question.\nA specialty cell phone manufacturer wants to determine how many pictures are stored on phones used by older consumers. A survey was conducted and a random sample of $6$ older consumers reported the following numbers of pictures on their cell phones:$25$ $\\ 6$ $22$ $26$ $31$ $18$. Historically, the average number of pictures stored on phones by this consumer group has been $10$. The company wants to know if this has changed. Use a hypothesis test at the $\\alpha =0.05$ level to answer this question.\nA Hypothesis Test for Two Population/Process Means Here is the procedure for testing two means.\n  Specify the hypotheses.\n$$\\begin{aligned} \\text{(a) }H_{0} \u0026amp;:\u0026amp;\\mu_{1}-\\mu_{2}\\leq d\\text{ or} \\ \\text{(b) }H_{0} \u0026amp;:\u0026amp;\\mu_{1}-\\mu_{2}\\geq d\\text{ or } \\ \\text{(c) }H_{0} \u0026amp;:\u0026amp;\\mu_{1}-\\mu_{2}\\neq d\\end{aligned}$$\nwhere $d$ is some specified difference (usually $0)$.\n  Calculate the test statistic.\nCase 1: $\\sigma_{1}=\\sigma_{2}$\n$$t_{0}=\\frac{\\overline{x}_{1}-\\overline{x}_{2}-d}{s_{p}\\sqrt{\\frac{1}{n_{1}}+% \\frac{1}{n_{2}}}}$$\nCase 2: $\\sigma_{1}\\neq \\sigma_{2}$\n$$t_{0}=\\frac{\\overline{x}_{1}-\\overline{x}_{2}-d}{\\sqrt{\\frac{s_{1}^{2}}{n_{1}% }+\\frac{s_{2}^{2}}{n_{2}}}}$$\n  Compute the p-value\nCase 1: $\\sigma_{1}=\\sigma_{2}$\n$$\\begin{aligned} \\text{(a) }p \u0026amp;=\u0026amp;P(t_{n_{1}+n_{2}-2}\u0026lt;t_{0}) \\ \\text{(b) }p \u0026amp;=\u0026amp;P(t_{n_{1}+n_{2}-2}\u0026gt;t_{0}) \\ \\text{(c) }p \u0026amp;=\u0026amp;2\\min [P(t_{n_{1}+n_{2}-2}\u0026gt;t_{0}),P(t_{n_{1}+n_{2}-2}\u0026lt;t_{0})]\\end{aligned}$$\nCase 2: $\\sigma_{1}\\neq \\sigma_{2}$\n$$\\begin{aligned} \\text{(a) }p \u0026amp;=\u0026amp;P(t_{\\nu }\u0026lt;t_{0}) \\ \\text{(b) }p \u0026amp;=\u0026amp;P(t_{\\nu }\u0026gt;t_{0}) \\ \\text{(c) }p \u0026amp;=\u0026amp;2\\min [P(t_{\\nu }\u0026gt;t_{0}),P(t_{\\nu }\u0026lt;t_{0})]\\end{aligned}$$\nwhere $\\nu =\\frac{\\left( \\frac{s_{1}^{2}}{n_{1}}+\\frac{s_{2}^{2}}{n_{2}}% \\right) ^{2}}{\\frac{\\left( \\frac{s_{1}^{2}}{n_{1}}\\right) ^{2}}{n_{1}-1}+% \\frac{\\left( \\frac{s_{2}^{2}}{n_{2}}\\right) ^{2}}{n_{2}-1}}$.\n  Make a conclusion\nIf $p\\leq \\alpha $, reject $H_{0}$. If $p\u0026gt;\\alpha $, fail to reject $H_{0}$.\n  Here are some examples:\nIn a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8$, respectively. Determine at the $% \\alpha =0.05$ level if there is a difference in the population mean amount of time spent studying per week between accounting and general business majors. Assume the population standard deviations are equal.\nIn a recent survey of $121$ undergraduates, students reported the number of hours per week they spend on school work. Figure [table1]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;table1\u0026rdquo;} below summarizes the data for finance and marketing majors.\nA researcher claims that marketing majors and finance majors do not study the same amount of time per week on average. Test the hypothesis at the $% 10%$ level of significance, assuming the two population standard deviations are not the same.\nThe critical region approach can be used for testing two means as well (and for every hypothesis test, in fact). However, because software generally reports $p$-values, and $p$-values are what are used most often in practice, we will not address this approach here.\nThis document has been a \u0026ldquo;quick and dirty\u0026rdquo; review of hypothesis testing. The basic principles discussed here will apply to all of the tests we examine in this class. I encourage you to go back to this document each time we discuss a new test so that you can see the general pattern.\n  If you want to know more about how statisticians really view the world, here\u0026rsquo;s a shameless plug for Kevin\u0026rsquo;s book: http://www.amazon.com/Understanding-Advanced-Statistical-Methods-Chapman/dp/1466512105 \u0026#x21a9;\u0026#xfe0e;\n   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"6c1d4ba53effc96aee994ae0aed2157e","permalink":"/courses/bana3363/3-1-sampling-distributions/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/bana3363/3-1-sampling-distributions/","section":"courses","summary":"Sampling Distributions Let\u0026rsquo;s get a definition out of the way. This comes very close to the \u0026ldquo;official\u0026rdquo;¬†Google definition:\n A sample is a small collection of objects whose properties are thought to correspond closely with those of a large (or even infinite) collection of items or quantities that is the main interest of the researcher.","tags":null,"title":"Sampling Distributions \u0026 Central Limit Theorem","type":"docs"},{"authors":null,"categories":null,"content":"Sampling Distributions Let\u0026rsquo;s get a definition out of the way. This comes very close to the \u0026ldquo;official\u0026rdquo;¬†Google definition:\nA sample is a small collection of objects whose properties are thought to correspond closely with those of a large (or even infinite) collection of items or quantities that is the main interest of the researcher.\nThis large collection of items or quantities about which we would like to make reasonable statements is called a population, or, more accurately, a statistical process, or just process.\nYou will see the word \u0026ldquo;population\u0026rdquo; used more often than \u0026ldquo;process\u0026rdquo; in most of the material you read despite \u0026ldquo;process\u0026rdquo; being a more accurate description of how a statistician views the natural world. The word \u0026ldquo;population\u0026rdquo; evokes images of a single, fixed collection of objects that we sample from. In much of statistics, and especially in forecasting and time series, the population view is simply wrong. In many cases, what a researcher is trying to study is an ever-changing process with many factors that can influence the actual data the researcher can see. For these situations, the process view is better 1.\nFor example, suppose wish to study income of beginning accounting professionals in the U. S. Many factors interact to produce the income population: labor markets, the government, regional events, education, etc.. You take a sample of $300$ professionals and invite them to complete a survey about income. Your goal might be to estimate the \u0026ldquo;population\u0026rdquo; mean beginning accounting salary, $\\mu ,$ using the sample of size $300$. There are two main problems. First, the population you want to make a conclusion about (all beginning accountants) is not the population you drew from (all beginning accountants who don\u0026rsquo;t mind filling out a survey). Furthermore, some respondents no doubt will (unintentionally or intentionally) give you incorrect responses, so the \u0026ldquo;population\u0026rdquo; you are sampling from is again not quite what you want. Taking the process view, though, you can view $\\mu$ as the mean of the process that produces the actual accounting salary data you see, and you can use inference to make conclusions about that process. If your study is designed properly, your results will be a reasonable representation of the \u0026ldquo;population\u0026rdquo; you are really interested in, beginning accounting graduates.\nFor purposes of doing the work in this class, however, you can use phrases such as \u0026ldquo;the population mean\u0026rdquo; (I probably will, just out of habit) and be fine. Now we need to be more specific about what a sample is.\nA random sample is a collection of $n$ independent random variables $Y_{1},Y_{2},\\ldots ,Y_{n}$ that are assumed to have been \u0026ldquo;drawn\u0026rdquo; from the same probability distribution $f(y|\\mathbf{\\theta ),}$where $\\mathbf{\\theta }$ is a generic collection of parameters. For example, if we are taking a sample from a normal distribution $\\mathbf{\\theta ={}\\mu ,\\sigma }.$\nFigure [sampling]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;sampling\u0026rdquo;} illustrates this idea. Essentially, before the researcher gathers data, he or she imagines that some (usually unknown) probability distribution produces data from which he or she will draw a single sample of size, say, $300$. When the data are actually gathered, the researcher takes the view that he or she is observing one possible realization of those $300$ random variables.\nThe key to understanding statistical inference is that different samples produce different values of calculated statistics such as the sample mean and sample standard deviation, but these values behave in a predictable way. Although we only work with one sample, we use the properties, we use the fact that the samples behave in a predictable way to make the leap from any one sample to the population or process of interest.\nA sampling distribution is the distribution of a calculated statistic (for example, the sample mean) when we imagine taking repeated samples from the same process and calculating the statistic over and over.\nThere is an entire distribution of values because each random sample of size $n$ will have different data in it. So, a statistic is actually a random variable too. Therefore, when we talk about a statistic like the sample mean in general terms, we will use a capital letter, $\\overline{Y}$ , and when we talk about a specific value of the statistic, like $2.3,$ we will use lowercase, as in $\\overline{y}=2.3.$ A good illustration of this concept is found here http://onlinestatbook.com/stat_sim/sampling_dist/ , courtesy of our friends at Rice University.\nWe can summarize the distribution like we have before using expected value and variance. It turns out that if we choose the right statistic, we can make some powerful conclusions about the population. Here are some cool facts.\n[[samp_dist]]{#samp_dist label=\u0026quot;samp_dist\u0026rdquo;} For a random sample of size $n$ from a process (or population), the sample mean, $\\overline{Y}$, is a random variable with mean $=E(\\overline{Y})=% %TCIMACRO{\\U{3bc} }% %BeginExpansion \\mu %EndExpansion$ (the process/population mean) and variance $V(\\overline{Y})=$ $\\frac{% \\sigma ^{2}}{n},$ the population variance divided by $n.$\nStand by for the most important fact in statistics.¬†It\u0026rsquo;s so important that it deserves its own section.\nSampling Distribution of the Sample Mean [[CLT]]{#CLT label=\u0026quot;CLT\u0026rdquo;}(The Central Limit Theorem ) The sampling distribution of the sample mean $\\overline{Y}$ is approximately a normal distribution with mean $=\\mu$ and variance $\\frac{\\sigma ^{2}}{n}$, no matter what the original distribution looks like, as the sample size $n$ gets large.\n\u0026ldquo;Large\u0026rdquo; here technically refers to letting the sample size go to infinity, but in most practical cases, the Central Limit Theorem (CLT) applies when $n$ is as small as $30.$\nIf we assume the sample is drawn from a normal distribution, then the distribution of $\\overline{Y}$ is normal regardless of sample size.\nThe CLT is the reason we don\u0026rsquo;t really have to worry about what the population distribution looks like in Figure [sampling]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;sampling\u0026rdquo;}. It says that the distribution can take any shape and the distribution of the sample average will still be approximately normal. The CLT is what makes most of statistics work for practical problems.\nWe can use the distribution of the sample mean to make conclusions about a sample. For example, we can ask, what is $P(\\overline{Y}\u0026gt;4)?$ To answer this question, we can use our old friend the $z$-score. Remember that a $z$-score is, in general, $$\\frac{\\text{observation }-\\text{ mean}}{\\text{standard deviation}}.$$\nZ-scores for sample averages are found by subtracting the mean and dividing by the standard deviation, which are given in Theorem [samp_dist]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;samp_dist\u0026rdquo;}:\n$$Z=\\frac{\\overline{Y}-\\mu }{\\sigma /\\sqrt{n}}$$\nHere are some examples to try out.\nThe foreman of a bottling plant has observed that the amount of soda in each \u0026ldquo;$32$-ounce\u0026rdquo;¬†bottle is actually a normally distributed random variable, with a mean of $32$ ounces and a standard deviation of $0.3$ ounce. If the foreman is doing quality control by taking samples of $n$ $=4$ bottles from the line and calculating the average fill, would a reading of $32.5$ be unusually high? Answer by finding $P(\\overline{Y}\u0026gt;32.5).$ How would your answer change if we took away the assumption that the amount of soda in each bottle comes from a normal distribution?\n\\vspace{1.5in} You take a random sample of size $50$ from a population with mean $\\mu =16$ and standard deviation $\\sigma =$ $8$.\na). What are the mean and standard deviation of $\\overline{Y}$, the sample mean?\nb). Sketch the distribution, indicating the intervals within which $68%,$ $% 95%$, and $99.7%$ of the observations are expected to lie.\nc). Find $P(\\overline{Y}\u0026gt;18)$\nd). Find $P(17\u0026lt;\\overline{Y}\u0026lt;19.5)$\ne). Explain why all of these probabilities are approximate.\n\\vspace{0.4in} In a random sample of $20$ college students, the average reported working time was $15.4$ hours per week. A researcher claims that the true working time of college students is normally distributed with a mean of $16$ hours and a standard deviation of $2.5$ hours. Let $\\overline{Y}$ denote the sample mean.\na). What is $P(\\overline{Y}\u0026lt;15.4)$?\nb). Using your answer to (a), how reasonable do you think the researcher\u0026rsquo;s claim is?\nc). Using the $68-95-99.7$ rule, find a set of sample averages that would be unusual to see (either \u0026ldquo;too large\u0026rdquo; or \u0026ldquo;too small\u0026rdquo;) if the true mean and standard deviation of college student working hours were as the researcher claims.\n\\vspace{1.5in} In the next section, we will discuss how we can use the sampling distribution of the sample mean to make inferences about $\\mu .$\nInterval Inference about a Single Population (Process) Mean Basic Concepts Our ultimate goal is to obtain a reasonable \u0026quot; guess\u0026rdquo;¬†at the true value of a process parameter (in this section, $\\mu$) using a sample of data. Before we do that, we have to introduce some terms.\nA**¬†point estimator** for a parameter is a particular function of the random sample that gives a single number that we hope is \u0026quot; close\u0026rdquo;¬†to the true value of the parameter.\nAn interval estimator is a range of values, constructed using observed data, within which the parameter is believed to fall.\nThis sounds very technical, but it really is not so bad. An example of a point estimator is the sample mean: $\\overline{Y}=\\frac{\\sum Y_{i}}{n}.$ Notice how it is a function of the data (the values of $Y_{i})$. In practice, we typically use the point estimator to construct an interval estimator.\nIn theory, there are an infinite number of point estimators we could pick to estimate $\\mu$, but it turns out that only one, $\\overline{Y}$, is \u0026ldquo;the best.\u0026rdquo; The following criteria are used to judge whether a point estimator is \u0026ldquo;good:\u0026rdquo;\n  Unbiasedness \u0026ndash; The average of the estimator is equal to the population parameter.\n  Consistency \u0026ndash; The estimator gets closer and closer to the true population parameter as the sample size increases\n  Relative efficiency \u0026ndash; Given two different estimators with the same sample size, we choose the one with the smaller variance.\n  We know from Theorem [samp_dist]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;samp_dist\u0026rdquo;} that the sample mean $% \\overline{Y}$ is unbiased because the theorem states that $E(\\overline{Y})=% %TCIMACRO{\\U{3bc} }% %BeginExpansion \\mu %EndExpansion .$ The other two properties are also true, but we won\u0026rsquo;t prove it; just take my word for it. Also, it can be shown that $\\overline{Y}$ is has the smallest variability among all unbiased estimators, so when we use it, we are using \u0026ldquo;the best\u0026rdquo; guess of $\\mu .$\nA Confidence Interval for the Mean When $\\sigma$ Is Known \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; By the Central Limit Theorem, Theorem [CLT]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;CLT\u0026rdquo;}, we know that the distribution of $\\overline{Y}$ is approximately $N(\\mu ,\\sigma /\\sqrt{n})$. This implies that we can predict something about the behavior of any given sample average $\\overline{x}.$ Specifically, we know that a sample average will have about a $68%$ chance of being within one standard deviation of $% \\mu$ about a $95%$ chance of being within two standard deviations of $\\mu$ , and about a $99.7%$ chance of being within three standard deviations of $% \\mu .$\nConfidence intervals flip this logic around and use the fact that if $% \\overline{Y}$ has a $68%$ chance of being within one standard deviation of $% \\mu$, then we can also say that $\\mu$ has a $68%$ chance of being within one standard deviation of $\\overline{Y}.$ This means that\n$$P(\\mu -2\\sigma /\\sqrt{n}\u0026lt;\\overline{Y}\u0026lt;\\mu +2\\sigma /\\sqrt{n})$$\nand\n$$P(\\overline{Y}-2\\sigma /\\sqrt{n}\u0026lt;\\mu \u0026lt;\\overline{Y}+2\\sigma /\\sqrt{n})$$\nare equal. One statement can be made from the other simply by rearranging what goes in the middle using algebra.\nHere\u0026rsquo;s a picture that might make things clearer.\nWe don\u0026rsquo;t know what $\\mu$ is, but we know that $\\overline{Y}$ should fall around it in a predictable way. So we use this idea to \u0026ldquo;guess\u0026rdquo; at the location of $\\mu .$ If we were to take the range of numbers $% \\overline{x}-2\\sigma /\\sqrt{n}$ to $\\overline{x}+2\\sigma /\\sqrt{n}$ as our interval estimator, about $95%$ of the time, the true value of $\\mu$ would fall in the interval; about $5%$ of the time it wouldn\u0026rsquo;t. So we could say we were $95%$ confident in the interval.\nWhat if we want a different level of confidence than $68,95,$ or $99.7%?$¬†Suppose we wanted to be $98%$ confident, for example? Recall from Handout 2 that $P(-z_{\\alpha /2}\u0026lt;Z\u0026lt;z_{\\alpha /2})=1-\\alpha .$ If we want $98%$ confidence, we¬†begin by setting $1-\\alpha =0.98$, which means $\\alpha =0.02.$ Then $z_{\\alpha /2}=z_{0.02/2}=z_{0.01}=2.33.$ Then we substitute $Z=\\frac{% \\overline{Y}-\\mu }{\\sigma /\\sqrt{n}}$ in the middle and solve for $\\mu :$\n$$\\begin{aligned} P(-2.33 \u0026amp;\u0026lt;\u0026amp;Z\u0026lt;2.33)=0.98 \\ \u0026amp;\\Rightarrow \u0026amp;P(-2.33\u0026lt;\\frac{\\overline{Y}-\\mu }{\\sigma /\\sqrt{n}}\u0026lt;2.33)=0.98 \\ \u0026amp;\\Rightarrow \u0026amp;P(-2.33\\sigma /\\sqrt{n}\u0026lt;\\overline{Y}-\\mu \u0026lt;2.33\\sigma /\\sqrt{n}% )=0.98 \\ \u0026amp;\\Rightarrow \u0026amp;P(-2.33\\sigma /\\sqrt{n}+\\overline{Y}\u0026lt;-\\mu \u0026lt;2.33\\sigma /\\sqrt{n}% -\\overline{Y})=0.98 \\ \u0026amp;\u0026amp;\\text{Multiply by }-1\\text{ and rearrange to get} \\ P(\\overline{Y}-2.33\\sigma /\\sqrt{n} \u0026amp;\u0026lt;\u0026amp;\\mu \u0026lt;\\overline{Y}+2.33\\sigma /\\sqrt{n}% )=0.98\\end{aligned}$$\nYou can apply these steps for any confidence level you want. Therefore, we have the following:\n[[CI_sig_known]]{#CI_sig_known label=\u0026quot;CI_sig_known\u0026rdquo;}A $(1-\\alpha )100%$ confidence interval for $\\mu$ when $\\sigma$ is known (or when $n$ is large) is given by\n$$\\overline{x}\\pm z_{\\alpha /2}\\frac{\\sigma }{\\sqrt{n}}$$\nCalculation is easy with software. Interpretation is key. We view the mean $% %TCIMACRO{\\U{3bc} }% %BeginExpansion \\mu %EndExpansion$ as a fixed but unknown quantity. Once we gather data and compute the sample mean $\\overline{x}$ and the associated confidence interval, the interval either contains $\\mu$ or it does not (see Figure [Figure_c_int]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;Figure_c_int\u0026rdquo;} above). In the long run, however, if you took repeated samples of size $n$ and computed the interval each time, $(1-\\U{3b1} )100%$ of them would contain $\\mu$ . Thus, the \u0026ldquo;confidence\u0026rdquo;¬†you have is in the method used to construct an interval, not in the particular interval you have constructed.\nRarely do we know $\\sigma$, but at times we have fairly large samples, so we can use Definition [CI_sig_known]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;CI_sig_known\u0026rdquo;} anyway by just replacing $\\sigma$ with the sample standard deviation $s.$ If we have a small sample and do not know $\\sigma$, we must use some other multiplier other than $z_{\\alpha /2}.$ For this, we use values from Student\u0026rsquo;s t distribution.\nA Confidence Interval for the Mean When $\\sigma$ Is Unknown Student\u0026rsquo;s $t$ distribution with $\\nu$ degrees of freedom is the probability distribution of the quantity\n$$t=\\frac{\\overline{Y}-\\mu }{s/\\sqrt{n}}$$\nAs with the standard normal distribution, it is symmetric, bell-shaped, and centered at $0.$ The degrees of freedom $\\nu$ is a parameter that governs the width of the distribution. The notation $t_{\\nu }$ refers to a random variable from a $t$ distribution with $\\nu$ degrees of freedom.\nHere\u0026rsquo;s a picture. As you can see in Figure [studt]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;studt\u0026rdquo;}, Student\u0026rsquo;s $t$ distribution looks a lot like the standard normal. In fact, it can be shown that as $\\nu$ gets very large, the the $t$ distribution \u0026ldquo;becomes\u0026rdquo; the standard normal distribution. That\u0026rsquo;s why we can use values of $z$ when $n$ is large in a confidence interval.\nUsing the $t$ distribution, we can define a confidence interval as follows:\n$(1-\\alpha )100%$ confidence interval for $\\mu$ when $\\sigma$ is unknown (or when $n$ is small) is given by\n$$\\overline{x}\\pm t_{n-1,\\alpha /2}\\frac{s}{\\sqrt{n}}$$\nwith $t_{n-1,\\alpha /2},$ being the value of a $t$ distribution with $n-1$ degrees of freedom that has probability $\\alpha /2$ to its right.\nHere are some examples.\nA recent survey asked, \u0026ldquo;What is the ideal number of children for a person to have?\u0026rdquo;¬†A sample of $15$ American adults aged $20$ to $30$ reported the following numbers: $1$ $\\ \\ 2$ $\\ \\ 0$ $\\ \\ 3$ $\\ \\ 4$ $\\ \\ 2$ $\\ \\ 0$ $\\ 1$ $\\ \\ 3$ $\\ \\ 4$ $\\ \\ 2$ $\\ \\ 2$ $\\ \\ 2$ $\\ \\ 3$ $\\ \\ 8.$ Find a $93%$ confidence interval for the mean number of children American adults in this age group believe is ideal.\nIn a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8,$ respectively. Calculate $95%$ confidence intervals for the amount of time spent studying per week for both accounting and general business majors. Interpret the intervals in context.\nIn the next subsection, we address the situation in which you wish examine two population/process means.\nInterval Inference about Two Population/Process Means Many times we are not interested only in a single group, but multiple $(\\geq 2)$ groups. For example, we might compare the amount of weight lost for two groups to determine the effect of a new drug on adult men $45-60$. Let Group $1$ be men 45-60 who follow a specific diet and let Group $2$ be men $45-60$ who use the same diet but also take a weight loss drug. In the language of experimental design, Group $1$ is a control group, the group that receives no treatment (i.e., a specific manipulation of the members of the group), or no treatment above a reasonable standard. Group $2$ is an experimental or treatment group.\nBy the fundamental assumption of statistics (that variability is inescapable), any one person in Group $1$ could lose more weight than any one person in Group $2$, even if the drug is effective, simply by random chance. The question is not whether the drug works for everyone, but if it \u0026ldquo;works on average.\u0026rdquo;¬†The average for Group $1$ we can specify as $\\mu _{1}$ and the average for Group 2 we can specify as $\\mu _{2}.$ Then the question is whether $\\mu _{1}=\\mu _{2},$ or, equivalently, whether $\\mu _{1}-\\mu _{2}=0.$ We can address this question either through a confidence interval or a hypothesis test. We will discuss the first of these here. There are two cases to consider:\n  You know (or assume) that the two population standard deviations are the same, that is, $\\sigma _{1}=\\sigma _{2}$\n  You don\u0026rsquo;t know (or don\u0026rsquo;t wish to assume) that the two population standard deviations are the same, that is $\\sigma _{1}\\neq \\sigma _{2}$\n  Case 1: Confidence Interval for Difference in Means $\\sigma _{1}=\\sigma _{2}$ A $(1-\\alpha )100%$ confidence interval for $\\mu {1}-\\mu {2}$ is given by $\\overline{x}{1}-\\overline{x}{2}\\pm t_{n_{1}+n_{2}-2,\\alpha /2}s_{p}\\sqrt{% \\frac{1}{n_{1}}+\\frac{1}{n_{2}}}$, where $s_{p}=\\sqrt{\\frac{% (n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2}}$ is the \u0026ldquo;pooled\u0026rdquo; standard deviation from the two samples.\nCase 2: Confidence Interval for Difference in Means $\\sigma _{1}\\neq \\sigma _{2}$ A $(1-\\alpha )100%$ confidence interval for $\\mu {1}-\\mu {2}$ is given by $\\overline{x}{1}-\\overline{x}{2}\\pm t_{\\nu ,\\alpha /2}\\sqrt{\\frac{s_{1}^{2}% }{n_{1}}+\\frac{s_{2}^{2}}{n_{2}}},$ where $\\nu =\\frac{\\left( \\frac{s_{1}^{2}% }{n_{1}}+\\frac{s_{2}^{2}}{n_{2}}\\right) ^{2}}{\\frac{\\left( \\frac{s_{1}^{2}}{% n_{1}}\\right) ^{2}}{n_{1}-1}+\\frac{\\left( \\frac{s_{2}^{2}}{n_{2}}\\right) ^{2}% }{n_{2}-1}}$ .\nThe Case 2 interval makes one less assumption than Case 1, so it is closer to the truth (in reality, it\u0026rsquo;s unlikely that the two standard deviations will be exactly equal). On the other hand, it is known that the Case 2 interval is only an approximate interval, although the approximation is quite good. As with most things in life, there is a trade-off. In general, however, Case 2 is the safer bet. The degrees of freedom parameter, $\\nu ,$ is easily calculated using software.\nHere\u0026rsquo;s an example:\nIn a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8,$ respectively. Calculate a $90%$ confidence interval for mean difference in the amount of time spent studying per week between accounting and general business majors. Interpret the interval in context. Assume the population standard deviations are equal.\nIn the next section, we\u0026rsquo;ll address another form of inference for means, hypothesis testing.\nHypothesis Testing Inference for a Single Population/Process Mean Basic Concepts Let\u0026rsquo;s begin with a definition.\nA hypothesis test (also called a significance test ) is a method of using data to evaluate the evidence about a specified value of a parameter.\nEvery statistical test has two mutually exclusive hypotheses, or claims about the value of a parameter.\nThe null hypothesis, denoted $H_{0}$ and pronounced \u0026ldquo;$H$ not\u0026rdquo;¬†or \u0026ldquo;$H$ sub-zero,\u0026rdquo;¬†is typically a statement of \u0026quot; no effect,\u0026rdquo;¬†\u0026ldquo;no difference,\u0026rdquo;¬†\u0026ldquo;no special ability beyond random chance,\u0026rdquo;¬†\u0026quot; equality,\u0026rdquo;¬†etc. This is usually what the researcher wants to disprove, or reject.\nThe alternative hypothesis, denoted $H_{a}$ or $H_{1}$, is the complement (\u0026ldquo;opposite\u0026rdquo;) of the null, and usually what the researcher is trying to establish.\nHypotheses always come in pairs, and the general forms of the hypothesis pairs are given in Figure [hyps]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;hyps\u0026rdquo;}.\nWhen performing a hypothesis test, it is possible to be wrong, or, more formally, to commit an error. Here are the two types of errors we\u0026rsquo;ll be concerned with (there is a third type as well but we won\u0026rsquo;t worry about that one).\nA Type I error occurs when we reject a true null hypothesis. This is akin to a \u0026ldquo;false positive\u0026rdquo; (claiming there is an effect or difference when there isn\u0026rsquo;t).\nA Type II error occurs when we don\u0026rsquo;t reject a false null hypothesis. This is akin to a \u0026ldquo;false negative\u0026rdquo; (claiming there is not an effect or difference when there is).\nFor any given test, we don\u0026rsquo;t know if we have committed a Type I error because we don\u0026rsquo;t know what the true value of the parameter is (if we did, why do a test at all?). What we can do is specify that we want to construct our test such that the $P($Type¬†I error$)$ $=\\alpha$ is \u0026ldquo;small.\u0026rdquo; That is, if we could imagine doing the test repeatedly for different samples from the same population, in the long run, we would only want to reject $H_{0}$ $% \\alpha (100)%$ of the time. A similar criterion exists for a Type II error: we want $P($Type II error) $=\\beta$ to be small as well.\nThe two types of errors cannot be minimized at the same time; lowering one error probability raises the other. What is done is to find tests with a small $\\alpha$, and from that set, find the test that also has a small $% \\beta .$ For our purposes, we will only consider Type I error. It can be shown that the hypothesis tests presented next also have small $\\beta .$\nHypothesis testing can be thought of as a process with a number of steps. Here they are:\n  Specify the hypothesis\n  Calculate the test statistic\n  Compute the p-value or rejection region\n  Make a conclusion\n  We already talked about (1) in Figure [hyps]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;hyps\u0026rdquo;}. Here are some more definitions:\nA test statistic is a specific function of data and the parameter value specified by the null hypothesis.\nThe p-value of a test is the probability, assuming the null is true, that we would observe a value of the test statistic as extreme or more extreme than what we actually got.\nWe make a conclusion based on the $p$-value or the rejection region. $P$ -values are more commonly used in practice. The judgement is simple: if the $% p$-value $\\leq$ $\\alpha$ (i.e., our Type I¬†error probability)$,$then we reject the null hypothesis; if the $p$-value $\u0026gt;\\alpha ,$ we fail to reject the null hypothesis. Most software programs report $p$-values automatically.\nAnother approach is to specify, before examining the data, the $% \\alpha$ (Type I error probability) that we want and determine a critical value, a value of test statistic distribution under the null hypothesis such that if the test statistic falls within that region, we reject the null hypothesis. A visual of this idea is shown in Figure [rr]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;rr\u0026rdquo;} for a test of a mean. If the calculated value of the test statistic falls in the shaded region, we would reject the null hypothesis.\nHere is an important fact.\nIf a $(1-\\alpha )100%$ confidence interval contains the value of $% H_{0},$ then a test of $H_{0}:\\mu =\\mu _{0}$ conducted at the $\\alpha$ level will **fail to reject** the null hypothesis.\nThis fact can be proven but the intuition is simple: if the interval contains the most reasonable values of the parameter (based on the data and the confidence level) then it\u0026rsquo;s sensible that the test should not reject one of the values that lies inside the interval. This logic extends to the situation of testing the difference of two means, described below.\nA Hypothesis Test for a Single Population/Process Mean Let\u0026rsquo;s now address a specific test. In every equation that follows, $\\mu _{0}$ is the value specified value of $\\mu$ that we are testing.\n  Specify the hypotheses\n$$\\begin{aligned} \\text{(a) }H_{0} \u0026amp;:\u0026amp;\\mu \\leq \\mu _{0}\\text{ or} \\ \\text{(b) }H_{0} \u0026amp;:\u0026amp;\\mu \\geq \\mu _{0}\\text{ or } \\ \\text{(c) }H_{0} \u0026amp;:\u0026amp;\\mu =\\mu _{0}\\end{aligned}$$\n  The alternative hypothesis is the complement of the null.\n  Calculate the test statistic\nFor a test of a single mean, the test statistic is\n$$t_{0}=\\frac{\\overline{y}-\\mu _{0}}{s/\\sqrt{n}}$$\n  Compute the rejection region\nThe the rejection region (i.e., the \u0026ldquo;critical values\u0026rdquo;) for a test with the $% \\alpha$ level of significance would be computed as follows:\n$$\\begin{aligned} \\text{(a)}{\\text{ }t_{n-1} \u0026amp;:\u0026amp;t_{n-1}\u0026gt;\\text{ }t_{\\alpha ,n-1}} \\ \\text{(b) }{t_{n-1} \u0026amp;:\u0026amp;t_{n-1}\u0026lt;-t_{\\alpha ,n-1}} \\ \\text{(c) }{t_{n-1} \u0026amp;:\u0026amp;|t_{n-1}|\u0026gt;\\text{ }t_{\\alpha /2,n-1}}\\end{aligned}$$\nwhere $t_{n-1}$is a Student\u0026rsquo;s $t$ random variable with $n_{d}-1$ degrees of freedom. Alternatively, we can calculate the $p$-value as\n  $$\\begin{aligned} \\text{(a) }p \u0026amp;=\u0026amp;P(t_{n-1}\u0026lt;t_{0}) \\ \\text{(b) }p \u0026amp;=\u0026amp;P(t_{n-1}\u0026gt;t_{0}) \\ \\text{(c) }p \u0026amp;=\u0026amp;2\\min [P(t_{n-1}\u0026gt;t_{0}),P(t_{n-1}\u0026lt;t_{0})]\\end{aligned}$$\nCalculating the $p$-value for a test involving the $t$ distribution requires software.\n Make a conclusion.  Here are some examples.\nA machine produces ball bearings is calibrated to produce diameters of $0.5$ inches. A sample of $10$ ball bearings from the process was taken. The average diameter was $0.493$ inches, with a standard deviation of $0.022$ inches. A quality control inspector wants to determine if the machine is not calibrated properly. Conduct a hypothesis test at the $\\alpha =0.05$ level to answer his question.\nA specialty cell phone manufacturer wants to determine how many pictures are stored on phones used by older consumers. A survey was conducted and a random sample of $6$ older consumers reported the following numbers of pictures on their cell phones:$25$ $\\ 6$ $\\ \\ 22$ $\\ \\ 26$ $\\ \\ 31$ $\\ \\ 18.$ Historically, the average number of pictures stored on phones by this consumer group has been $10.$ The company wants to know if this has changed. Use a hypothesis test at the $\\alpha =0.05$ level to answer this question.\nA Hypothesis Test for Two Population/Process Means Here is the procedure for testing two means.\n  Specify the hypotheses.\n$$\\begin{aligned} \\text{(a) }H_{0} \u0026amp;:\u0026amp;\\mu _{1}-\\mu _{2}\\leq d\\text{ or} \\ \\text{(b) }H_{0} \u0026amp;:\u0026amp;\\mu _{1}-\\mu _{2}\\geq d\\text{ or } \\ \\text{(c) }H_{0} \u0026amp;:\u0026amp;\\mu _{1}-\\mu _{2}\\neq d\\end{aligned}$$\nwhere $d$ is some specified difference (usually $0).$\n  Calculate the test statistic.\nCase 1: $\\sigma _{1}=\\sigma _{2}$\n$$t_{0}=\\frac{\\overline{x}_{1}-\\overline{x}_{2}-d}{s_{p}\\sqrt{\\frac{1}{n_{1}}+% \\frac{1}{n_{2}}}}$$\nCase 2: $\\sigma _{1}\\neq \\sigma _{2}$\n$$t_{0}=\\frac{\\overline{x}_{1}-\\overline{x}_{2}-d}{\\sqrt{\\frac{s_{1}^{2}}{n_{1}% }+\\frac{s_{2}^{2}}{n_{2}}}}$$\n  Compute the p-value\nCase 1: $\\sigma _{1}=\\sigma _{2}$\n$$\\begin{aligned} \\text{(a) }p \u0026amp;=\u0026amp;P(t_{n_{1}+n_{2}-2}\u0026lt;t_{0}) \\ \\text{(b) }p \u0026amp;=\u0026amp;P(t_{n_{1}+n_{2}-2}\u0026gt;t_{0}) \\ \\text{(c) }p \u0026amp;=\u0026amp;2\\min [P(t_{n_{1}+n_{2}-2}\u0026gt;t_{0}),P(t_{n_{1}+n_{2}-2}\u0026lt;t_{0})]\\end{aligned}$$\nCase 2: $\\sigma _{1}\\neq \\sigma _{2}$\n$$\\begin{aligned} \\text{(a) }p \u0026amp;=\u0026amp;P(t_{\\nu }\u0026lt;t_{0}) \\ \\text{(b) }p \u0026amp;=\u0026amp;P(t_{\\nu }\u0026gt;t_{0}) \\ \\text{(c) }p \u0026amp;=\u0026amp;2\\min [P(t_{\\nu }\u0026gt;t_{0}),P(t_{\\nu }\u0026lt;t_{0})]\\end{aligned}$$\nwhere $\\nu =\\frac{\\left( \\frac{s_{1}^{2}}{n_{1}}+\\frac{s_{2}^{2}}{n_{2}}% \\right) ^{2}}{\\frac{\\left( \\frac{s_{1}^{2}}{n_{1}}\\right) ^{2}}{n_{1}-1}+% \\frac{\\left( \\frac{s_{2}^{2}}{n_{2}}\\right) ^{2}}{n_{2}-1}}.$\n  Make a conclusion\nIf $p\\leq \\alpha ,$ reject $H_{0}.$ If $p\u0026gt;\\alpha ,$ fail to reject $H_{0}.$\n  Here are some examples:\nIn a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8,$ respectively. Determine at the $% \\alpha =0.05$ level if there is a difference in the population mean amount of time spent studying per week between accounting and general business majors. Assume the population standard deviations are equal.\nIn a recent survey of $121$ undergraduates, students reported the number of hours per week they spend on school work. Figure [table1]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;table1\u0026rdquo;} below summarizes the data for finance and marketing majors.\nA researcher claims that marketing majors and finance majors do not study the same amount of time per week on average. Test the hypothesis at the $% 10%$ level of significance, assuming the two population standard deviations are not the same.\nThe critical region approach can be used for testing two means as well (and for every hypothesis test, in fact). However, because software generally reports $p$-values, and $p$-values are what are used most often in practice, we will not address this approach here.\nThis document has been a \u0026ldquo;quick and dirty\u0026rdquo; review of hypothesis testing. The basic principles discussed here will apply to all of the tests we examine in this class. I encourage you to go back to this document each time we discuss a new test so that you can see the general pattern.\n  If you want to know more about how statisticians really view the world, here\u0026rsquo;s a shameless plug for Kevin\u0026rsquo;s book: http://www.amazon.com/Understanding-Advanced-Statistical-Methods-Chapman/dp/1466512105 \u0026#x21a9;\u0026#xfe0e;\n   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"e7a47e4299db6a93870b62b6efbabe3e","permalink":"/courses/bana3363/3-2-sampling-distributions-clt/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/bana3363/3-2-sampling-distributions-clt/","section":"courses","summary":"Sampling Distributions Let\u0026rsquo;s get a definition out of the way. This comes very close to the \u0026ldquo;official\u0026rdquo;¬†Google definition:\nA sample is a small collection of objects whose properties are thought to correspond closely with those of a large (or even infinite) collection of items or quantities that is the main interest of the researcher.","tags":null,"title":"Sampling Distributions, the Central Limit Theorem, Inference about Means","type":"docs"},{"authors":null,"categories":null,"content":"Hypothesis Test for a Proportion (Large Sample) Just as the case with means, we might be interested in coming up with a hypothesis test for the population proportion. There are two approaches we could take. The most common is to use the approximate normal sampling distribution of the sample proportion for large sample sizes. This method is the one presented in most introductory books. Alternatively, we could work with the exact distribution of the sample proportion. However, this approach does not lend itself well to hand calculation, and the benefits of an exact test diminish as the sample size becomes larger1.\nThe general four-step procedure still applies. What changes are the specifics.¬†Let\u0026rsquo;s see how.\n  Specify the hypothesis:\nNow instead of $\\mu$ we work with the population proportion, $p:$\n$$\\begin{aligned} \\text{(a) }H_{0} \u0026amp;:\u0026amp;p\\leq p_{0}\\text{ vs. }H_{1}:p\u0026gt;p_{0} \\ \\text{(b) }H_{0} \u0026amp;:\u0026amp;p\\geq p_{0}\\text{ vs. }H_{1}:p\u0026lt;p_{0}\\text{ } \\ \\text{(c) }H_{0} \u0026amp;:\u0026amp;p=p_{0}\\text{ vs. }H_{1}:p\\neq p_{0}\\end{aligned}$$\n  Calculate the test statistic\nThe test statistic will always be a standard normal $z$ statistic since we assume $n$ is large:\n$$z=\\frac{\\widehat{p}-p_{0}}{\\sqrt{p_{0}(1-p_{0})/n}}$$\nWhen the null hypothesis is true, the test statistic has the standard normal distribution. Notice that we use $p_{0}$ in the denominator rather than $% \\widehat{p}.$ This is because we always assume $H_{0}$ is true when we calculate the test statistic. If $H_{0}$ is true, $p_{0}$ is the true population proportion by definition.\n  Compute the p-value or rejection region\n$$\\begin{aligned} \\text{(a) }pval \u0026amp;=\u0026amp;P(Z\u0026gt;z_{0}) \\ \\text{(b) }pval \u0026amp;=\u0026amp;P(Z\u0026lt;z_{0}) \\ \\text{(c) }pval \u0026amp;=\u0026amp;2\\min [P(Z\u0026gt;z_{0}),P(Z\u0026lt;z_{0})]\\end{aligned}$$\nThe the rejection region (i.e., the \u0026ldquo;critical values\u0026rdquo;) for a test with the $% \\alpha$ level of significance would be computed as follows:\n$$\\begin{aligned} \\text{(a)}{\\text{ }z \u0026amp;:\u0026amp;z\u0026gt;\\text{ }z_{\\alpha }} \\ \\text{(b) }{z \u0026amp;:\u0026amp;z\u0026lt;-z_{\\alpha }} \\ \\text{(c) }{z \u0026amp;:\u0026amp;|z|\u0026gt;\\text{ }z_{\\alpha /2}}\\end{aligned}$$\nAs an example, the rejection regions for a test at the $\\alpha =0.05$ level are depicted in Figures [left_tail]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;left_tail\u0026rdquo;}, [right_tail]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;right_tail\u0026rdquo;}, and [two_tail]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;two_tail\u0026rdquo;} . Make a conclusion    If the $p$-value $\\leq \\alpha ,$ we reject $H_{0}.$ If the $p$-value $% \u0026gt;\\alpha ,$ we fail to reject $H_{0}.$ Equivalently, if $z$ lies within the rejection region, we reject $H_{0}$. If $z$ lies outside the rejection region, we fail to reject $H_{0}.$\nLet\u0026rsquo;s go back to the issue of marijuana legalization. The latest General Social Survey, which was conducted in $2012$, gives the following breakdown on the question of whether marijuana should be made legal or not.\n   Response (2012; all ages) Count of Response (2012; all ages)     LEGAL $586$   NOT LEGAL $648$   TOTAL $1234$    .\nIf we take LEGAL as the event of interest, then we can calculate the sample proportion as\n$$\\widehat{p}=\\frac{586}{1234}=0.475$$\nSuppose a research firm wishes to see if the true proportion of individuals who favor legalization is less than $0.50$ using an $\\alpha =0.01$ level of significance. Then we have the following hypotheses:\n$$H_{0}:p\\geq 0.50\\text{ \\ \\ \\ vs. \\ \\ \\ }H_{1}:p\u0026lt;0.50$$\nThe test statistic is calculated as\n$$z=\\frac{0.475-0.50}{\\sqrt{0.50(1-0.50)/1234}}=-1.76$$\nThe p-value can be calculated as\n$$\\begin{aligned} P(Z \u0026amp;\u0026lt;\u0026amp;-1.76) \\ \u0026amp;=\u0026amp;0.0392\\end{aligned}$$\nAlternatively, the rejection region is defined as all values of $z$ that are greater than $z_{0.01}=2.33,$ as shown in Figure [rr1]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;rr1\u0026rdquo;}:\nUsing either method, we would fail-to-reject $H_{0}$ and state that we do not have enough evidence to conclude that the proportion of Americans favoring legalization exceeds $0.50.$\nLet\u0026rsquo;s work some more examples of this.\nThe number of individuals for and against legalization of marijuana in the $% 30$ and older age group is given in the table below. Conduct an $\\alpha =0.05$ test of the hypothesis that the proportion of adults over $30$ who favor legalization is less than $0.50.$\n   Response (2012; 30 and above) Count of Response (2012; 30 and above)     LEGAL $476$   NOT LEGAL $555$   TOTAL $1031$    In a recent survey of $260$ undergraduates $83$ indicated that they had at least one tattoo. A cultural researcher wants to test whether the true proportion of undergraduates who have tattoos is different from $38%,$ the figure given in a recent report by the Pew Research Center2. Conduct a hypothesis test at the $\\alpha =0.10$ level of significance.\nThe Centers for Disease Control and Prevention (CDC) claims 3 that $18%$ of all crashes involve some sort of \u0026ldquo;distracted driving,\u0026rdquo; which is defined as driving while engaged in another activity (such as texting, eating, and using GPS devices) that steals the driver\u0026rsquo;s attention from the road. An insurance investigator takes a random sample of $450$ crash records from one particular state and determines that $90$ indicated the primary cause of the accident was a distracted driver. Does the investigator have enough evidence at the $0.01$ level of significance to conclude that the proportion of distracted drivers in this state differs from the CDC claim?\nAn attorney representing a group of people who took Drug X and experienced a particularly severe side effect is interested in determining whether the true proportion of the population who might take the drug and who would experience the side effect is greater than $0.02,$ the proportion reported by the drug company. After obtaining side effect reports for a random sample of $400$ patients who took the drug, she finds $11$ reported cases of the side effect. Test the hypothesis at the $\\alpha =0.05$ level\n  A good overview of the various methods of forming confidence intervals for proportions can be found on Wikipedia: http://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval \u0026#x21a9;\u0026#xfe0e;\n http://www.pewsocialtrends.org/files/2010/10/millennials-confident-connected-open-to-change.pdf \u0026#x21a9;\u0026#xfe0e;\n http://www.cdc.gov/Motorvehiclesafety/Distracted_Driving/ \u0026#x21a9;\u0026#xfe0e;\n   ","date":1595949567,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595949567,"objectID":"bd5fd436dc01090b37b71b815c394644","permalink":"/courses/bana3363/6-hypothesis-testing-for-a-proportion/","publishdate":"2020-07-28T15:19:27.628999Z","relpermalink":"/courses/bana3363/6-hypothesis-testing-for-a-proportion/","section":"courses","summary":"Hypothesis Test for a Proportion (Large Sample) Just as the case with means, we might be interested in coming up with a hypothesis test for the population proportion. There are two approaches we could take.","tags":null,"title":"Hypothesis Testing for a Proportion","type":"docs"},{"authors":null,"categories":null,"content":"Confidence Intervals for the Difference in Two Means (Independent Samples) Many times we are not interested only in a single group, but in multiple $% (\\geq 2)$ groups. For example, we might compare the amount of weight lost on a new drug for men $45-60$. Let Group $1$ be men $45-60$ who follow a specific healthful diet and let Group $2$ be men $45-60$ who follow the same diet but also take a weight loss drug. In the language of experimental design, Group $1$ is a control group, the group that receives no treatment (i.e., a specific manipulation of the members of the group), or no treatment above a reasonable standard. Group $2$ is an experimental or treatment group.\nThe general notation for inference about two population/process means is as follows.\n   Metric Group 1 Group 2     Sample of size $n_{1}:$ $X_{11},X_{12},\\ldots ,X_{1n_{1}}$ , all either $0$ or $1$ $n_{2}:$ $X_{21},X_{22},\\ldots ,X_{2n_{2}},$ all either $0$ or $1$   Mean: $p_{1}$ $p_{2}$   Sample Proportion $\\widehat{p_{1}}=\\frac{\\sum_{j}x_{1j}}{n_{1}}$ (i.e., number of successes in Group 1) $\\widehat{p_{2}}=\\frac{\\sum_{j}x_{2j}}{n_{2}}$ (i.e., number of successes in Group 2)   Estimated Standard Deviation $\\sqrt{\\frac{\\widehat{p_{1}}(1-\\widehat{p_{1}{n_1}}$ $\\sqrt{\\frac{\\widehat{p_{2}}(1-\\widehat{p_{2}}{n_{2}}}$    By the fundamental assumption of statistics (i.e., that variability is inescapable), any one person in Group $1$ could lose more weight than any one person in Group $2$, even if the drug is effective, simply by random chance. See Figure [drug_diet]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;drug_diet\u0026rdquo;}. The question, then, is not whether the drug works for everyone, but if it \u0026ldquo;works on average.\u0026rdquo;¬†The average for Group $1$ we can specify as $\\mu_{1}$ and the average for Group 2 we can specify as $\\mu_{2}.$ Then the question is whether $\\mu_{1}=\\mu_{2},$ or, equivalently, whether $\\mu_{1}-\\mu_{2}=0.$ We can address this question either through a confidence interval or a hypothesis test. We will discuss the interval here.\nThere are two cases to consider:\n  You know (or assume) that the two population standard deviations are the same, that is, $\\sigma_{1}=\\sigma_{2}$\n  You don\u0026rsquo;t know (or don\u0026rsquo;t wish to assume) that the two population standard deviations are the same, that is $\\sigma_{1}\\neq \\sigma_{2}$\n  Case 1: Confidence Interval for Difference in Means $\\sigma_{1}=\\sigma_{2}$ A $(1-\\alpha )100%$ confidence interval for $\\mu_{1}-\\mu_{2}$ is given by $\\overline{Y}_{1}-\\overline{Y}_{2}\\pm t_{n_{1}+n_{2}-2,\\alpha /2}s_{p}\\sqrt{% \\frac{1}{n_{1}}+\\frac{1}{n_{2}}}$, where $s_{p}=\\sqrt{\\frac{% (n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2}}$ is the \u0026ldquo;pooled\u0026rdquo; standard deviation from the two samples.\nCase 2: Confidence Interval for Difference in Means $\\sigma_{1}\\neq \\sigma_{2}$ A $(1-\\alpha )100%$ confidence interval for $\\mu_{1}-\\mu_{2}$ is given by $\\overline{Y}_{1}-\\overline{Y}_{2}\\pm t_{\\nu ,\\alpha /2}\\sqrt{\\frac{s_{1}^{2}% }{n_{1}}+\\frac{s_{2}^{2}}{n_{2}}},$ where $\\nu =\\frac{\\left( \\frac{s_{1}^{2}% }{n_{1}}+\\frac{s_{2}^{2}}{n_{2}}\\right) ^{2}}{\\frac{\\left( \\frac{s_{1}^{2}}{% n_{1}}\\right) ^{2}}{n_{1}-1}+\\frac{\\left( \\frac{s_{2}^{2}}{n_{2}}\\right) ^{2}% }{n_{2}-1}}$ .\nThe Case $2$ interval makes one fewer assumption than Case $1$, so it is closer to the truth because in reality, it is unlikely that the two standard deviations will be exactly equal. On the other hand, it is known that the Case $2$ interval is only an approximate interval, although the approximation is usually quite good. It is also more work to calculate the appropriate degrees of freedom because of the rather unpleasant looking formula for $\\nu .$ As with most things in life, there is a trade off. In general, Case $2$ is the safer bet. The degrees of freedom parameter, $\\nu ,$ is always calculated for you using software. In large samples $\\nu$ will be large as well, so $t_{\\nu ,\\alpha /2}$ will almost be the same as $z_{\\alpha /2},$ the critical value from the standard normal distribution.\nHere are some examples.\nA real estate agent in Ames, Iowa, has access to a large data set with $80$ variables related to the sale of nearly $3,000$ houses. The variables include total square footage, number of bathrooms, the age of each house at the time of sale, and whether the house had a number of amenities such as a fireplace or a pool1. The agent is interested in what effect, if any, having at least one fireplace in the house has on the mean selling price. Using a Pivot Table, the agent has the following data. Calculate a $95%$ confidence interval for the difference in the mean selling price for homes with and without at least one fireplace. Interpret the interval in context. Assume the population standard deviations are not equal.\nIn a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8,$ respectively. Calculate a $90%$ confidence interval for the mean difference in the amount of time spent studying per week between accounting and general business majors. Interpret the interval in context. Assume the population standard deviations are equal.\nIs there a difference in the average number of hours worked per week between those with a bachelor\u0026rsquo;s degree and those with a high school diploma? A sample of $354$ people with bachelor\u0026rsquo;s degrees and $976$ people with high school diplomas was taken. The average number of hours worked for those with bachelor\u0026rsquo;s degrees was $42.6$ with a standard deviation of $15.5$, and the average number of hours worked for those with high school diplomas was $39.5$ with a standard deviation of $15.1$.\na). Construct a $99%$ confidence interval for the true mean difference in work hours, assuming the two population standard deviations are the same.\nThe most recent General Social Survey asked American adults over $18$ years of age how many total weeks they worked in the previous year. The data is summarized in the table shown below, which was created using Excel\u0026rsquo;s pivot table feature2. Find a $95%$ confidence interval for the true mean difference in the number of weeks spent at work for persons possessing a bachelor\u0026rsquo;s degree versus a high school diploma, assuming the population standard deviations are the same.\n  The data are available here: http://www.amstat.org/publications/jse/jse_data_archive.htm \u0026#x21a9;\u0026#xfe0e;\n See http://office.microsoft.com/en-us/excel-help/pivottable-reports-101-HA001034632.aspx for more information about pivot tables. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1595949567,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595949567,"objectID":"db101945017d3a1fd74f09d17c14075f","permalink":"/courses/bana3363/7-interval-for-differences-in-means-independent-samples/","publishdate":"2020-07-28T15:19:27.628999Z","relpermalink":"/courses/bana3363/7-interval-for-differences-in-means-independent-samples/","section":"courses","summary":"Confidence Intervals for the Difference in Two Means (Independent Samples) Many times we are not interested only in a single group, but in multiple $% (\\geq 2)$ groups. For example, we might compare the amount of weight lost on a new drug for men $45-60$.","tags":null,"title":"Interval for Differences in Means--Independent Samples","type":"docs"},{"authors":null,"categories":null,"content":"Hypothesis Test for a Difference in Proportion (Independent Samples) Just as with means, we might be interested in testing whether two population proportions are equal. The general four-step procedure still applies. What changes are the specifics.¬†Let\u0026rsquo;s see how.\n  Specify the hypothesis:\nNow instead of $\\mu {1}-\\mu {2},$ we work with the population proportions, $p{1}-p{2}:$\n$$\\begin{aligned} \\text{(a) }H_{0} \u0026amp;:\u0026amp;p_{1}-p_{2}\\leq 0\\text{ vs. }H_{1}:p_{1}-p_{2}\u0026gt;0\\text{ } \\ \\text{(b) }H_{0} \u0026amp;:\u0026amp;p_{1}-p_{2}\\geq 0\\text{ vs. }H_{1}:p_{1}-p_{2}\u0026lt;0 \\ \\text{(c) }H_{0} \u0026amp;:\u0026amp;p_{1}-p_{2}=0\\text{ vs. }H_{1}:p_{1}-p_{2}\\neq 0\\end{aligned}$$\nNote that now we explicitly say that $d=0$. It doesn\u0026rsquo;t have to be, but if we consider nonzero differences, we must change the test statistic. The additional burden of describing two cases ($d=0$ versus $d$ being nonzero) is not warranted for our purposes, since testing the zero difference is far more common.\n  Calculate the test statistic\nThe test statistic will always be a $z$ statistic since we assume $n$ is large:\n$$z=\\frac{\\widehat{p_{1}}-\\widehat{p_{2}}}{\\sqrt{\\widehat{p}% _{p}(1-p_{p})\\left( \\frac{1}{n_{1}}+\\frac{1}{n_{2}}\\right) }}$$\nwhere $\\widehat{p}{p}$ is the \u0026ldquo;pooled\u0026rdquo; proportion of successes, i.e., $% \\widehat{p}{p}=\\frac{\\sum_{j}x_{1j}+\\sum_{j}x_{2j}}{n_{1}+n_{2}}=\\frac{% \\text{Total Number of \u0026ldquo;Successes\u0026rdquo;}}{\\text{Total Sample Size}}$\n  Determine the rejection region or compute the $p$-value.\n  The the rejection region for a test with the $\\alpha$ level of significance would be determined using the standard normal distribution using the following critical values. Note that these are the same critical values that we used for the test of a single proportion.\n$$\\begin{aligned} \\text{(a)}{\\text{ }z \u0026amp;:\u0026amp;z\u0026gt;\\text{ }z_{\\alpha }} \\ \\text{(b) }{z \u0026amp;:\u0026amp;z\u0026lt;-z_{\\alpha }} \\ \\text{(c) }{z \u0026amp;:\u0026amp;|z|\u0026gt;\\text{ }z_{\\alpha /2}}\\end{aligned}$$\nAlternatively, we can compute the $p$-value as follows:\n$$\\begin{aligned} \\text{(a) }pval \u0026amp;=\u0026amp;P(Z\u0026gt;z) \\ \\text{(b) }pval \u0026amp;=\u0026amp;P(Z\u0026lt;z) \\ \\text{(c) }pval \u0026amp;=\u0026amp;2\\min [P(Z\u0026gt;z),P(Z\u0026lt;z)]\\end{aligned}$$\n Make a conclusion  If the $p$-value $\\leq \\alpha ,$ we reject $H_{0}.$ If the $p$-value $% \u0026gt;\\alpha ,$ we fail to reject $H_{0}.$ Equivalently, if $z$ lies within the rejection region, we reject $H_{0}$. If $z$ lies outside the rejection region, we fail to reject $H_{0}.$\nTime for some examples.\nLet\u0026rsquo;s go back to the issue of marijuana legalization from a previous handout Test the hypothesis that the proportion of people in the $18-30$ group who favor legalization is different from the proportion favoring legalization in the $31$ and over group. Use $\\alpha =0.10.$\n   Response (2012; 18-30) Count (18-30) Count (31 \u0026amp; over) Total     LEGAL $131$ $455$ $586$   NOT LEGAL $104$ $541$ $645$   TOTAL $235$ $996$ $1231$    A website designer wants to examine the effectiveness of two different layouts (A and B) for a small retail client in terms of the proportion of individuals who find and click on a special coupon within the first five minutes of browsing the retailer\u0026rsquo;s site. Presumably, the more people who find and click the coupon, the more effective the layout is. For two weeks, whenever a customer accessed the retailer\u0026rsquo;s site, one of the two layouts (A or B) was randomly chosen to load on the customer\u0026rsquo;s screen. Customers were tracked by their IP addresses, numbers that uniquely identify devices over the internet. Of the customers who browsed the site for at least five minutes, the proportion who clicked the coupon using each layout was recorded. The data are below. At the $\\alpha =0.01$ level of significance, can the website designer conclude that the layouts differ in effectiveness?\n   Layout Found Coupon Did Not Find Coupon Total     A $675$ $658$ $1333$   B $690$ $477$ $1167$   Total $1365$ $1135$ $2500$    In a recent study, a drug manufacturer wanted to determine if a new antidepressant (Drug X) could also reduce pain that often comes with diabetes, called diabetic neuropathy. The data for the study are shown below. At the $\\alpha =0.05$ level of significance, can the company conclude that the proportion of people who experience relief from Drug X is larger than for the placebo?.\n|\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- |\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; |\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; |\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | |Dosage |Some/Significant Relief |No Relief |Total | |Drug X |$159$ |$122$ |$281$ | |Placebo |$114$ |$197$ |$311$ | |Total | $273$ |$319$ |$592$ |\nA researcher is interested to see if there is a difference between married and unmarried people when it comes to certain social behavior, in particular, the frequency of sexual activity. The data from the study are shown below. at the $\\alpha =0.01$ level of significance, can the researcher conclude a difference exists between the two groups?\n   Group $\\geq$Once/Week $\\mathbf{\u0026lt;}$ Once/Week Total     Married $608$ $28$ $636$   Unmarried $553$ $7$ $560$   Total $1161$ $35$ $1196$    Example: Astrology Astrology is a system of belief that, broadly speaking, claims that astronomical events somehow influence human behavior. One aspect of astrology is belief that the zodiac sign that one is born under (e.g., Pisces, Libra, Leo, Scorpio, etc.) somehow influences a person\u0026rsquo;s behavior. Is belief in astrology related significantly to education level? The $2012$ General Social Survey asked adults $18$ and over whether they believed at all in the zodiac. The results are presented below for individuals with an associate\u0026rsquo;s degree or higher (\u0026ldquo;College\u0026rdquo;) and those with a high school diploma or less (\u0026ldquo;HS or Below\u0026rdquo;). Can we conclude at the $\\alpha =0.10$ level of significance that the proportion of individuals with a college degree who believe in the zodiac is smaller than that of individuals with a high school diploma or below?\n|Dosage |Believe |Do Not Believe |Total | |\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; |\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- |\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | |College |$162$ |$198$ |$360$ | |HS or Below |$330$ |$306$ |$636$ | |Total |$492$ |$504$ |$996$ |\n","date":1595949567,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595949567,"objectID":"0c9e2a976937e4771111b33118cbf1bc","permalink":"/courses/bana3363/11-test-for-differences-in-proportions-independent-samples/","publishdate":"2020-07-28T15:19:27.628999Z","relpermalink":"/courses/bana3363/11-test-for-differences-in-proportions-independent-samples/","section":"courses","summary":"Hypothesis Test for a Difference in Proportion (Independent Samples) Just as with means, we might be interested in testing whether two population proportions are equal. The general four-step procedure still applies.","tags":null,"title":"Hypothesis Test for a Difference in Proportion (Independent Samples)","type":"docs"},{"authors":null,"categories":null,"content":"Confidence Interval for Difference in Proportion We might be interested in the difference in the proportions of two groups who meet a certain criterion. For example, we might want to determine what type of website banner ad (one that flashes or one that is steady) leads to a higher proportion of people clicking on it. We might want to determine which of two packaging designs leads to a higher proportion of people who indicate they would buy the product. An excellent example of inference for proportions comes from prescription drug testing. Researchers might be interested to see if a new cancer drug has a reduced chance of causing nausea in patients compared to an existing drug. Opinion polls on issues such as marijuana legalization can be analyzed using techniques for differences in proportions.\nThe general notation for inference about two population/process proportions is similar to that of two population/process means. The big difference is that the data for both groups can only take on the values of $0$ or $1,$ with $1$ indicating a \u0026ldquo;success\u0026rdquo; (either a click on an ad, an indication to purchase, or a person indicating that they did not experience nausea, for example).\n   Metric Group 1 Group 2     Sample of size $n_{1}:$ $X_{11},X_{12},\\ldots ,X_{1n_{1}}$ , all either $0$ or $1$ $n_{2}:$ $X_{21},X_{22},\\ldots ,X_{2n_{2}},$ all either $0$ or $1$   Mean: $p_{1}$ $p_{2}$   Sample Proportion $\\widehat{p_{1}}=\\frac{\\sum_{j}x_{1j}}{n_{1}}$ (i.e., number of successes in Group 1) $\\widehat{p_{2}}=\\frac{\\sum_{j}x_{2j}}{n_{2}}$ (i.e., number of successes in Group 2)   Estimated Standard Deviation $\\sqrt{\\frac{\\widehat{p_{1}}(1-\\widehat{p_{1}{n_1}}$ $\\sqrt{\\frac{\\widehat{p_{2}}(1-\\widehat{p_{2}}{n_{2}}}$    We can now introduce the confidence interval for the difference in two proportions.\nA $100(1-\\alpha )%$ confidence interval for the difference in two population proportions $p_{1}-p_{2}$, if the two sample sizes $n_{1}$ and $% n_{2}$ are sufficiently large, is given by\n$$\\widehat{p_{1}}-\\widehat{p_{2}}\\pm z_{\\alpha /2}\\sqrt{\\frac{\\widehat{p_{1}}% (1-\\widehat{p_{1}})}{n_{1}}+\\frac{\\widehat{p_{2}}(1-\\widehat{p_{2}})}{n_{2}}}$$\nThe interval for the difference in proportions is interpreted in a similar manner as with the interval for the difference in two means. Because we are comparing two population proportions, we must phrase our interpretation in terms of being confident that the difference in proportions lies between the lower and upper bounds. Note that the interval will always fall between $-1$ and $1.$ Negative values will arise when the proportion in Group $2$ is higher than the proportion in Group $1.$ In a single sample interval for a proportion, we cannot have negative values, but when we discuss differences in proportions, we certainly can.\nAnother issue regarding interpretation should be mentioned here. Because proportions can be interpreted as percentages, it would be tempting to say, if your interval were $[0.03,0.05]$ for example, that the difference between Group 1 and Group 2 on an issue is between $3%$ and $5%$. This is wrong because the difference in two percents is NOT the percentage difference. For example, if the interval above represented the difference in the percentage of people in two voting populations who favored a new tax initiative, we cannot say that between $3%$ and $5%$ more of Group $1$ favors the initiative than Group $2$. If, in a population of $12,000,$ $% 1,440$ $(12%)$ of people in Group 2 really do favor the initiative, then saying $3%$ more people in Group 1 favor the initiative implies that $1,483$ people in that group should be in favor. But if the population size of Group 1 is, let\u0026rsquo;s say, $34,000$ (populations can be different sizes) and $5,100$ $% \\left( \\frac{5,100}{34,000}=15%\\right)$ were in favor, a $3%$ increase would suggest that $5,253$ people in Group 1 would be in favor, not $1,483.$ Thus, we must always speak of percentage-point differences when we interpret these intervals.\nThe number of individuals for and against legalization of marijuana in the $% 18-30$ and $31$ and over age groups are given in the table below. Find a $% 95%$ confidence interval for the true difference in the proportion of people favoring marijuana legalization between the two groups.\n   Group Legal Not Legal Total     18-30 (Group 1) $131$ $104$ $235$   31 \u0026amp; over (Group 2) $455$ $541$ $996$   TOTAL $586$ $645$ $1231$    A website designer wants to examine the effectiveness of two different layouts (A and B) for a small retail client in terms of the proportion of individuals who find and click on a special coupon within the first five minutes of browsing the retailer\u0026rsquo;s site. Presumably, the more people who find and click the coupon, the more effective the layout is. For two weeks, whenever a customer accessed the retailer\u0026rsquo;s site, one of the two layouts (A or B) was randomly chosen to load on the customer\u0026rsquo;s screen. Customers were tracked by their IP addresses, numbers that uniquely identify devices over the internet. Of the customers who browsed the site for at least five minutes, the proportion who clicked the coupon using each layout was recorded. The data are below. Construct a $95%$ confidence interval for the difference in the two proportions and interpret the interval in context.\n   Layout Found Coupon Did Not Find Coupon Total     A $675$ $658$ $1333$   B $690$ $477$ $1167$   Total $1365$ $1135$ $2500$    In a recent study, a drug manufacturer wanted to determine if a new antidepressant (Drug X) could also reduce pain that often comes with diabetes, called diabetic neuropathy. The data for the study are shown below. Calculate a $92%$ confidence interval for the difference in the proportion of people who experience relief from Drug X and the placebo.\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; Dosage Some/Significant Relief No Relief Total\nDrug X $159$ $122$ $281$\nPlacebo $114$ $197$ $311$ $273$ $319$ $592$ \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;\nA researcher is interested to see if there is a difference between married and unmarried people when it comes to certain social behavior, in particular, the frequency of sexual activity. The data from the study are shown below. Calculate a $90%$ confidence interval for the difference in the proportion of married and unmarried persons who have sex at least once per week.\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; Group $\\geq$Once/Week $\\mathbf{\u0026lt;}$ Once/Week Total Married $608$ $28$ $636$ Unmarried $553$ $7$ $560$ Total $1161$ $35$ $1196$ \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;\nAstrology is a system of belief that, broadly speaking, claims that astronomical events somehow influence human behavior. One aspect of astrology is belief that the zodiac sign that one is born under (e.g., Pisces, Libra, Leo, Scorpio, etc.) somehow influences a person\u0026rsquo;s behavior. Is belief in astrology related significantly to education level? The $2012$ General Social Survey asked adults $18$ and over whether they believed at all in the zodiac. The results are presented below for individuals with an associate\u0026rsquo;s degree or higher (\u0026ldquo;College\u0026rdquo;) and those with a high school diploma or less (\u0026ldquo;HS or Below\u0026rdquo;). Calculate a $98%$ confidence interval for the difference in proportions between the two groups and interpret the interval in context.\n Dosage Believe Do Not Believe Total\nCollege $162$ $198$ $360$\nHS or Below $330$ $306$ $636$ Total $492$ $504$ $996$\n ","date":1595949567,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595949567,"objectID":"3cb3e99c0e7616a1426aff4a623e4102","permalink":"/courses/bana3363/8-intervals-for-differences-in-proportions-independent-samples/","publishdate":"2020-07-28T15:19:27.628999Z","relpermalink":"/courses/bana3363/8-intervals-for-differences-in-proportions-independent-samples/","section":"courses","summary":"Confidence Interval for Difference in Proportion We might be interested in the difference in the proportions of two groups who meet a certain criterion. For example, we might want to determine what type of website banner ad (one that flashes or one that is steady) leads to a higher proportion of people clicking on it.","tags":null,"title":"Intervals for Differences in Proportions--Independent Samples","type":"docs"},{"authors":null,"categories":null,"content":"Basic Concepts Because the values and associated probabilities for a continuous random variable cannot be listed, we use a probability density function (pdf) to describe the \u0026ldquo;relative likelihoods\u0026rdquo; of various values of the function. For a pdf to be valid, 1). it must lie completely above the horizontal axis, and 2). the total area under the curve must equal $1$ exactly.\nWe can plug in values to the pdf to graph the distribution of a continuous random variable and we can use calculus to find the mean and variance of the random variable. For any continuous random variable $X$, the probability that $X$ lies between two values, say $3$ and $8$, is denoted $P(3\\leq X\\leq 8)$ and would be found by taking $\\int_{3}^{8}f(x)dx$, where $f(x)$ is the probability density function. In simpler terms, probabilities are found by finding the area under the curve (the integral) between two specified values and $\\int$ is the integral sign from calculus.\nIn this class, we won\u0026rsquo;t worry about finding the mean and variance of a continuous distribution, nor will we find probabilities directly. For the most part, we will restrict our attention to a few distributions for which we can use software or tables to find probabilities.\nFor continuous distributions, the probability that the random variable $X$ equals any specific value $x$ is always $0,$ so $P(X\\leq x)=P(X\u0026lt;x).$ The same goes if you reverse the inequality.\nThe normal distribution is a continuous probability distribution that is completely defined by its mean, $\\mu ,$ and standard deviation, $% \\sigma .$ If a random variable $Y$ has a normal distribution with a specified mean $\\mu$ and standard deviation $\\sigma$, we write this as $Y% \\symbol{126}N(\\mu ,\\sigma )$, where the $\\symbol{126}$ symbol stands for \u0026ldquo;is distributed as.\u0026rdquo; Also, $Y$ has the following pdf:\n$$f(y|\\mu ,\\sigma )=\\frac{1}{\\sqrt{2\\pi }\\sigma }e^{\\frac{-(y-\\mu )^{2}}{% 2\\sigma ^{2}}},\\text{ }-\\infty \u0026lt;\\mu \u0026lt;\\infty ;\\text{ }\\sigma \u0026gt;0 \\label{norm_pdf}$$\nThe possible values of $y$ range from $-\\infty$ to $\\infty$, but most of \u0026ldquo;the action\u0026rdquo; of the curve takes place within a narrow range. The graph of a normal distribution is, in general, symmetric around $\\mu$ and bell shaped.\nFigure [normcrve]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;normcrve\u0026rdquo;} examples of normal distributions. In general, the effect of changing $\\mu$ is to move the graph to the left or right on the horizontal axis, while the effect of changing $\\sigma$ is to make the graph narrower (decreasing $\\sigma$) or wider (increasing $\\sigma$).\n($68-95-99.7$¬†Rule). If $Y$ has a normal distribution with mean $\\mu$ and standard deviation $\\sigma$, then the following are true:\n  About $68%$ of the normal curve lies within $1$ standard deviation of the mean. That is, $P(\\mu -\\sigma \\leq Y\\leq \\mu +\\sigma )\\approx 0.68$\n  About $95%$ of the normal curve lies within $2$ standard deviations of the mean. That is, $P(\\mu -2\\sigma \\leq Y\\leq \\mu +2\\sigma )\\approx 0.95$\n  About $99.7%$ of the normal curve lies within $3$ standard deviation of the mean. That is, $P(\\mu -3\\sigma \\leq Y\\leq \\mu +3\\sigma )\\approx 0.997$\n  A special type of normal distribution where $\\mu =0$ and $\\sigma =1$ is called the standard normal distribution or \u0026ldquo;$Z$ distribution,\u0026rdquo; after the letter that we use to denote a random variable with a standard normal distribution.\nThis is the distribution that you had a table for in BANA 2372.\nThe standard normal distribution is so important because of the following fact.\n($Z$**-Score Theorem**). If $Y$ has a normal distribution with mean $\\mu$ and standard deviation $\\sigma$, then\n$$Z=\\frac{Y-\\mu }{\\sigma }$$\nhas a standard normal distribution\nFor any specific value of $y$ (note the lowercase letter), we can calculate the $z$ (again, not the lowercase) score as\n$$z=\\frac{y-\\mu }{\\sigma }$$\nSo, you can \u0026ldquo;transform\u0026rdquo; any normal random variable to a standard normal simply by subtracting the mean and dividing by the standard deviation.\nAnother important implication of equation $(\\ref{z_score})$ above is that you can think of a $Z$¬†score as the number of standard deviations that an observation is away from its mean**.** This idea comes into play when we look at finding confidence intervals.\nThe number of hits per day to a small company\u0026rsquo;s website have a normal distribution with mean $\\mu =90,000$ and standard deviation $\\sigma = 5,000.$ Sketch this distribution and indicate the intervals within with about $68%$, $95%$, and $99.7%$ of the hits are expected to lie.\nFinding Normal Probabilities Essentially, there are three types of questions we can ask about a normal random variable, say, $Y$. For each of these we can \u0026ldquo;turn a question about $% Y$ into a question about $Z\u0026quot;$ using the Z-Score Theorem. The idea is to transform each side of the inequality into a $z$-score using the basic algebra concept of \u0026ldquo;whatever you do to one side you do to the other,\u0026rdquo; as shown below:\n  What is the probability that $Y$ is less than or equal to some specific number $y?$ Answer:$\\ P(Y\\leq y)=P\\left( \\frac{Y-\\mu }{\\sigma }\\leq \\frac{y-\\mu }{\\sigma }\\right) =P(Z\\leq z)$\n  What is the probability that $Y$ is greater than or equal to some value $y?$ Answer: $P(Y\\geq y)=1-P(Y\u0026lt;y)=1-P\\left( \\frac{Y-\\mu }{\\sigma }\u0026lt;% \\frac{y-\\mu }{\\sigma }\\right) =1-P(Z\u0026lt;z)$\n  What is the probability that $Y$ is between $y_{1}$ and $y_{2}?$ Answer: $P(y_{1}\\leq Y\\leq y_{2})=P(Y\\leq y_{2})-P(Y\\leq y_{1})$\n  $=P\\left( \\frac{Y-\\mu }{\\sigma }\\leq \\frac{y_{2}-\\mu }{\\sigma }\\right) -P\\left( \\frac{Y-\\mu }{\\sigma }\\leq \\frac{y_{1}-\\mu }{\\sigma }\\right) =P(Z\\leq z_{2})-P(Z\\leq z_{1})$\nYou can see how three works by drawing a picture.\nThe number of hits per day to a small company\u0026rsquo;s website have a normal distribution with mean $\\mu = 90,000$ and standard deviation $\\sigma = 5,000.$\n(a). Find the z-score associated with $84,000$ hits.\nb). Find the probability that the website will have fewer than $84,000$ hits on any given day.\nc). Find the probability that the website will have more than $84,000$ hits on any given day.\nThe number of hits per day to a small company\u0026rsquo;s website have a normal distribution with mean $\\mu =90,000$ and standard deviation $\\sigma =$ $% 5,000.$ The company believes that customers will be dissatisfied with the site\u0026rsquo;s performance if daily hits get above $99,000$, since that might make the site less responsive. What is the probability that customers will be dissatisfied on any given day due to this issue?\nThe number of hits per day to a small company\u0026rsquo;s website have a normal distribution with mean $\\mu =90,000$ and standard deviation $\\sigma =$ $% 5,000.$ The IT¬†department has concluded that if the site receives between $% 89,000$ and $97,000$ hits per day, service should be considered \u0026ldquo;acceptable\u0026rdquo; for that day. What is the probability that on any given day, service is considered acceptable?\nc). Adult systolic blood pressure (the top number) has a normal distribution with a mean of $120$ and a standard deviation of $20$. What percentage of adults have blood pressures greater than $140?$\nFinding a Normal Interval with a Specified Probability Suppose now that I¬†want to find two values, call them $z_{L}$ and $z_{U}$, on the standard normal curve so that $P(z_{L}\\leq Z\\leq z_{U})=1-\\alpha .$ To make things easier, we\u0026rsquo;ll take advantage of the symmetry of the normal distribution and say that $% -z_{L}=z_{U},$ that is, the two numbers are just negatives of one another. The area we want is the area between the negative and positive value of $% z_{U}.$\nFor example, suppose we want to find two values $z_{L}$ and $z_{U}$ so that $% P(z_{L}\\leq Z\\leq z_{U})=0.90.$ So $1-\\alpha =0.90,$ and therefore $\\alpha =0.10.$ Look at Figure [shade_norm]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;shade_norm\u0026rdquo;}. If we want the area *between* $z_{L}$ and $z_{U}$ to be $0.90$, that means that the area *outside* of this interval must be $0.10$. Furthermore, since we specify that $% -z_{L}=z_{U}$, then the symmetry of the distribution implies that $% P(Z\u0026gt;z_{U})=P(Z\u0026lt;z_{L})$, i.e., the area to the right of $z_{U}$ is the same as the area to the left of $z_{L}.$ That combined area must be $0.10,$ so that must mean $P(Z\u0026gt;z_{U})=P(Z\u0026lt;z_{L})=0.05$ (i.e., we have $0.05$ on either end, since $0.05+0.05=0.10).$ Therefore, we can find these two values of $z$ simply by finding $z_{U}=z_{0.05}$ and then taking the negative of this amount to find $z_{L}.$\nTo answer this question, we can use the fact that we can solve the $z$-score formula for $y$ to \u0026ldquo;recover\u0026rdquo; an original observation if we know a specific value of $z:$\n$$z=\\frac{y-\\mu }{\\sigma }\\Leftrightarrow y=\\sigma z+\\mu$$\nFor example, if $z=2.5$, $\\mu =3$, and $\\sigma =10$, then\n$$\\begin{aligned} y \u0026amp;=\u0026amp;\\sigma z+\\mu \\ \u0026amp;=\u0026amp;(10)\\ast 2.5+3=28\\end{aligned}$$\nLet\u0026rsquo;s practice this concept first.\nFind the original observation $(y)$ from the following $z$-scores, listed along with the mean $\\mu$ and standard deviation $\\sigma$.\na). $z=1.23,\\mu =12,\\sigma =3$\nb). $z=-0.65,\\mu =30,\\sigma =11$\nc). $z=2.33,\\mu =9,\\sigma =15$\nThe value $z_{\\alpha }$ is the value of the standard normal curve that has area (i.e., probability) $\\alpha$ **to its right**. That is, $% P(Z\u0026gt;z_{\\alpha })=\\alpha$ (and therefore, $P(Z\\leq z_{\\alpha })=1-\\alpha )$.\nFor example, $z_{0.30\\text{ }}$is the value of $z$ such that $% P(Z\u0026gt;z_{0.30})=0.30.$ This is just notation that we will have to get used to. These values of $z$ can be found two ways. One way is to go inside the $z$ table and find $(1-\\alpha )$, and then read outward to the associated row and column. An easier way is to use the **quantile function** that is found in statistics software. For example, in Excel, the function is  NORMINV. You supply $(1-\\alpha )$ and it gives you $z_{\\alpha }.$ We will focus on using tables to find these values.\nLet\u0026rsquo;s make sure we have this down.\nFind the values of $z_{\\alpha }$ indicated below.\na). $z_{0.10}$\nb). $z_{0.025}$\nc). $z_{0.98}$\nd). $z_{0.80}$\ne). $z_{0.1271}$\nf). $z_{0.9838}$\nIn this case of $P(z_{L}\\leq Z\\leq z_{U})=0.90$, $z_{0.05}$ can be found (using software or a table) to be $1.645,$ so $z_{L}=-1.645,$ and we get the important result that $P(-1.645\\leq Z\\leq 1.645)=0.90$.\nWe can state the general result:\n(math fact) $P(-z_{\\alpha /2}\\leq Z\\leq z_{\\alpha /2})=1-\\alpha .$\nOnce we find the value of $z$ we can \u0026ldquo;recover\u0026rdquo; the original observations by setting $y_{L}=-z_{\\alpha /2}\\sigma +\\mu$ and $y_{L}=z_{\\alpha /2}\\sigma +\\mu$, and therefore state that $Y$ falls within $z_{\\alpha /2}$ standard deviations of the mean with probability $1-\\alpha .$ Here are some practice exercises.\nWithin how many standard deviations from the mean does $88%$ of any normal curve lie?\nThe diameter of a hypodermic needle is measured in \u0026ldquo;gauge.\u0026rdquo; Different gauges are used depending on the type of substance that needs to be injected. Becton Company manufactures insulin needles which must have an inside diameter of $0.016$ inches. Their current manufacturing process has a standard deviation of $0.0001$ (this is high-precision business)$.$ Within what interval will $97%$ of all the needles produced by the company fall?\nThe weights of babies at birth have a normal distribution with a mean of $119.6$ ounces and a standard deviation of $18.2$ ounces. Birth weights that are too large or too small might indicate a serious problem. What interval will contain $95%$ of birth weights? Find the answer using the $68-95-99.7$ rule and the approach just discussed. Compare your answers.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"0929c33e7d765706e7dbace2d5a4a8f7","permalink":"/courses/bana3363/2-the-normal-distribution/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/bana3363/2-the-normal-distribution/","section":"courses","summary":"Basic Concepts Because the values and associated probabilities for a continuous random variable cannot be listed, we use a probability density function (pdf) to describe the \u0026ldquo;relative likelihoods\u0026rdquo; of various values of the function.","tags":null,"title":"The Normal Distribution","type":"docs"},{"authors":null,"categories":null,"content":"Descriptive type As the name suggests, the descriptive statistic is used to describe the data. It describes the basic features of the data and shows or summarizes it in a rational way.\nLimitation: Descriptive statistics do not allow making conclusions. You cannot get conclusions or make generalizations that extend beyond the observed data.\nInferential type Inferential statistics is a result of more complicated mathematical estimations and allows us to infer trends about a larger population based on observed data.\nThis type of statistical analysis is used to study the relationships between variables within a sample and you can make conclusions, generalization, or predictions about a bigger population.\nPredictive Analytics Predictive analytics uses techniques such as statistical algorithms and machine learning to define the likelihood of future results, behavior, and trends based on both new and historical data.\nIt is important to note that no statistical method can predict the future with 100% certainty. Businesses use these statistics to answer the question \u0026ldquo;What might happen?\u0026rdquo;\nPrescriptive Analytics Prescriptive analytics is a study which examines data to answer the question \u0026ldquo;What should be done?\u0026rdquo;\nPrescriptive analysis aims to find the optimal recommendations for a decision-making process. It is all about providing advice.\nCausal analysis Causal inference is used to hen you would like to understand and identify the reasons why things are as they are. This type of analysis answers the question \u0026quot; Why did this happen?\u0026rdquo;\nThe business world is full of events that lead to failure. Causal inference seeks to identify the reasons why things occurred. It is better to find causes and to treat them instead of treating symptoms.\nExploratory data analysis (EDA) EDA is an analysis approach that focuses on identifying general patterns in the data n to find previously unknown relationships.\nEDA alone should not be used for generalizing or predicting. EDA is used for taking a bird\u0026rsquo;s eye view of the data and trying to make some sense of it. Commonly, it is the first step in data analysis preformed before other formal statistical techniques.\nMechanistic analysis The mechanistic analysis is about understanding the exact changes in given variables that lead to changes in other variables. However, mechanistic does not considered external influences.\nThe assumption is that a give system is affected by interaction of its own components. It is useful on those systems for which there are very clear definitions.\nSource: http://www.intellspot.com/types-statistical-analysis/\n","date":1588633200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588633200,"objectID":"0cae80f37aa2b12b30a13289d0a0f525","permalink":"/courses/bana3363/2-types-of-statistical-analysis/","publishdate":"2020-05-05T00:00:00+01:00","relpermalink":"/courses/bana3363/2-types-of-statistical-analysis/","section":"courses","summary":"Descriptive type As the name suggests, the descriptive statistic is used to describe the data. It describes the basic features of the data and shows or summarizes it in a rational way.","tags":null,"title":"Types of Statistical Analysis","type":"docs"},{"authors":null,"categories":null,"content":"Concepts of Inference Our ultimate goal is to obtain a reasonable \u0026ldquo;guess\u0026rdquo;¬†at the true value of a process parameter (in this section, the population/process mean $\\mu$) using a sample of data. Before we do that, we have to introduce some terms.\nA point estimator for a parameter is a particular function of the random sample that gives a single number that we hope is \u0026quot; close\u0026rdquo;¬†to the true value of the parameter.\nAn interval estimator is a range of values, constructed using observed data, within which the parameter is believed to fall.\nThis sounds very technical, but it really is not so bad. An example of a point estimator is the sample mean: $\\overline{X}=\\frac{\\sum X_{i}}{n}$. Notice how it is a function of the data (the values of $X_{i})$. In practice, we typically use the point estimator to construct an interval estimator.\nIn theory, there are an infinite number of point estimators we could pick to estimate $\\mu$, but it turns out that one, $\\overline{X}$, is \u0026ldquo;the best.\u0026rdquo; The following general criteria are used to judge whether a point estimator is \u0026ldquo;good:\u0026rdquo;\n  Unbiasedness\u0026ndash;The average of the estimator is equal to the population parameter. This means that the estimator \u0026ldquo;gets it right on average.\u0026rdquo;\n  Consistency\u0026ndash; The estimator gets closer and closer to the true population parameter as the sample size increases. This is reflected in the sampling distribution getting increasingly narrow around the true value of the parameter.\n  Relative efficiency\u0026ndash; Given two different estimators with the same sample size, we choose the one with the smaller variance.\n  How do all of these apply to $\\overline{X}?$ We know from Handout 4 that the sample mean $\\overline{X}$ is unbiased because the theorem in that handout states that $E(\\overline{X})=% %TCIMACRO{\\U{3bc} }% %BeginExpansion \\mu %EndExpansion$. You can see from the variance of $\\overline{X}$ , that is, $\\frac{\\sigma ^{2}}{n}$, that as $n$ gets larger, the variance gets smaller ($\\sigma ^{2}$ does not change with $n)$. If $n$ is large enough, the variability essentially goes to $0$ and $\\overline{X}$ \u0026ldquo;collapses\u0026rdquo; onto $\\mu$. Lastly, it can be shown (not here) that $\\overline{X}$ is has the smallest variability among all unbiased estimators, so when we use it, we are using \u0026ldquo;the best\u0026rdquo; guess of $\\mu$.\nA Confidence Interval for the Mean When $\\sigma$ Is Known or $n$ is Large By the Central Limit Theorem, we know that the distribution of $\\overline{X}$ is approximately $N\\left( 0,\\frac{\\sigma }{\\sqrt{n}}\\right)$. This implies that we can say something about any given sample average $\\overline{x}$. Specifically, we know that a sample average will have about a $68%$ chance of being within one standard deviation of $\\mu$ about a $95%$ chance of being within two standard deviations of $\\mu$, and about a $99.7%$ chance of being within three standard deviations of $\\mu$. Remember that \u0026ldquo;standard deviation\u0026rdquo; here refers to the standard deviation of $\\overline{X}$, which is $\\frac{\\sigma }{\\sqrt{n}}$, so the intervals would be $\\mu \\pm \\frac{\\sigma }{\\sqrt{n}},\\mu \\pm 2\\frac{\\sigma }{\\sqrt{n}}$, and $\\mu \\pm 3\\frac{\\sigma }{% \\sqrt{n}}$ respectively.\nConfidence intervals flip this logic around and use the fact that if $\\overline{X}$ has a $68%$ chance of being within one standard deviation of $\\mu$, then we can also say that $\\mu$ has a $68%$ chance of being within one standard deviation of $\\overline{X}$. This means that\n$$P(\\mu -2\\sigma /\\sqrt{n}\\leq \\overline{X}\\leq \\mu +2\\sigma /\\sqrt{n})$$\nand\n$$P(\\overline{X}-2\\sigma /\\sqrt{n}\\leq \\mu \\leq \\overline{X}+2\\sigma /\\sqrt{n})$$\nare equal. One statement can be made from the other simply by rearranging what goes in the middle using algebra.\nFigure [ci_pic]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;ci_pic\u0026rdquo;} might make things clearer.\nWe don\u0026rsquo;t know what $\\mu$ is, but we know that the sample averages $\\overline{x}$ should fall around it in a certain pattern. So we use this idea to \u0026ldquo;guess\u0026rdquo; at the location of $\\mu$. If we were to take the range of numbers $\\overline{x}-2\\sigma /\\sqrt{n}$ to $\\overline{x}+2\\sigma /\\sqrt{n}$ as our interval estimator, about $95%$ of the time, if we took repeated samples from the population and calculated an interval for each one, the true value of $\\mu$ would fall in the interval; about $5%$ of the time it wouldn\u0026rsquo;t. So we could say we were \u0026ldquo;$95%$ confident\u0026rdquo; in the interval.\nWhat if we want a different level of confidence than $68,95$, or $99.7%?$¬†Suppose we wanted to be $98%$ confident, for example? Recall from Handout 2 that $P(-z_{\\alpha /2}\\leq Z\\leq z_{\\alpha /2})=1-\\alpha$. If we want $98%$ confidence, we¬†begin by setting $1-\\alpha =0.98$, which means $\\alpha =0.02$. Then $z_{\\alpha /2}=z_{0.02/2}=z_{0.01}=2.33$. Then we substitute $Z=\\frac{\\overline{X}-\\mu }{\\sigma /\\sqrt{n}}$ in the middle and solve for $\\mu$:\n$$P(-2.33 \u0026amp;\\leq \u0026amp;Z\\leq 2.33)=0.98 \\ \u0026amp;\\Rightarrow \u0026amp;P(-2.33\\leq \\frac{\\overline{X}-\\mu }{\\sigma /\\sqrt{n}}\\leq 2.33)=0.98$$ $$P(-2.33\\sigma /\\sqrt{n}\\leq \\overline{X}-\\mu \\leq 2.33\\sigma /% \\sqrt{n})=0.98$$ $$P(-2.33\\sigma /\\sqrt{n}-\\overline{X}\\leq -\\mu \\leq 2.33\\sigma /% \\sqrt{n}-\\overline{X})=0.98 \\ \u0026amp;\u0026amp;\\text{Multiply through by}-1\\text{ and rearrange to get} \\ P(\\overline{X}-2.33\\sigma /\\sqrt{n} \u0026amp;\\leq \u0026amp;\\mu \\leq \\overline{X}+2.33\\sigma /% \\sqrt{n})=0.98$$\nThen, to construct the interval, substitute in the observed sample average $\\overline{x}$ to get the $98%$ confidence interval to be $\\overline{x}$ $\\pm 2.33\\sigma /\\sqrt{n}$.\nYou can apply these steps for any confidence level you want. Therefore, we have the following:\n[[CI_sig_known]]{#CI_sig_known label=\u0026quot;CI_sig_known\u0026rdquo;}A $(1-\\alpha )100%$ confidence interval for $\\mu$ when $\\sigma$ is known (or when $n$ is large) is given by\n$$\\overline{x}\\pm z_{\\alpha /2}\\frac{\\sigma }{\\sqrt{n}}$$\nCalculation is easy; interpretation is key. We view the mean $%TCIMACRO{\\U{3bc} }% %BeginExpansion \\mu %EndExpansion$ as a fixed but unknown quantity. Once we gather data and compute the sample mean $\\overline{x}$ and the associated confidence interval, the interval either contains $\\mu$ or it does not (see Figure [ci_pic]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;ci_pic\u0026rdquo;}). In the long run, however, if you took repeated samples of size $n$ and computed the interval each time, $(1-\\alpha )100%$ of them would contain $\\mu$. Thus, the \u0026ldquo;confidence\u0026rdquo;¬†you have is in the method used to construct an interval, not in the particular interval you have constructed.\nRarely do we know $\\sigma$, but at times we have fairly large samples, so we can use Definition [CI_sig_known]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;CI_sig_known\u0026rdquo;} anyway by just replacing $\\sigma$ with the sample standard deviation $s$. If we have a small sample and do not know $\\sigma$, we must use some other multiplier besides $z_{\\alpha /2}$. For this, we use values from Student\u0026rsquo;s $t$ distribution.\nA Confidence Interval for the Mean When $\\sigma$ Is Unknown or $n$ is Small Student\u0026rsquo;s $t$**¬†distribution with **$\\n-1$**¬†degrees of freedom is the probability distribution of the quantity\n$$t=\\frac{\\overline{X}-\\mu }{s/\\sqrt{n}}$$\nAs with the standard normal distribution, it is symmetric, bell-shaped, and centered at $0$. The degrees of freedom $\\nu$ is a parameter that governs the width of the distribution. The notation $t_{\\nu }$ refers to a random variable from a $t$ distribution with $\\nu$ degrees of freedom.\nAs you can see in Figure [studt]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;studt\u0026rdquo;}, Student\u0026rsquo;s $t$ distribution looks a lot like the standard normal. In fact, it can be shown that as $\\nu$ gets very large, the the $t$ distribution \u0026ldquo;becomes\u0026rdquo; the standard normal distribution. That\u0026rsquo;s why we can use values of $z$ when $n$ is large in a confidence interval.\nUsing the $t$ distribution, we can define a confidence interval as follows:\nA $(1-\\alpha )100%$ confidence interval for $\\mu$ when $\\sigma$ is unknown (or when $n$ is small) is given by\n$$\\overline{x}\\pm t_{n-1,\\alpha /2}\\frac{s}{\\sqrt{n}}$$\nwith $t_{n-1,\\alpha /2}$, being the value of a $t$ distribution with $n-1$ degrees of freedom that has probability $\\alpha /2$ to its right.\nUnlike with the $z$ interval in Section [z_int]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;z_int\u0026rdquo;}, we cannot use a table to find any confidence level that we want. We would, in theory, have to have a table for every possible value of $\\nu$, which would lead to an infinite number of tables. Instead, we use one table and specify certain values of $\\alpha /2$. The table we will use looks like Figure [t_table]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;t_table\u0026rdquo;}.\nThe logic of the $t$ interval proceeds just like the $z$ interval, except that we work with the quantity $P(-t_{\\nu ,\\alpha /2}\\leq t\\leq t_{\\nu ,\\alpha /2})=1-\\alpha$. So, if we want to make a $1-\\alpha$ confidence interval, we need to find $t_{\\alpha /2}$ in the table across the columns. Then we need to find the degrees of freedom down the rows. The intersection of the row and column gives the value of $t_{\\nu },_{\\alpha /2}$. Here are some \u0026ldquo;self-check\u0026rdquo; exercises.\nFind the values of $t_{\\nu ,\\alpha /2}$ for the following confidence interval scenarios.\na). $n=10$, $95%$ confidence [Answer: 2.262]\nb). $n=8;$ $90%$ confidence [Answer: 1.895]\nc). $n=51$, $98%$ confidence [Answer: 2.403]\nExamples Here are some examples.\nA recent survey asked, \u0026ldquo;What is the ideal number of children for a person to have?\u0026rdquo;¬†A sample of $15$ American adults aged $20$ to $30$ reported the following numbers: $1$ $2$ $0$ $3$ $4$ $2$ $0$ $1$ $3$ $4$ $2$ $2$ $2$ $3$ $8$.\na). Find a $93%$ confidence interval for the mean number of children American adults in this age group believe is ideal. Assume the population standard deviation is known to be $2.1$.\nb). Find a $90%$ confidence interval for the mean number of children American adults in this age group believe is ideal under the more realistic assumption that you do not know $\\sigma$. Hint: You need to first calculate $s$, the sample standard deviation (remember that formula?)\nA recent survey of $121$ undergraduates asked the question, \u0026ldquo;How much money do you have available to you right now, in cash and on a debit card?\u0026rdquo;¬†The sample mean was $104.74. Suppose the standard deviation in the population is known to be $$12$. Compute a $99%$ confidence interval for the true mean amount of money that undergraduates have on hand. Interpret the interval in context.\nIn a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8$, respectively. Calculate $95%$ confidence intervals for the amount of time spent studying per week for both accounting and general business majors. Interpret the intervals in context.\nIs there a difference in the average number of hours worked per week between those with a bachelor\u0026rsquo;s degree and those with a high school diploma? A sample of $354$ people with bachelor\u0026rsquo;s degrees and $976$ people with high school diplomas was taken. The average number of hours worked for those with bachelor\u0026rsquo;s degrees was $42.6$ with a standard deviation of $15.5$, and the average number of hours worked for those with high school diplomas was $39.5$ with a standard deviation of $15.1$. Calculate $95%$ confidence intervals for the mean amount of hours worked per week for the two groups.\nAdditional Examples of Confidence Intervals for a Mean An information systems analyst is interested in estimating the mean number of hours per week that executives spend reading and answering email. In a small survey, a random sample of $56$ executives reported their best guess at how much time they spent per week using email. The average reported time was $5.1$ hours with a standard deviation of $7.6$ hours. Calculate a $99%$ confidence interval for the true mean amount of time executives spend reading and answering email.\nOne of the many variables in the General Social Survey is \u0026ldquo;hompop,\u0026rdquo; the total number of people living in a household. In a random sample of $500$ homes, the average number of people living in a home was found to be $2.45$ with a standard deviation of $1.46.$ Find a $98%$ confidence interval for the true mean number of people living in a household.\nThe Texas Tribune lists the salaries of persons employed with public agencies in Texas, including school districts, local governments, teaching hospitals, and universities1. A random sample of $76$ individuals with the title of \u0026ldquo;Nurse\u0026rdquo; resulted in an average annual salary of $$50,376$ and a standard deviation of $$7,825.$ Find a $95%$ confidence interval for the true mean salary of nurses employed by public agencies in Texas. Explain why the mean might not be the best measure of the \u0026ldquo;typical\u0026rdquo; salary of a nurse.\nEmployment is an important issue to many university students. Many people choose to go to college with the aim of finding a steady job. The most recent General Social Survey asked American adults over $18$ years of age how many weeks they worked in the previous year. The data is summarized in the table shown below, which was created using Excel\u0026rsquo;s pivot table feature 2. Find a $95%$ confidence interval for the true mean number of weeks spent at work for persons possessing a bachelor\u0026rsquo;s degree.\nConfidence Interval for a Proportion Just as the case with means, we might be interested in coming up with a range of reasonable values for the true population proportion, given a sample of data. What we will see is that, like the confidence interval for the mean, the interval for a proportion will follow the general formula\n$$\\lbrack estimate]\\pm k\\times \\lbrack standard\\text{ \\ \\ }deviation\\text{ \\ \\ }of\\text{ \\ \\ }estimate]$$\nWith the interval for the mean, $k$ was either a value of the standard normal distribution, $z_{\\alpha /2},$ if the true population standard deviation was known (or the sample size $n$ was large), or it was a value of from Student\u0026rsquo;s $t$ distribution, $t_{\\alpha /2},_{n-1}$, if the true population standard deviation was not known. With the interval for a population proportion, we won\u0026rsquo;t have to worry about two cases; we will always use $z_{\\alpha /2}$ because we will always assume $n$ is large.\nAs an example, let\u0026rsquo;s go back to the issue of marijuana legalization. Suppose you were interested in estimating the proportion of Americans who favored full legalization. The General Social Survey1 is an extensive demographic, behavioral, and attitudinal survey of Americans that has been administered since $1972$. The survey \u0026ldquo;takes the pulse of America\u0026rdquo; according to the National Opinion Research Center (NORC), the organization responsible for the administration of the survey at the University of Chicago. For over forty years, it has tracked the opinions of Americans on hundreds of issues, from marriage to gender roles in society, from views on the ideal number of children couples should have, to opinions on life after death, to belief in the zodiac. The data is completely free to download and analyze.\nThe table below summarizes the results of a question asking individuals to state whether they believed marijuana should be legal or illegal.\n   Response Count of Response     LEGAL $586$   NOT LEGAL $648$   TOTAL $1234$    If the proportion of people who agree with legalization the event of interest, we can define the Bernoulli random variable $X$ as taking the value $1$ if the response is \u0026ldquo;LEGAL\u0026rdquo; and $0$ otherwise. Then the sample proportion is just the average of the $0/1$ data. We can calculate the sample proportion easily as\n$$\\widehat{p}=\\frac{586}{1234}=0.47$$\nTo justify the confidence interval formula, we recall that the sample proportion $\\widehat{P}$ is approximately normally distributed with a mean of $p$ and a standard deviation of $\\sqrt{p(1-p)/n}$. Using the relationship between a standard normal random variable and any normal distribution, we can therefore write\n$$P(-z_{\\alpha /2}\\leq \\frac{\\widehat{P}-p}{\\sqrt{p(1-p)/n}}\\leq z_{\\alpha /2})=1-\\alpha$$\nBy rearranging this inequality, we can get that\n$$P(\\widehat{P}-z_{\\alpha /2}\\sqrt{p(1-p)/n}\\leq p\\leq \\widehat{P}+z_{\\alpha /2}\\sqrt{p(1-p)/n})=1-\\alpha$$\nwhich is a correct but useless interval since it involves $p$, the parameter we are trying to estimate. To work around this problem, we make the natural choice to substitute the calculated value of $\\widehat{p}$ for $% p$ in the formula. This gives us a confidence interval we can actually work with.\nA $(1-\\alpha )100$% confidence interval for the true population proportion, $p$, is given by\n$$\\widehat{p}\\pm z_{\\alpha /2}\\sqrt{\\widehat{p}(1-\\widehat{p})/n}$$\nAs with the confidence interval for a population mean, computing the interval isn\u0026rsquo;t the important part. Interpretation is key. Fortunately, with any confidence interval, our interpretation of the word \u0026ldquo;confidence\u0026rdquo; is in terms of the method. When we are, for example, $95$% confident in a given interval, we are saying that, if we were to repeatedly take samples of size $% n$ from the population, calculate the proportion, and compute the interval, then in the long run, $95$% of those intervals would contain the true value of $p$. We cannot say that the probability is $95$%*¬†that any particular interval contain $p$, since we view* $p$*¬†as a fixed value. Either the specific interval we calculated contains* $p$*¬†or it does not.* We can only say that this particular interval was computed using a method that \u0026ldquo;gets it right\u0026rdquo;¬†$95$% of the time.\nFor the marijuana example, a $95$% confidence interval is then\n$$\\begin{aligned} \u0026amp;\u0026amp;0.47\\pm 1.96\\sqrt{0.47(1-0.47)/1234} \\ \u0026amp;=\u0026amp;0.44\\text{ to }0.50\\end{aligned}$$\nThus, we are $95$% confident that the true proportion of individuals who believe that marijuana should be made legal is between $0.44$ and $0.50$. This result may seem surprising given that marijuana seems to be rising in popularity if we look at television shows, movies, and the news. The reason for the low numbers here is that the data is aggregated, or grouped, which means that younger people¬†(who might tend to be more accepting of marijuana) are put in together with older people who are likely to be less tolerant. In the first example, we will restrict our age limit to $18-30$ to see if we get different results.\nThe number of individuals for and against legalization of marijuana in the $% 18-30$ age group is given in Figure [grass_table]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;grass_table\u0026rdquo;}. Calculate a $99$% confidence interval for the true proportion of individuals $18-30$ supporting legalization. Interpret the interval in context.\nThe GSS asked the question, \u0026ldquo;Are taxes on high income people too high?\u0026rdquo; The results are shown below in Figure [rich_tax]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;rich_tax\u0026rdquo;}.\nCalculate a $90$% confidence interval for the true proportion of individuals who feel that taxes are \u0026ldquo;about right\u0026rdquo; for high-income people. Interpret the interval in context.\nA social science researcher is studying Americans\u0026rsquo; beliefs about premarital sex. Using the GSS figure below,¬†Figure [sex_table]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;sex_table\u0026rdquo;}, calculate a $95$% confidence interval for the true proportion of Americans who believe that premarital sex is \u0026ldquo;not wrong at all\u0026rdquo; or \u0026ldquo;wrong only sometimes.\u0026rdquo;\n  See http://www3.norc.org/gss+website/ \u0026#x21a9;\u0026#xfe0e;\n See http://office.microsoft.com/en-us/excel-help/pivottable-reports-101-HA001034632.aspx for more information about pivot tables. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"651b32d4cefb439cadcc458a36daa026","permalink":"/courses/bana3363/5-ci-for-mean/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/bana3363/5-ci-for-mean/","section":"courses","summary":"Concepts of Inference Our ultimate goal is to obtain a reasonable \u0026ldquo;guess\u0026rdquo;¬†at the true value of a process parameter (in this section, the population/process mean $\\mu$) using a sample of data.","tags":null,"title":"Confidence Interval for the Mean","type":"docs"},{"authors":null,"categories":null,"content":"Hypothesis Testing Inference for a Single Population/Process Mean Basic Concepts In the previous two handouts, we discussed confidence intervals for a population mean and a population proportion. Confidence intervals give us a range of reasonable values for the true parameter, based on the data at hand. That is, confidence intervals \u0026ldquo;fix the data\u0026rdquo; and create an interval of possible values of the parameter are consistent with what has been observed. Hypothesis testing addresses the inference problem from another angle by fixing the parameter at a specified value and determining if the observed data is consistent with that chosen value.\nTo discuss hypothesis testing, we need some terms. Let\u0026rsquo;s begin with some definitions:\nA hypothesis test (also called a significance test ) is a method of using data to evaluate the evidence about a specified value of a parameter.\nEvery statistical test has two mutually exclusive hypotheses, or claims about the value of a parameter.\nThe null hypothesis, denoted $H_{0}$ and pronounced \u0026ldquo;$H$ not\u0026rdquo;¬†or \u0026ldquo;$H$ sub-zero,\u0026rdquo;¬†is typically a statement of \u0026ldquo;no effect,\u0026rdquo;¬†\u0026ldquo;no difference,\u0026rdquo;¬†\u0026ldquo;no special ability beyond random chance,\u0026rdquo;¬†\u0026ldquo;equality,\u0026rdquo;¬†etc. This is usually what the researcher wants to disprove or **reject**.\nThe alternative hypothesis, denoted $H_{1}$, is the complement (\u0026ldquo;opposite\u0026rdquo;) of the null, and usually what the researcher is trying to establish.\nHypotheses always come in pairs, and the general forms of the hypothesis pairs are given in Figure [hyps]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;hyps\u0026rdquo;}.\nWhen performing a hypothesis test, it is possible to be wrong, or, more formally, to commit an error. Here are the two types of errors we\u0026rsquo;ll be concerned with (there is a third type as well but we won\u0026rsquo;t worry about that one).\nA Type I error occurs when we reject a true null hypothesis. This is akin to a \u0026ldquo;false positive\u0026rdquo; (claiming there is an effect or difference in the population when there isn\u0026rsquo;t).\nA Type II error occurs when we don\u0026rsquo;t reject a false null hypothesis. This is akin to a \u0026ldquo;false negative\u0026rdquo; (claiming there is not an effect or difference in the population when there is).\nThese errors are depicted in Figure below.\nFor any given test, we don\u0026rsquo;t know if we have committed a Type I error because we don\u0026rsquo;t know what the true value of the parameter is (if we did, we wouldn\u0026rsquo;t be doing any testing!). What we can do is specify that we want to construct our test such that the $P($Type¬†I error$)$ $=\\alpha$ is \u0026ldquo;small.\u0026rdquo; That is, if we could imagine doing the test repeatedly for different samples from the same population, in the long run, we would only want an error $\\alpha (100)%$ of the time. A similar criterion exists for a Type II error: we want $P($Type II error) $=\\beta$ to be small as well.\nThe two types of errors cannot be minimized at the same time; lowering one error probability raises the other. What is done in practice is to find tests with a small $\\alpha$, and from that set, find the test that also has a small $\\beta$. For our purposes, we will only consider Type I error. It can be shown that the hypothesis tests presented next also have \u0026ldquo;small\u0026rdquo; $% \\beta$.\nHypothesis testing can be thought of as a process with a number of steps. Here they are:\n Define the hypotheses Calculate the test statistic Calculate the p-value or determine the rejection region based on (1) Make a conclusion  We already talked about $(1)$ in Figure [hyps]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;hyps\u0026rdquo;}. Here are some more definitions:\nA test statistic is a specific function of data and the parameter value specified by the null hypothesis.\nThe test statistic is chosen so that it has a known probability distribution (e. g. the standard normal or Student\u0026rsquo;s t) when the null hypothesis is true.\nThe p-value of a test is the probability, assuming the null is true, that we would observe a value of the test statistic as extreme or more extreme than what we actually got.\nA rejection region is the set of all values of the test statistic distribution for which we would reject $H_{0}$.\nWe make a conclusion based on the $p$-value or the rejection region. In either case, we decide before we analyze the data what the Type I error rate $\\alpha$ should be. Typical choices are $0.05,0.01,$ or $0.10$ (in order of popularity in research). $P$-values are more commonly used in practice because they are calculated easily using software. Using $p$ -values, the judgement is simple:\n if the $p$-value $\\leq \\alpha$, then we reject the null hypothesis if the $p$-value $\u0026gt;\\alpha$, we fail to reject the null hypothesis  Notice that we do not say \u0026ldquo;accept the null hypothesis.\u0026rdquo; To \u0026ldquo;fail to reject\u0026rdquo;¬†means to say that there is not enough evidence to reject the null hypothesis. It is a weak conclusion, similar to the \u0026ldquo;not guilty\u0026rdquo;¬†verdict in the U. S. court system. When someone is found \u0026ldquo;not guilty,\u0026rdquo; it does not mean that the person is innocent; rather such a verdict claims lack of evidence to support a conviction. We can never conclude that the null hypothesis is true using data. There is an entire set of values of the parameter that you could plug into the null hypothesis that would lead to a \u0026ldquo;fail to reject\u0026rdquo; decision. This set is, in fact, the $(1-\\alpha )100%$ confidence interval for the parameter (see Theorem [ci_tst_theorem]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;ci_tst_theorem\u0026rdquo;} below).\nAlternatively, we could, using our specified value of $\\alpha$, the form of the hypothesis, and the distribution of the test statistic under the null hypothesis to decide on a critical value that defines the boundary of the rejection region on the distribution of the test statistic. This region is a collection of values of the test statistic that have a low probability, specified as $\\alpha$, of being observed if the null hypothesis is true. The logic is that if our calculated test statistic falls in the rejection region, either a very unlikely event occurred or the null hypothesis is false. Practically, if our test statistic falls in the rejection region, we reject the null hypothesis.\nA visual of this idea is shown in Figure [rr]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;rr\u0026rdquo;} for a test of a population mean with a sample size of $15$. In this example, let\u0026rsquo;s assume that under the null hypothesis, the test statistic $t_{0}$ has Student\u0026rsquo;s $t$ distribution with $n-1=14$ degrees of freedom. If the alternative hypothesis $H_{1}$ is that the true population mean $\\mu \u0026gt;\\mu _{0}$, for some specified $\\mu _{0},$ then \u0026ldquo;large\u0026rdquo; values of $t_{0}$ would support the alternative and make us doubt the null hypothesis that $\\mu \\leq \\mu _{0}$. Our rejection region is defined as all values of the Student\u0026rsquo;s $t$ distribution that are larger than the critical value. To determine the critical value, we would first have to set $\\alpha$ to some small value, say $0.05.$ In this example, the critical value is $1.761,$ since $P(t_{n-1}\u0026gt;1.761)=0.05.$ If the calculated value of the test statistic falls in the shaded region, we would reject the null hypothesis; otherwise we would fail to reject.\nYou might wonder why we\u0026rsquo;re using $\\alpha$ for both confidence intervals and hypothesis tests. There is a good reason! Here is an important fact connecting hypothesis testing and confidence intervals:\n[[ci_tst_theorem]]{#ci_tst_theorem label=\u0026quot;ci_tst_theorem\u0026rdquo;}If a $(1-\\alpha )100%$ confidence interval contains the parameter value specified by $H_{0},$ then a test of $H_{0}$ conducted at the $\\alpha$ level will **fail to reject** the null hypothesis.\nThis fact can be proven but the intuition is simple: if the interval contains the most reasonable values of the parameter (based on the data and the confidence level) then it\u0026rsquo;s sensible that the test should not reject one of the values that lies inside the interval. This logic extends to all hypothesis tests by construction.\nA Hypothesis Test for a Single Population/Process Mean Let\u0026rsquo;s now address a specific test. In every equation that follows, $\\mu _{0}$ is a specified value of $\\mu$, such as $6.2$, that we are testing.\n  Specify the hypotheses: Choose one of the three hypotheses depending upon what you are trying to show.\n$$\\begin{aligned} \\text{(a) }H_{0} \u0026amp;:\u0026amp;\\mu \\leq \\mu _{0}\\text{ vs. }H_{1}:\\mu \u0026gt;\\mu _{0} \\ \\text{(b) }H_{0} \u0026amp;:\u0026amp;\\mu \\geq \\mu _{0}\\text{ vs. }H_{1}:\\mu \u0026lt;\\mu _{0}\\text{ } \\ \\text{(c) }H_{0} \u0026amp;:\u0026amp;\\mu =\\mu _{0}\\text{ vs. }H_{1}:\\mu \\neq \\mu _{0}\\end{aligned}$$\n  As you can see, the alternative hypothesis is the complement or \u0026ldquo;opposite\u0026rdquo; of the null.\n  Calculate the test statistic:\nFor a test of a single mean, it can be shown that the best test statistic is\n$$t=\\frac{\\overline{y}-\\mu _{0}}{s/\\sqrt{n}}$$\n  Notice that we plug in the null hypothesis value of $\\mu _{0}$ into the test statistic. We do this because it turns out that if we can reject for this specific value, we could also reject for any value less than (in the case of hypothesis (a)) or any value greater than (in the case of hypothesis set (b)) $\\mu _{0}.$\nWhen the null hypothesis is true, $t$ has the Student\u0026rsquo;s $t$ distribution with $n-1$ degrees of freedom. Therefore, we know that $P(-t_{\\alpha /2,n-1}\u0026lt;t,\u0026lt;t_{\\alpha /2,n-1})=1-\\alpha$. This means that there is a total area of $\\alpha$ either greater than $t_{\\alpha /2,n-1}$ or less than $-t_{\\alpha /2,n-1}.$ We use this fact in step 3 to determine the rejection region.\n  Calculate the $p$-value (or determine the rejection region)\nIn practice, software almost always reports $p$-values by default. Without software, you cannot calculate $p$-values for the Student\u0026rsquo;s $t$ distribution. Still, the concept of a $p$-value is important, so it is useful to see how they could be calculated in theory. The form of the $p$ -value depends on the form of the alternative hypothesis. The following table gives the $p$-values associated with the hypotheses in (1). $$\\begin{aligned} \\text{(a) }p-value \u0026amp;=\u0026amp;P(t_{n-1}\u0026gt;t) \\ \\text{(b) }p-value \u0026amp;=\u0026amp;P(t_{n-1}\u0026lt;t) \\ \\text{(c) }p-value \u0026amp;=\u0026amp;2\\min [P(t_{n-1}\u0026gt;t),P(t_{n-1}\u0026lt;t)]\\end{aligned}$$\n  The rejection regions also depend on the form of the alternative hypothesis. The general appearances of these rejection regions are given in the following figures.\nReferring to step 1 above, the rejection region for hypothesis set (a) shown in Figure [right_tail]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;right_tail\u0026rdquo;} contains all values of $t$ that are \u0026ldquo;too large,\u0026rdquo; expected to be seen with probability $\\alpha$ or less. A test with rejection region in the right tail is called a right-tailed test.\nThe rejection region for hypothesis set (b) shown in Figure [left_tail]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;left_tail\u0026rdquo;} contains all values of $t$ that are \u0026ldquo;too small,\u0026rdquo; expected to be seen with probability $\\alpha$ or less. A test with rejection region in the left tail is called a left-tailed test.\nThe rejection region for hypothesis set (c) contains all the values of $t$ that are either \u0026ldquo;too small\u0026rdquo; or \u0026ldquo;too large.\u0026rdquo; Since the $t$ distribution is symmetric around 0 under the null hypotheses, by convention we split $\\alpha$ into two equal parts, putting $\\frac{\\alpha }{2}$ area in each tail. Because there is a rejection region on either side of the distribution, this type of test is known as a two-tailed test. See Figure [two_tail]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;two_tail\u0026rdquo;} below.\n Make a conclusion.  If the $p$-value $\\leq \\alpha$, we reject $H_{0}.$ If the $p$-value $% \u0026gt;\\alpha$, we fail to reject $H_{0}.$ Equivalently, if $t$ lies within the rejection region, we reject $H_{0}$. If $t$ lies outside the rejection region, we fail to reject $H_{0}.$\nHere are some examples.\nA machine produces ball bearings is calibrated to produce diameters of $0.5$ inches on average. A sample of $10$ ball bearings from the process was taken. The average diameter was $0.493$ inches, with a standard deviation of $0.022$ inches. A quality control inspector wants to determine if the machine is calibrated properly. Conduct a hypothesis test at the $\\alpha =0.05$ level to answer his question.\nA specialty cell phone manufacturer wants to determine how many pictures are stored on phones used by older consumers. A survey was conducted and a random sample of six older consumers reported the following numbers of pictures on their cell phones: $13$ $\\ 6$ $\\ \\ 9$ $\\ \\ 10$ $\\ \\ 16$ $\\ 18.$ Historically, the average number of pictures stored on phones by this consumer group has been $10.$ The company wants to know if this number has increased. Use a hypothesis test at the $\\alpha =0.10$ level to answer this question.\nA recent article on pregnancy1 discussed a study performed in California on the effects of maternal age on birth defects. The study found that the best age for a woman to have a first child, in terms of the lowest rate of birth defects was $26.$ In the $% 2012$ General Social Survey (GSS), a sample of $846$ women were asked the question, \u0026ldquo;How old were you when you had your first child?\u0026rdquo; The average age was $25.1$ years, with a standard deviation of $5.2$ years. Test the hypothesis that the mean age of GSS respondents is less than $26$ at the $% \\alpha =0.05$ level.\nA researcher is interested in the amount of time young men $18-30$ spend on the internet per week. In the $2012$ GSS, $121$ men aged $18-30$ reported an average time on the internet of $15.6$ hours per week, with a standard deviation of $18.8$ hours. Test the hypothesis that the average time spent on the internet for young men is greater than $14$ hours, at the $\\alpha =0.01$ level. Then, construct a $99%$ confidence interval for the true average time spent on the internet.\n  http://www.huffingtonpost.com/robin-marantz-henig/whats-the-best-age-to-hav_b_2206136.html \u0026#x21a9;\u0026#xfe0e;\n   ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"25f62534ce40f5189066d14dd45c7e0b","permalink":"/courses/bana3363/5-1-hypothesis-testing-basics/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/bana3363/5-1-hypothesis-testing-basics/","section":"courses","summary":"Hypothesis Testing Inference for a Single Population/Process Mean Basic Concepts In the previous two handouts, we discussed confidence intervals for a population mean and a population proportion. Confidence intervals give us a range of reasonable values for the true parameter, based on the data at hand.","tags":null,"title":"Hypothesis Testing for a Population Mean","type":"docs"},{"authors":null,"categories":null,"content":"Many times we are not interested only in a single group, but in multiple $% (\\geq 2)$ groups. For example, we might compare the amount of weight lost on a new drug for men $45-60$. Let Group $1$ be men $45-60$ who follow a specific healthful diet and let Group $2$ be men $45-60$ who follow the same diet but also take a weight loss drug. In the language of experimental design, Group $1$ is a control group, the group that receives no treatment (i.e., a specific manipulation of the members of the group), or no treatment above a reasonable standard. Group $2$ is an experimental or treatment group.\nThe general notation for inference about two population/process means is as follows.\n   Metric Group 1 Group 2     Random sample of size $n_{1}:$ $X_{11},X_{12},\\ldots ,X_{1n_{1}}$ $n_{2}:$ $X_{21},X_{22},\\ldots ,X_{2n_{2}}$    Mean $\\mu_{1}$ $\\mu_{2}$   Standard Deviation: $\\sigma_{1}$ $\\sigma_{2}$    Sample Mean $\\overline{x}{1}=\\frac{\\sum{j}x_{1j}}{n_{1}}$ $%\\overline{x}{2}=\\frac{\\sum{j}x_{2j}}{n_{2}}$   Sample Standard Deviation $s_{1}=\\sqrt{\\frac{\\sum_{j}x_{1j}^{2}-n_{1}%$ $s_{2}=\\sqrt{% \\overline{x}_{1}^{2}}{n_{1}-1}}\\frac{\\sum_{j}x_{2j}^{2}-n_{2}\\overline{x}_{2}^{2}}{n_{2}-1}}$    By the fundamental assumption of statistics (i.e., that variability is inescapable), any one person in Group $1$ could lose more weight than any one person in Group $2$, even if the drug is effective, simply by random chance. See Figure [drug_diet]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;drug_diet\u0026rdquo;}. The question, then, is not whether the drug works for everyone, but if it \u0026ldquo;works on average.\u0026rdquo;¬†The average for Group $1$ we can specify as $\\mu_{1}$ and the average for Group 2 we can specify as $\\mu_{2}.$ Then the question is whether $\\mu_{1}=\\mu_{2},$ or, equivalently, whether $\\mu_{1}-\\mu_{2}=0.$ We can address this question either through a confidence interval or a hypothesis test. We will discuss the interval here.\nThere are two cases to consider:\n  You know (or assume) that the two population standard deviations are the same, that is, $\\sigma_{1}=\\sigma_{2}$\n  You don\u0026rsquo;t know (or don\u0026rsquo;t wish to assume) that the two population standard deviations are the same, that is $\\sigma_{1}\\neq \\sigma_{2}$\n  Case 1: Confidence Interval for Difference in Means $\\sigma_{1}=\\sigma_{2}$ A $(1-\\alpha )100%$ confidence interval for $\\mu_{1}-\\mu_{2}$ is given by $\\overline{x}_{1}-\\overline{x}_{2}\\pm t_{n_{1}+n_{2}-2,\\alpha /2}s_{p}\\sqrt{% \\frac{1}{n_{1}}+\\frac{1}{n_{2}}}$, where $s_{p}=\\sqrt{\\frac{% (n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2}}$ is the \u0026quot;pooled\u0026quot; standard deviation from the two samples.\nCase 2: Confidence Interval for Difference in Means $\\sigma_{1}\\neq \\sigma_{2}$ A $(1-\\alpha )100%$ confidence interval for $\\mu_{1}-\\mu_{2}$ is given by $\\overline{x}_{1}-\\overline{x}_{2}\\pm t_{\\nu ,\\alpha /2}\\sqrt{\\frac{s_{1}^{2}% }{n_{1}}+\\frac{s_{2}^{2}}{n_{2}}},$ where $\\nu =\\frac{\\left( \\frac{s_{1}^{2}% }{n_{1}}+\\frac{s_{2}^{2}}{n_{2}}\\right) ^{2}}{\\frac{\\left( \\frac{s_{1}^{2}}{% n_{1}}\\right) ^{2}}{n_{1}-1}+\\frac{\\left( \\frac{s_{2}^{2}}{n_{2}}\\right) ^{2}% }{n_{2}-1}}$ .\nThe Case $2$ interval makes one less assumption than Case $1$, so it is closer to the truth (in reality, it\u0026rsquo;s unlikely that the two standard deviations will be exactly equal). On the other hand, it is known that the Case $2$ interval is only an approximate interval, although the approximation is usually quite good. As with most things in life, there is a trade-off. In general, however, Case $2$ is the safer bet. The degrees of freedom parameter, $\\nu ,$ is always calculated for you using software. In large samples $\\nu$ will be large as well, so $t_{\\nu ,\\alpha /2}$ will almost be the same as $z_{\\alpha /2},$ the critical value from the standard normal distribution.\nHere are some examples.\nIn a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8,$ respectively. Calculate a $90%$ confidence interval for the mean difference in the amount of time spent studying per week between accounting and general business majors. Interpret the interval in context. Assume the population standard deviations are equal.\nA restaurant is investigating a new device that allows restaurant users to swipe their own credit cards at the table rather than wait for the server to pick up the check. In an experiment to determine which of two methods (the new device or the usual procedure) leads to larger tips, a random sample of $10$ receipts from the usual method and $11$ receipts from the new device were gathered. The mean value of the tip for customers using the device was $12.62$ with a standard deviation of $1.72$, and the mean and standard deviation for the usual method were $14.20$ and $1.61$. Construct a $90%$ confidence for the true mean difference in work hours, assuming the two population standard deviations are equal.\nIs there a difference in the average number of hours worked per week between those with a bachelor\u0026rsquo;s degree and those with a high school diploma? A sample of $354$ people with bachelor\u0026rsquo;s degrees and $976$ people with high school diplomas was taken. The average number of hours worked for those with bachelor\u0026rsquo;s degrees was $42.6$ with a standard deviation of $15.5$, and the average number of hours worked for those with high school diplomas was $39.5$ with a standard deviation of $15.1$.\na). Construct a $99%$ confidence interval for the true mean difference in work hours, assuming the two population standard deviations are the same.\nb). Explain why assuming the standard deviations were not the same would not have any practical effect on your calculation of the interval in this case.\n","date":1595949567,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595949567,"objectID":"727d2a6b0729b37721af19876e3d62bb","permalink":"/courses/bana3363/6-1-interval-for-differences-in-means-independent-samples/","publishdate":"2020-07-28T15:19:27.628999Z","relpermalink":"/courses/bana3363/6-1-interval-for-differences-in-means-independent-samples/","section":"courses","summary":"Many times we are not interested only in a single group, but in multiple $% (\\geq 2)$ groups. For example, we might compare the amount of weight lost on a new drug for men $45-60$.","tags":null,"title":"Interval Inference about Two Population/Process Means","type":"docs"},{"authors":null,"categories":null,"content":"Confidence Interval for the Difference in Means (Matched Pairs) The confidence interval that we examined in the last handout applies when we are working with two independent samples. Often, however, it makes sense to examine the effect of a treatment on samples that are in some sense related. These samples are called matched pairs. One way a matched pair can form is if each unit that is sampled (a person or object) is studied before and after a treatment is applied. The two sets of data\u0026ndash;measurements before and after treatment\u0026ndash;will not be independent because the same entity is being measured both times. Another way to form matched pairs is to make groups that are related genetically (e.g. twins), by proximity to one another (e.g., neighbors), by time of measurement (e.g., months of the year), by relationship to one another (e.g., husband and wife), or in some other fashion (e.g., socioeconomic status, GPA).\nWhen we have a matched pairs design, the analysis is actually easier than for the two independent samples case. What we do, essentially, is make a new variable that represents the differences between the groups on the measure of interest, and then perform one-sample techniques for $\\mu_{d},$ which is our notation for the population mean difference. We need to calculate $\\overline{y}_{d},$ the sample mean difference, and $s_{d},$ the standard deviation of the differences. Both of these are calculated using the number of pairs, $n_{d}.$ The formula for the confidence interval is given now.\nA $(1-\\alpha )100%$ confidence interval for the mean difference between two related groups, denoted $\\mu_{d},$ is\n$$\\overline{y_{d}}\\pm t_{\\alpha /2,n_{d}-1}\\frac{s_{d}}{\\sqrt{n_{d}}}$$\nHere are some examples.\nA sociologist wants to determine if children, on average, are more educated than their parents. Using the General Social Survey, she takes a sample of $12$ respondents and compares the number of years of formal education the respondent reported (\u0026ldquo;EDUC\u0026rdquo;) with the average of the years the respondent\u0026rsquo;s mother and father spent in formal schooling¬†(\u0026ldquo;P_EDUC\u0026rdquo;). The data are shown below. Calculate a $95%$ confidence interval for the true difference in the mean number of years of education between children and parents.\n   EDUC P_EDUC     16 16   12 12   12 15   13 17   16 12   19 13   15 13   11 11   9 12   17 13   16 13   12 8    In a recent study, the effect that cell phone use had on reaction time while driving was studied. A sample of $20$ drivers was selected and each one was asked to drive through an obstacle course. Each person\u0026rsquo;s average reaction time (in seconds) to stimuli along the course was recorded both during a drive with no cell phone use and during a drive where the subject was engaged in a casual conversation with a friend. The data are summarized in the table below. Compute a $99%$ confidence interval for the true average difference in reaction time between using a cell phone and not using a cell phone. Use the fact that $\\overline{y_{d}}=0.067$ and $s_{d}=0.029.$\n   Driver Cell No_Cell Difference     1 0.85 0.78 0.07   2 0.91 0.85 0.06   3 0.84 0.74 0.1   4 0.81 0.75 0.06   5 0.8 0.73 0.07   6 0.85 0.76 0.09   7 0.87 0.78 0.09   8 0.85 0.79 0.06   9 0.84 0.77 0.07   10 0.85 0.82 0.03   11 0.93 0.85 0.08   12 0.85 0.79 0.06   13 0.85 0.8 0.05   14 0.86 0.79 0.07   15 0.91 0.91 0   16 0.8 0.74 0.06   17 0.9 0.82 0.08   18 0.91 0.83 0.08   19 0.82 0.72 0.1   20 0.83 0.78 0.05   21 0.92 0.82 0.1   22 0.85 0.83 0.02   23 0.91 0.88 0.03   24 0.92 0.8 0.12   25 0.86 0.77 0.09   26 0.87 0.78 0.09   27 0.79 0.78 0.01   28 0.84 0.74 0.1   29 0.9 0.81 0.09   30 0.84 0.8 0.04    A consumer group wants to determine if husbands and wives spend differing amounts of money on each other on Valentine\u0026rsquo;s Day. A sample of seven married couples who had been married at least one year was taken, and the data (in dollars spent) are shown in the table below. Compute a $95%$ confidence interval for the true average difference between husbands and wives in the amount of money spent on Valentine\u0026rsquo;s Day .\n   Couple Husband Wife Difference     1 25 18 7   2 21 42 -21   3 38 55 -17   4 64 41 23   5 52 37 15   6 16 26 -10   7 26 24 2    How objectively do we view ourselves in terms of attractiveness? A website offers to rate an individual based on facial characteristics on a scale from $1$ to $10$, with $10$ being most attractive, using a photo. The program incorporates current research on characteristics of human faces that are thought to be associated with attractiveness1. A random sample of five people uploaded a photo the website. Before their scores were given, they were asked to predict what the computer would say. The data are shown below. Compute a $90%$ confidence interval for the true mean difference between a person\u0026rsquo;s self rating and the computer\u0026rsquo;s rating.\n   Couple Husband Wife Difference     1 25 18 7   2 21 42 -21   3 38 55 -17   4 64 41 23   5 52 37 15   6 16 26 -10   7 26 24 2      Here is one research paper describing an attempt to build a program to predict attractiveness: http://papers.nips.cc/paper/3111-a-humanlike-predictor-of-facial-attractiveness.pdf. Here\u0026rsquo;s a website that will rate your attractiveness with a photo: http://www.anaface.com/ \u0026#x21a9;\u0026#xfe0e;\n   ","date":1595949567,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595949567,"objectID":"529d5b8bdb89f33306dd7d96c7709584","permalink":"/courses/bana3363/9-confidence-interval-for-matched-pairs/","publishdate":"2020-07-28T15:19:27.628999Z","relpermalink":"/courses/bana3363/9-confidence-interval-for-matched-pairs/","section":"courses","summary":"Confidence Interval for the Difference in Means (Matched Pairs) The confidence interval that we examined in the last handout applies when we are working with two independent samples. Often, however, it makes sense to examine the effect of a treatment on samples that are in some sense related.","tags":null,"title":"Confidence Interval for Matched Pairs","type":"docs"},{"authors":null,"categories":null,"content":"A Hypothesis Test for Two Population/Process Means (Independent Samples) Here is the procedure for testing the difference between two population/process means from independent samples. You can see that it follows the general pattern that we have seen.\n  Specify the hypotheses.\n$$\\begin{aligned} \\text{(a) }H_{0} \u0026amp;:\u0026amp;\\mu _{1}-\\mu _{2}\\leq d\\text{ vs. }H_{1}:\\mu _{1}-\\mu _{2}\u0026gt;d\\text{ } \\text{(b) }H_{0} \u0026amp;:\u0026amp;\\mu _{1}-\\mu _{2}\\geq d\\text{ vs. }H_{1}:\\mu _{1}-\\mu _{2}\u0026lt;d \\text{(c) }H_{0} \u0026amp;:\u0026amp;\\mu _{1}-\\mu _{2}=d\\text{ vs. }H_{1}:\\mu _{1}-\\mu _{2}\\neq d\\end{aligned}$$\nwhere $-\\infty \u0026lt;d$ $\u0026lt;\\infty$ is some specified difference (usually $0$, but it could be anything$)$.\n  Calculate the test statistic.\nCase 1: $\\sigma _{1}=\\sigma _{2}$ (the only case we will consider in this class)\n$$t_{0}=\\frac{\\overline{x}_{1}-\\overline{x}_{2}-d}{s_{p}\\sqrt{\\frac{1}{n_{1}}+% \\frac{1}{n_{2}}}}$$\nwhere $s_{p}=\\sqrt{\\frac{(n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2% }}$ is the pooled standard deviation estimate.\nCase 2: $\\sigma _{1}\\neq \\sigma _{2}$\n$$t_{0}=\\frac{\\overline{x}_{1}-\\overline{x}_{2}-d}{\\sqrt{\\frac{s_{1}^{2}}{n_{1}% }+\\frac{s_{2}^{2}}{n_{2}}}}$$\n  Compute the p-value\nCase 1: $\\sigma _{1}=\\sigma _{2}$\n$$\\begin{aligned} \\text{(a) }p \u0026amp;=\u0026amp;P(t_{n_{1}+n_{2}-2}\u0026gt;t_{0}) \\text{(b) }p \u0026amp;=\u0026amp;P(t_{n_{1}+n_{2}-2}\u0026lt;t_{0}) \\text{(c) }p \u0026amp;=\u0026amp;2\\min [P(t_{n_{1}+n_{2}-2}\u0026gt;t_{0}),P(t_{n_{1}+n_{2}-2}\u0026lt;t_{0})]\\end{aligned}$$\nCase 2: $\\sigma _{1}\\neq \\sigma _{2}$\n$$\\begin{aligned} \\text{(a) }p \u0026amp;=\u0026amp;P(t_{\\nu }\u0026gt;t_{0}) \\text{(b) }p \u0026amp;=\u0026amp;P(t_{\\nu }\u0026lt;t_{0}) \\text{(c) }p \u0026amp;=\u0026amp;2\\min [P(t_{\\nu }\u0026gt;t_{0}),P(t_{\\nu }\u0026lt;t_{0})]\\end{aligned}$$\nwhere $\\nu =\\frac{\\left( \\frac{s_{1}^{2}}{n_{1}}+\\frac{s_{2}^{2}}{n_{2}}% \\right) ^{2}}{\\frac{\\left( \\frac{s_{1}^{2}}{n_{1}}\\right) ^{2}}{n_{1}-1}+% \\frac{\\left( \\frac{s_{2}^{2}}{n_{2}}\\right) ^{2}}{n_{2}-1}}$.\nAlternatively, we can define the rejection region as follows:\n$$\\begin{aligned} \\text{(a)}{\\text{ }t_{n_{1}+n_{2}-2} \u0026amp;:\u0026amp;t_{n_{1}+n_{2}-2}\u0026gt;\\text{ }t_{\\alpha ,t_{n_{1}+n_{2}-2}}} \\text{(b) }{t_{n_{1}+n_{2}-2} \u0026amp;:\u0026amp;t_{n_{1}+n_{2}-2}\u0026lt;-t_{\\alpha ,t_{n_{1}+n_{2}-2}}} \\text{(c) }{t_{n_{1}+n_{2}-2} \u0026amp;:\u0026amp;|t_{n_{1}+n_{2}-2}|\u0026gt;\\text{ }t_{\\alpha /2,t_{n_{1}+n_{2}-2}}}\\end{aligned}$$\n  Make a conclusion\nIf the $p$-value $\\leq \\alpha ,$ we reject $H_{0}$. If the $p$-value $% \u0026gt;\\alpha ,$ we fail to reject $H_{0}$. Equivalently, if $t_{0}$ lies within the rejection region, we reject $H_{0}$. If $t_{0}$ lies outside the rejection region, we fail to reject $H_{0}$.\nHere are some examples:\n  In a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8,$ respectively. Determine at the $% \\alpha =0.05$ level if there is a difference in the population mean amount of time spent studying per week between accounting and general business majors. Assume the population standard deviations are equal.\nIn a recent survey of $121$ undergraduates, students reported the number of hours per week they spend on school work. Figure [table1]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;table1\u0026rdquo;} below summarizes the data for finance and marketing majors.\nA researcher wants to see if marketing majors and finance majors differ in the amount of time they study per week on average. Test the hypothesis at the $10%$ level of significance, assuming the two population standard deviations are equal. What could you say about a $90%$ confidence interval for the difference in the two group means?\nA clothing store wants to determine which of two methods, a security guard or a video surveillance system, would be more effective in reducing losses due to theft. For six months, the store hired a security guard and recorded monthly losses, and for another six months, the store used cameras. The table below summarizes the data. The manager of the store wants to know if the cameras are more effective at reducing the average loss, because they are cheaper to use than a guard. population standard deviations are the same.\na). Test the hypothesis at the $\\alpha =0.05$ level of significance, assuming the population standard deviations are the same.\nb). Find a $95%$ confidence interval for the true mean difference in average loss between guards and cameras, assuming the population standard deviations are the same.\nConfidence Interval for Difference in Proportion We might be interested in the difference in the proportions of two groups who meet a certain criterion. For example, we might want to determine what type of website banner ad (one that flashes or one that is steady) leads to a higher proportion of people clicking on it. We might want to determine which of two packaging designs leads to a higher proportion of people who indicate they would buy the product. An excellent example of inference for proportions comes from prescription drug testing. Researchers might be interested to see if a new cancer drug has a reduced chance of causing nausea in patients compared to an existing drug. Opinion polls on issues such as marijuana legalization can be analyzed using techniques for differences in proportions.\nThe general notation for inference about two population/process proportions is similar to that of two population/process means. The big difference is that the data for both groups can only take on the values of $0$ or $1,$ with $1$ indicating a \u0026quot;success\u0026quot; (either a click on an ad, an indication to purchase, or a person indicating that they did not experience nausea, for example).\n   Metric Group 1 Group 2     Sample of size $n_{1}:$ $X_{11},X_{12},\\ldots ,X_{1n_{1}}$ , all either $0$ or $1$ $n_{2}:$ $X_{21},X_{22},\\ldots ,X_{2n_{2}},$ all either $0$ or $1$   Mean: $p_{1}$ $p_{2}$   Sample Proportion $\\widehat{p_{1}}=\\frac{\\sum_{j}x_{1j}}{n_{1}}$ (i.e., number of successes in Group 1) $\\widehat{p_{2}}=\\frac{\\sum_{j}x_{2j}}{n_{2}}$ (i.e., number of successes in Group 2)   Estimated Standard Deviation $\\sqrt{\\frac{\\widehat{p_{1}}(1-\\widehat{p_{1}{n_1}}$ $\\sqrt{\\frac{\\widehat{p_{2}}(1-\\widehat{p_{2}}{n_{2}}}$    We can now introduce the confidence interval for the difference in two proportions.\nA $100(1-\\alpha )%$ confidence interval for the difference in two population proportions $p_{1}-p_{2}$, if the two sample sizes $n_{1}$ and $% n_{2}$ are sufficiently large, is given by\n$$\\widehat{p_{1}}-\\widehat{p_{2}}\\pm z_{\\alpha /2}\\sqrt{\\frac{\\widehat{p_{1}}% (1-\\widehat{p_{1}})}{n_{1}}+\\frac{\\widehat{p_{2}}(1-\\widehat{p_{2}})}{n_{2}}}$$\nLet\u0026rsquo;s work with an example that should be familiar by now.\nThe number of individuals for and against legalization of marijuana in the $% 18-30$ and $31$ and over age groups are given in the table below. Find a $% 95%$ confidence interval for the true difference in the proportion of people favoring marijuana legalization between the two groups.\n   Response (2012) Count (18-30) Count (31 \u0026amp; over) Total     LEGAL $131$ $455$ $586$   NOT LEGAL $104$ $541$ $645$   TOTAL $235$ $996$ $1231$    In a recent study, a drug manufacturer wanted to determine if a new antidepressant (Drug X) could also reduce pain that often comes with diabetes, called diabetic neuropathy. The data for the study are shown below. Calculate a $92%$ confidence interval for the difference in the proportion of people who experience relief from Drug X and the placebo.\nA researcher is interested to see if there is a difference between men and women when it comes to certain social behavior. The data from the study are shown below. Calculate a $90%$ confidence interval for the difference in the proportion of men and women who have had sex with an acquaintance last year.\nJust as with means, we might be interested in testing whether two population proportions are equal. We will discuss this in the next section.\nHypothesis Test for Difference in Proportions The general four-step procedure still applies. What changes are the specifics.¬†Let\u0026rsquo;s see how.\n  Specify the hypothesis:\nNow instead of $\\mu {1}-\\mu {2},$ we work with the population proportions, $p{1}-p{2}:$\n$$\\begin{aligned} \\text{(a) }H_{0} \u0026amp;:\u0026amp;p_{1}-p_{2}\\leq 0\\text{ vs. }H_{1}:p_{1}-p_{2}\u0026gt;0\\text{ } \\text{(b) }H_{0} \u0026amp;:\u0026amp;p_{1}-p_{2}\\geq 0\\text{ vs. }H_{1}:p_{1}-p_{2}\u0026lt;0 \\text{(c) }H_{0} \u0026amp;:\u0026amp;p_{1}-p_{2}=0\\text{ vs. }H_{1}:p_{1}-p_{2}\\neq 0\\end{aligned}$$\nNote that now we explicitly say that $d=0$. It doesn\u0026rsquo;t have to be, but if we consider nonzero differences, we must change the test statistic. The additional burden of describing two cases ($d=0$ versus $d$ being nonzero) is not warranted, since testing the zero difference is far more common.\n  Calculate the test statistic\nThe test statistic will always be a $z$ statistic since we assume $n$ is large:\n$$z_{0}=\\frac{\\widehat{p_{1}}-\\widehat{p_{2}}}{\\sqrt{\\widehat{p}% _{p}(1-p_{p})\\left( \\frac{1}{n_{1}}+\\frac{1}{n_{2}}\\right) }}$$\nwhere $\\widehat{p}{p}$ is the \u0026quot;pooled\u0026quot; proportion of successes, i.e., $% \\widehat{p}{p}=\\frac{\\sum_{j}x_{1j}+\\sum_{j}x_{2j}}{n_{1}+n_{2}}=\\frac{% \\text{Total Number of \u0026ldquo;Successes\u0026rdquo;}}{\\text{Total Sample Size}}$\n  Compute the p-value or rejection region\n$$\\begin{aligned} \\text{(a) }pval \u0026amp;=\u0026amp;P(Z\u0026gt;z_{0}) \\text{(b) }pval \u0026amp;=\u0026amp;P(Z\u0026lt;z_{0}) \\text{(c) }pval \u0026amp;=\u0026amp;2\\min [P(Z\u0026gt;z_{0}),P(Z\u0026lt;z_{0})]\\end{aligned}$$\nThe the rejection region (i.e., the \u0026quot;critical values\u0026quot;) for a test with the $% \\alpha$ level of significance would be computed as follows:\n$$\\begin{aligned} \\text{(a)}{\\text{ }z \u0026amp;:\u0026amp;z\u0026gt;\\text{ }z_{\\alpha }} \\text{(b) }{z \u0026amp;:\u0026amp;z\u0026lt;-z_{\\alpha }} \\text{(c) }{z \u0026amp;:\u0026amp;|z|\u0026gt;\\text{ }z_{\\alpha /2}}\\end{aligned}$$\n  Make a conclusion\n  If the $p$-value $\\leq \\alpha ,$ we reject $H_{0}$. If the $p$-value $% \u0026gt;\\alpha ,$ we fail to reject $H_{0}$. Equivalently, if $z_{0}$ lies within the rejection region, we reject $H_{0}$. If $z_{0}$ lies outside the rejection region, we fail to reject $H_{0}$.\nLet\u0026rsquo;s go back to the issue of marijuana legalization. Test the hypothesis that the proportion of people in the $18-30$ group who favor legalization is different from the proportion favoring legalization in the $31$ and over group. Use $\\alpha =0.10$.\n   Response (2012; 18-30) Count (18-30) Count (31 \u0026amp; over) Total     LEGAL $131$ $455$ $586$   NOT LEGAL $104$ $541$ $645$   TOTAL $235$ $996$ $1231$    For the drug study example, test the hypothesis that the proportion getting relief from Drug X is greater than that getting relief from the placebo at the $\\alpha =0.01$ level.\nFor the behavior study, test the hypothesis of a difference in the proportion of men and women who have had sex with an acquaintance last year at the $\\alpha =0.05$ level.\n","date":1595949567,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595949567,"objectID":"81ed85d83ca918b10fd194572d873630","permalink":"/courses/bana3363/9-1-test-for-diff-in-means-proportions/","publishdate":"2020-07-28T15:19:27.628999Z","relpermalink":"/courses/bana3363/9-1-test-for-diff-in-means-proportions/","section":"courses","summary":"A Hypothesis Test for Two Population/Process Means (Independent Samples) Here is the procedure for testing the difference between two population/process means from independent samples. You can see that it follows the general pattern that we have seen.","tags":null,"title":"Test for Diff in Means, Proportions","type":"docs"},{"authors":null,"categories":null,"content":"Confidence Interval for Difference in Proportion We might be interested in the difference in the proportions of two groups who meet a certain criterion. For example, we might want to determine what type of website banner ad (one that flashes or one that is steady) leads to a higher proportion of people clicking on it. We might want to determine which of two packaging designs leads to a higher proportion of people who indicate they would buy the product. An excellent example of inference for proportions comes from prescription drug testing. Researchers might be interested to see if a new cancer drug has a reduced chance of causing nausea in patients compared to an existing drug. Opinion polls on issues such as marijuana legalization can be analyzed using techniques for differences in proportions.\nThe general notation for inference about two population/process proportions is similar to that of two population/process means. The big difference is that the data for both groups can only take on the values of $0$ or $1,$ with $1$ indicating a \u0026quot;success\u0026quot; (either a click on an ad, an indication to purchase, or a person indicating that they did not experience nausea, for example).\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; Group 1 Group 2 Sample of size $n_{1}:$ $X_{11},X_{12},\\ldots ,X_{1n_{1}}$ , all either $0$ or $1$ Sample of size $n_{2}:$ $X_{21},X_{22},\\ldots ,X_{2n_{2}},$ all either $0$ or $1$ Mean: $p_{1}$ Mean: $p_{2}$ Sample Proportion: $\\widehat{p_{1}}=\\frac{\\sum_{j}x_{1j}}{n_{1}}$ (i.e., number of successes in Group 1) Sample proportion: $\\widehat{p_{2}}=\\frac{% \\sum_{j}x_{2j}}{n_{2}}$ (i.e., number of successes in Group 2) Estimated Standard Deviation: $\\sqrt{\\frac{\\widehat{p_{1}}(1-\\widehat{p_{1}})% Estimated Standard Deviation:$\\sqrt{\\frac{\\widehat{p_{2}}(1-% }{n_{1}}}$ \\widehat{p_{2}})}{n_{2}}}$ \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;\nWe can now introduce the confidence interval for the difference in two proportions.\nA $100(1-\\alpha )%$ confidence interval for the difference in two population proportions $p_{1}-p_{2}$, if the two sample sizes $n_{1}$ and $% n_{2}$ are sufficiently large, is given by\n$$\\widehat{p_{1}}-\\widehat{p_{2}}\\pm z_{\\alpha /2}\\sqrt{\\frac{\\widehat{p_{1}}% (1-\\widehat{p_{1}})}{n_{1}}+\\frac{\\widehat{p_{2}}(1-\\widehat{p_{2}})}{n_{2}}}$$\nThe interval for the difference in proportions is interpreted in a similar manner as with the interval for the difference in two means. Because we are comparing two population proportions, we must phrase our interpretation in terms of being confident that the difference in proportions lies between the lower and upper bounds. Note that the interval will always fall between $-1$ and $1.$ Negative values will arise when the proportion in Group $2$ is higher than the proportion in Group $1.$ In a single sample interval for a proportion, we cannot have negative values, but when we discuss differences in proportions, we certainly can.\nAnother issue regarding interpretation should be mentioned here. Because proportions can be interpreted as percentages, it would be tempting to say, if your interval were $[0.03,0.05]$ for example, that the difference between Group 1 and Group 2 on an issue is between $3%$ and $5%$. This is wrong because the difference in two percents is NOT the percentage difference. For example, if the interval above represented the difference in the percentage of people in two voting populations who favored a new tax initiative, we cannot say that between $3%$ and $5%$ more of Group $% 1$ favors the initiative than Group $2$. If, in a population of $12,000,$ $% 1,440$ $(12%)$ of people in Group 2 really do favor the initiative, then saying $3%$ more people in Group 1 favor the initiative implies that $1,483$ people in that group should be in favor. But if the population size of Group 1 is, let\u0026rsquo;s say, $34,000$ (populations can be different sizes) and $5,100$ $% \\left( \\frac{5,100}{34,000}=15%\\right)$ were in favor, a $3%$ increase would suggest that $5,253$ people in Group 1 would be in favor, not $1,483.$ Thus, we must always speak of percentage-point differences when we interpret these intervals.\nThe number of individuals for and against legalization of marijuana in the $% 18-30$ and $31$ and over age groups are given in the table below. Find a $% 95%$ confidence interval for the true difference in the proportion of people favoring marijuana legalization between the two groups.\nGroup Legal Not Legal Total \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; 18-30 (Group 1) $131$ $104$ $235$ 31 \u0026amp; over (Group 2) $455$ $541$ $996$ TOTAL $586$ $645$ $1231$\nA website designer wants to examine the effectiveness of two different layouts (A and B) for a small retail client in terms of the proportion of individuals who find and click on a special coupon within the first five minutes of browsing the retailer\u0026rsquo;s site. Presumably, the more people who find and click the coupon, the more effective the layout is. For two weeks, whenever a customer accessed the retailer\u0026rsquo;s site, one of the two layouts (A or B) was randomly chosen to load on the customer\u0026rsquo;s screen. Customers were tracked by their IP addresses, numbers that uniquely identify devices over the internet. Of the customers who browsed the site for at least five minutes, the proportion who clicked the coupon using each layout was recorded. The data are below. Construct a $95%$ confidence interval for the difference in the two proportions and interpret the interval in context.\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; Layout Found Coupon Did Not Find Coupon Total A $675$ $658$ $1333$ B $690$ $477$ $1167$ Total $1365$ $1135$ $2500$ \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;\nIn a recent study, a drug manufacturer wanted to determine if a new antidepressant (Drug X) could also reduce pain that often comes with diabetes, called diabetic neuropathy. The data for the study are shown below. Calculate a $92%$ confidence interval for the difference in the proportion of people who experience relief from Drug X and the placebo.\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; Dosage Some/Significant Relief No Relief Total Drug X $159$ $122$ $281$ Placebo $114$ $197$ $311$ $273$ $319$ $592$ \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;\n\\vspace{1.7in} A researcher is interested to see if there is a difference between married and unmarried people when it comes to certain social behavior, in particular, the frequency of sexual activity. The data from the study are shown below. Calculate a $90%$ confidence interval for the difference in the proportion of married and unmarried persons who have sex at least once per week.\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; Group $\\geq$Once/Week $\\mathbf{\u0026lt;}$ Once/Week Total Married $608$ $28$ $636$ Unmarried $553$ $7$ $560$ Total $1161$ $35$ $1196$ \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;\nAstrology is a system of belief that, broadly speaking, claims that astronomical events somehow influence human behavior. One aspect of astrology is belief that the zodiac sign that one is born under (e.g., Pisces, Libra, Leo, Scorpio, etc.) somehow influences a person\u0026rsquo;s behavior. Is belief in astrology related significantly to education level? The $2012$ General Social Survey asked adults $18$ and over whether they believed at all in the zodiac. The results are presented below for individuals with an associate\u0026rsquo;s degree or higher (\u0026quot;College\u0026quot;) and those with a high school diploma or less (\u0026quot;HS or Below\u0026quot;). Calculate a $98%$ confidence interval for the difference in proportions between the two groups and interpret the interval in context.\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; Dosage Believe Do Not Believe Total College $162$ $198$ $360$ HS or Below $330$ $306$ $636$ $492$ $504$ $996$ \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;\n","date":1595949567,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595949567,"objectID":"b7d10bc2018b27beb80d824119147822","permalink":"/courses/bana3363/10-1-test-for-differences-in-proportions-independent-samples/","publishdate":"2020-07-28T15:19:27.628999Z","relpermalink":"/courses/bana3363/10-1-test-for-differences-in-proportions-independent-samples/","section":"courses","summary":"Confidence Interval for Difference in Proportion We might be interested in the difference in the proportions of two groups who meet a certain criterion. For example, we might want to determine what type of website banner ad (one that flashes or one that is steady) leads to a higher proportion of people clicking on it.","tags":null,"title":"Test for Differences in Proportions--Independent Samples","type":"docs"},{"authors":null,"categories":null,"content":"Hypothesis Tests for Difference in Means (Independent Samples) Just as in the single sample case, we might be interested in testing whether the difference in two population means is equal to a specified value. Usually\u0026ndash;but not always\u0026ndash;we specify that the difference is $0$ in the null hypothesis in which case the test becomes one of equal population means. Doing so makes the confidence interval for the difference in means correspond to the test: if $0$ is not in the $(1-\\alpha )100%$ interval, you conclude that the two means differ. If you were to conduct the hypothesis test at level $\\alpha ,$ you would reject $H_{0}.$\nHere is the general procedure for testing the difference between two population/process means from independent samples. You can see that it follows the general pattern that we have seen before.\n  Specify the hypotheses.\n$$\\begin{aligned} \\text{(a) }H_{0} \u0026amp;:\u0026amp;\\mu _{1}-\\mu _{2}\\leq d\\text{ vs. }H_{1}:\\mu _{1}-\\mu _{2}\u0026gt;d\\text{ } \\ \\text{(b) }H_{0} \u0026amp;:\u0026amp;\\mu _{1}-\\mu _{2}\\geq d\\text{ vs. }H_{1}:\\mu _{1}-\\mu _{2}\u0026lt;d \\ \\text{(c) }H_{0} \u0026amp;:\u0026amp;\\mu _{1}-\\mu _{2}=d\\text{ vs. }H_{1}:\\mu _{1}-\\mu _{2}\\neq d\\end{aligned}$$\nwhere $-\\infty \u0026lt;d$ $\u0026lt;\\infty$ is some specified difference (usually $0$, but it could be anything$).$\n  Calculate the test statistic.\nCase 1: $\\sigma _{1}=\\sigma _{2}$\n$$t=\\frac{\\overline{x}{1}-\\overline{x}{2}-d}{s_{p}\\sqrt{\\frac{1}{n_{1}}+% \\frac{1}{n_{2}}$$\nwhere $s_{p}=\\sqrt{\\frac{(n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2% }}$ is the pooled standard deviation estimate.\nCase 2: $\\sigma _{1}\\neq \\sigma _{2}$\n$$t=\\frac{\\overline{x}{1}-\\overline{x}{2}-d}{\\sqrt{\\frac{s_{1}^{2}}{n_{1}}+% \\frac{s_{2}^{2}}{n_{2}}}}$$\n  Determine the rejection region (or calculate the $p$-value).\n  The rejection region comes from a Student\u0026rsquo;s $t$ distribution with the appropriate degrees of freedom. In Case $2$ (unequal population variances) the degrees of freedom $\\nu =\\frac{\\left( \\frac{s_{1}^{2}}{n_{1}}+\\frac{% s_{2}^{2}}{n_{2}}\\right) ^{2}}{\\frac{\\left( \\frac{s_{1}^{2}}{n_{1}}\\right) ^{2}}{n_{1}-1}+\\frac{\\left( \\frac{s_{2}^{2}}{n_{2}}\\right) ^{2}}{n_{2}-1}},$ which would always be calculated by software in practice. But for large samples, $\\nu$ will be large as well, so we can just using the $\u0026rdquo;\\infty \u0026ldquo;$ row of the table in this case. For Case $1$ (equal population variances)$,$ the rejection region is given by the following:\n$$\\begin{aligned} \\text{(a)}{\\text{ }t_{n_{1}+n_{2}-2} \u0026amp;:\u0026amp;t_{n_{1}+n_{2}-2}\u0026gt;\\text{ }t_{\\alpha ,t_{n_{1}+n_{2}-2}}} \\ \\text{(b) }{t_{n_{1}+n_{2}-2} \u0026amp;:\u0026amp;t_{n_{1}+n_{2}-2}\u0026lt;-t_{\\alpha ,t_{n_{1}+n_{2}-2}}} \\ \\text{(c) }{t_{n_{1}+n_{2}-2} \u0026amp;:\u0026amp;|t_{n_{1}+n_{2}-2}|\u0026gt;\\text{ }t_{\\alpha /2,t_{n_{1}+n_{2}-2}}}\\end{aligned}$$\nIf software is available, the $p$-value calculations are as follows:\nCase 1: $\\sigma _{1}=\\sigma _{2}$\n $$\\begin{aligned} \\text{(a) }p \u0026amp;=\u0026amp;P(t_{n_{1}+n_{2}-2}\u0026gt;t) \\ \\text{(b) }p \u0026amp;=\u0026amp;P(t_{n_{1}+n_{2}-2}\u0026lt;t) \\ \\text{(c) }p \u0026amp;=\u0026amp;2\\min [P(t_{n_{1}+n_{2}-2}\u0026gt;t),P(t_{n_{1}+n_{2}-2}\u0026lt;t)]\\end{aligned}$$  Case 2: $\\sigma _{1}\\neq \\sigma _{2}$\n $$\\begin{aligned} \\text{(a) }p \u0026amp;=\u0026amp;P(t_{\\nu }\u0026gt;t) \\ \\text{(b) }p \u0026amp;=\u0026amp;P(t_{\\nu }\u0026lt;t) \\ \\text{(c) }p \u0026amp;=\u0026amp;2\\min [P(t_{\\nu }\u0026gt;t),P(t_{\\nu }\u0026lt;t)]\\end{aligned}$$  where $\\nu =\\frac{\\left( \\frac{s_{1}^{2}}{n_{1}}+\\frac{s_{2}^{2}}{n_{2}}% \\right) ^{2}}{\\frac{\\left( \\frac{s_{1}^{2}}{n_{1}}\\right) ^{2}}{n_{1}-1}+% \\frac{\\left( \\frac{s_{2}^{2}}{n_{2}}\\right) ^{2}}{n_{2}-1}}.$\n Make a conclusion  If the $p$-value $\\leq \\alpha ,$ we reject $H_{0}.$ If the $p$-value $% \u0026gt;\\alpha ,$ we fail to reject $H_{0}.$ Equivalently, if $t_{0}$ lies within the rejection region, we reject $H_{0}$. If $t_{0}$ lies outside the rejection region, we fail to reject $H_{0}.$\nHere are some examples:\nIn a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8,$ respectively. Determine at the $% \\alpha =0.10$ level if there is a difference in the population mean amount of time spent studying per week between accounting and general business majors. Assume the population standard deviations are equal.\n\\vspace{2in} In a recent survey of $121$ undergraduates, students reported the number of hours per week they spend on school work. Figure [table1]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;table1\u0026rdquo;} below summarizes the data for finance and marketing majors. A researcher wants to see if marketing majors and finance majors differ in the amount of time they study per week on average. Test the hypothesis at the $5%$ level of significance, assuming the two population standard deviations are not equal, using the fact that $\\nu =26.$\nMajor n Average Standard Deviation\n Finance (Group 1) 15 9.8 4.9\nMarketing (Group 2) 16 11.9 7.4\nThe most recent General Social Survey asked American adults over $18$ years of age how many total weeks they worked in the previous year. The data is summarized in the table shown below, which was created using Excel\u0026rsquo;s pivot table feature1. Test the hypothesis that those with a junior college education¬†(Group 1) work more weeks per year on average than those with a high school diploma (Group 2) at the $0.01$ level of significance. Assume that the population standard deviations are equal.\n\\FRAME{ftbphFU}{5.5054in}{1.5039in}{0pt}{\\Qcb{Average number of weeks spent in part-time and full-time employement in the previous year by highest degree earned.}}{\\Qlb{weeks}}{Table}{\\special{language \u0026ldquo;Scientific Word\u0026rdquo;;type \u0026ldquo;GRAPHIC\u0026rdquo;;maintain-aspect-ratio TRUE;display \u0026ldquo;USEDEF\u0026rdquo;;valid_file \u0026ldquo;T\u0026rdquo;;width 5.5054in;height 1.5039in;depth 0pt;original-width 5.4483in;original-height 1.4684in;cropleft \u0026ldquo;0\u0026rdquo;;croptop \u0026ldquo;1\u0026rdquo;;cropright \u0026ldquo;1\u0026rdquo;;cropbottom \u0026ldquo;0\u0026rdquo;;tempfilename \u0026lsquo;N7C7J106.wmf\u0026rsquo;;tempfile-properties \u0026ldquo;XPR\u0026rdquo;;}}\n  See http://office.microsoft.com/en-us/excel-help/pivottable-reports-101-HA001034632.aspx for more information about pivot tables. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1595949567,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595949567,"objectID":"7299860199fc5fffb57eec01b1578d8c","permalink":"/courses/bana3363/10-2-test-for-diff-in-means-independent-samples/","publishdate":"2020-07-28T15:19:27.628999Z","relpermalink":"/courses/bana3363/10-2-test-for-diff-in-means-independent-samples/","section":"courses","summary":"Hypothesis Tests for Difference in Means (Independent Samples) Just as in the single sample case, we might be interested in testing whether the difference in two population means is equal to a specified value.","tags":null,"title":"Test for Diff in Means Independent Samples","type":"docs"},{"authors":null,"categories":null,"content":"Motivation for Examining Variances The notion of quality means different things to different people. Some popular definitions include: fitness for an intended purpose; conformance to specifications; value as perceived by the customer; meeting design specifications; meeting customer expectations; and many more. Quality is of particular interest to managers because, after all, everyone wants to make a \u0026quot;quality product.\u0026quot; There is even an organization called the American Society for Quality devoted to the lofty goal of transforming the world by promoting the notion of quality assurance as a key component of any organization\u0026rsquo;s strategic plan. This group offers certification and training in several areas, including IS09000 , and conducts research on emerging issues that affect quality. It is a good bet that during your career you will hear about concepts such as Six Sigma, total quality management, quality at the source, process capability, kaizen, poka-yoke, jidoka, lean manufacturing, control charts, and people such as W. Edwards Deming, Walter Shewhart, and Joseph Juran. In an era where traditional barriers to competitor entry¬†(such as access to customer base and capital) have been weakened, the notion of quality has become a key competitive tool for businesses around the world.\nYou will notice that one of the definitions of quality is \u0026quot;conformance to specifications.\u0026quot; We can map this idea directly to the notion of variability. When things \u0026quot;conform,\u0026quot; by definition, they exhibit little variability. Thus, one might consider a service or product as being of poor quality if the performance of the product or service varies highly from one experience to the next. Conversely, a high-quality product or service might be defined as one that offers consistent performance from one experience to the next. Of course, consistency is not enough; after all, for example, a car that consistently overheats cannot be said to be high quality. Thus, it is important to set the standards of performance properly before the product or service is offered to the marketplace. Proper standard setting begins and ends with the customer in mind, and is often accomplished through observational studies, surveys, and focus groups. Product development is a key component of marketing science.\nAssuming that we do have some idea of a standard, how can we use statistics to help us see if our product is of high quality? If we are willing to make some assumptions about the populations, we can derive a confidence interval to determine whether the variability of two populations is the same. The two populations could be defined by the time of collection (say, yesterday\u0026rsquo;s production run versus today\u0026rsquo;s) or by process changes¬†(comparing an old production method versus a new one). The key assumptions will be that we have two independent, normally distributed populations from which we are taking samples of size $n_{1}$ and $n_{2}.$ 1 We then construct a confidence interval, not for the difference but for the *ratio of the two variances.* Using the general relationship between confidence intervals and hypothesis tests, we can use the confidence interval to perform the test.\nThe Confidence Interval for Differences in Two Variances\u0026ndash;Normal Populations The distribution used to form the confidence interval is the $F$ distribution. This distribution arises when we calculate the variances from samples from two normally distributed populations and make a quotient out of them in a certain way. The following theorem spells out the details.\nIf $s_{1}^{2}$ and $s_{2}^{2}$ are two sample variances calculated by taking samples of size $n_{1}$ and $n_{2}$ from two independent normal populations with variances $\\sigma _{1}^{2}$ and $\\sigma _{2}^{2},$ then the quantity $$F_{\\nu _{1},\\nu _{2}}=\\frac{s_{1}^{2}/\\sigma _{1}^{2}}{s_{2}^{2}/\\sigma _{2}^{2}}$$\nhas an $F$ distribution with $\\nu {1}=n{1}-1$ numerator degrees of freedom and and $\\nu {2}=n{2}-1$ denominator degrees of freedom. The $F$ distribution takes on a variety of appearances depending on these two parameters, as you can see in Figure [fdist]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;fdist\u0026rdquo;}.\n\\FRAME{ftbphFU}{3.5552in}{2.6749in}{0pt}{\\Qcb{The $F_{v_{1},\\protect\\nu _{2}} $ distribution for various values of $v_{1}$ and $\\protect\\nu _{2}.$}}{}{% Figure}{\\special{language \u0026ldquo;Scientific Word\u0026rdquo;;type \u0026ldquo;GRAPHIC\u0026rdquo;;maintain-aspect-ratio TRUE;display \u0026ldquo;USEDEF\u0026rdquo;;valid_file \u0026ldquo;T\u0026rdquo;;width 3.5552in;height 2.6749in;depth 0pt;original-width 4.5143in;original-height 3.3892in;cropleft \u0026ldquo;0\u0026rdquo;;croptop \u0026ldquo;1\u0026rdquo;;cropright \u0026ldquo;1\u0026rdquo;;cropbottom \u0026ldquo;0\u0026rdquo;;tempfilename \u0026lsquo;N7I88L01.bmp\u0026rsquo;;tempfile-properties \u0026ldquo;XPR\u0026rdquo;;}}\nNotice that the $F$ distribution, unlike the Student\u0026rsquo;s $t$ and the standard normal, is not, in general, symmetric. In fact, it is right-skewed, meaning it has a \u0026quot;tail\u0026quot;¬†to its right. Therefore, we must find two values, $% F_{\\alpha /2,\\nu _{1},\\nu _{2}}$ and $F_{1-\\alpha /2,v_{1},\\nu _{2}}$ to specify the confidence interval. If software were not available, we would use an $F$ table, an excerpt of which is shown in Figure [ftable]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;ftable\u0026rdquo;}. Then, using the theorem above, we can say that\n$$P(F_{1-\\alpha /2,v_{1},\\nu _{2}}\\leq \\frac{s_{1}^{2}/\\sigma _{1}^{2}}{% s_{2}^{2}/\\sigma _{2}^{2}}\\leq F_{\\alpha /2,\\nu _{1},\\nu _{2}})=1-\\alpha$$\nBy manipulating this equation to get $\\frac{\\sigma _{1}^{2}}{% \\sigma _{2}^{2}}$ in the middle (details omitted) we get the confidence interval.\nA $(1-\\alpha )100%$ confidence interval for $\\frac{\\sigma {1}^{2}}{\\sigma {2}^{2}}$ is given by $\\left[ \\frac{s{1}^{2}}{s{2}^{2}}\\frac{1}{F_{\\alpha /2,\\nu _{1},\\nu _{2}}},\\frac{s_{1}^{2}}{s_{2}^{2}}F_{\\alpha /2,\\nu _{2},\\nu _{1}}\\right]$\nNotice that on the upper bound the degrees of freedom are switched, with $% \\nu _{2}$ becoming the numerator degrees of freedom and $\\nu _{1}$ now the denominator degrees of freedom. This interval can be used to test the two-sided hypothesis\n$$\\text{ }H_{0}:\\frac{\\sigma _{1}^{2}}{\\sigma _{2}^{2}}=1\\text{ vs. }H_{1}:% \\frac{\\sigma _{1}^{2}}{\\sigma _{2}^{2}}\\neq 1\\text{ }$$\nTo use the confidence interval, we simply look to see if it contains $1.$ The logic is that if the two population variances are equal, their ratio will be exactly $1$ (any number divided by itself is $1).$ Therefore, if $1$ is in the interval, it is plausible that the two variances are equal.\n\\FRAME{ftbphFU}{4.8317in}{2.6299in}{0pt}{\\Qcb{Table of $F$ distribution critical values. The value $F_{0.05,3,2}$, for example, would be $19.16$}}{% \\Qlb{ftable}}{Figure}{\\special{language \u0026ldquo;Scientific Word\u0026rdquo;;type \u0026ldquo;GRAPHIC\u0026rdquo;;maintain-aspect-ratio TRUE;display \u0026ldquo;USEDEF\u0026rdquo;;valid_file \u0026ldquo;T\u0026rdquo;;width 4.8317in;height 2.6299in;depth 0pt;original-width 10.6528in;original-height 5.7726in;cropleft \u0026ldquo;0\u0026rdquo;;croptop \u0026ldquo;1\u0026rdquo;;cropright \u0026ldquo;1\u0026rdquo;;cropbottom \u0026ldquo;0\u0026rdquo;;tempfilename \u0026lsquo;N33PC601.wmf\u0026rsquo;;tempfile-properties \u0026ldquo;XPR\u0026rdquo;;}}\nLet us work some examples.\nA golf club manufacturer is designing a new driver and wants to market it as giving more consistent results than the competitor\u0026rsquo;s club. To test this, the manufacturer used a specially designed machine to precisely hit $51$ golf balls with the new club $($Group $1)$ and $25$ golf balls with the competitor\u0026rsquo;s club $($Group $2)$. The new club resulted in an average drive length of $150.1$ yards with a standard deviation of $3.1$ yards, while the competitor\u0026rsquo;s club gave an average drive length of $150.5$ yards with a standard deviation of $5.8$ yards. Can the manufacturer conclude at the $% \\alpha =0.05$ level that the two population variances are not equal?\nThe critical value of $F$ that we need is $F_{0.025,50,24}=2.11$, which gives $F_{0.025,24,50}=1.93$ for the upper bound (notice the $50$ and $24$ have switched places). So the interval is $\\left[ \\frac{3.1^{2}}{5.8^{2}}% \\frac{1}{2.11},\\frac{3.1^{2}}{5.8^{2}}1.93\\right] =[0.14,0.55].$ This interval does not include $1,$ so we would conclude that the two population variances are different, with evidence that the variance of the old club is larger, making it less consistent.\n\\bigskip A fertilizer plant produces $60$-pound bags to sell at a nationwide home-improvement store. To minimize costs, it buys a filling machine that has a tolerance of $+/-0.03$pounds. One day, the store weighs a sample of $13$ bags using a precision scale and finds that the average weight of the bags is $60.001$ pounds, with a standard deviation of $0.045$ pounds. The next day, they weigh another sample of $13$ bags and find that the average weight is $60.002$ pounds, with a standard deviation of $0.058$ pounds. Can the manufacturer conclude that the process variability differs between the two days at the $0.10$ level?\n\\bigskip In a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8,$ respectively. Determine at the $% \\alpha =0.10$ level if there is a difference in the variability in study time between accounting and general business majors.\n  Other tests are more sensitive to differences in the population variances when the assumption of normality in the populations is violated. The violation only has to be slight for the test to lose sensitivity, or \u0026quot;power,\u0026quot; to detect differences in variances. For this reason, this test is considered \u0026quot;not robust\u0026quot; to violations of normality. The Levene and Brown-Forsythe tests work better in this case. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1595949567,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595949567,"objectID":"6e0057ef3203d58582de6e792befe58b","permalink":"/courses/bana3363/10-3-test-for-diff-in-variances/","publishdate":"2020-07-28T15:19:27.628999Z","relpermalink":"/courses/bana3363/10-3-test-for-diff-in-variances/","section":"courses","summary":"Motivation for Examining Variances The notion of quality means different things to different people. Some popular definitions include: fitness for an intended purpose; conformance to specifications; value as perceived by the customer; meeting design specifications; meeting customer expectations; and many more.","tags":null,"title":"Confidence Interval for the Difference in Variances","type":"docs"},{"authors":null,"categories":null,"content":"ANOVA Basics In the last couple of chapters, we have discussed how to make inferences about the differences in two populations in terms of means and proportions. What if we wanted to compare more than two populations simultaneously? For example, what if we wanted to compare the abilities of four different real estate appraisal firms in terms of how accurate they are in predicting the sale price of a home? For convenience, we could label the firms with numbers (Firm $1$, Firm $2$, Firm $3$, Firm $4$). To compare the firms\u0026rsquo; accuracy, you might randomly assign each firm $10$ houses to appraise and then calculate an accuracy measure, for example,\n$$Y_{ij}=\\frac{|\\text{Appraised Price for home }j\\text{ for Firm }i\\text{ }-% \\text{ Sale Price for home }j\\text{ for Firm }i|}{\\text{Sale Price for home }% j\\text{ for Firm }i}\\times 100$$\nThe question would then be whether the mean accuracy differs among the four firms. Notice that double subscripts are now required to completely identify a single observation. We must specify the group number $(i=1,2,3,4)$ and the observation number within the group $(j=1,2,\\ldots ,10)$. For example, $% y_{34}=2.3$ would be the fourth observation for Firm $3$. The observed data, in general, would appear as in Table [anova_general]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;anova_general\u0026rdquo;} below.\n   Firm 1 Firm 2 Firm 3 Firm 4     $y_{11}$ $y_{21}$ $y_{31}$ $y_{41}$   $y_{12}$ $y_{22}$ $y_{32}$ $y_{42}$   $\\vdots$ $\\vdots$ $\\vdots$ $\\vdots$   $y_{1,10}$ $y_{2,10}$ $y_{3,10}$ $y_{4,10}$    General appearance of an ANOVA data table for the real estate appraisal example.\n[[anova_general]]{#anova_general label=\u0026quot;anova_general\u0026rdquo;}\nOne way to do the comparisons would be to evaluate each pair of group means using the two-sample $t$ test or confidence interval that we have already studied. Thus, you would compare Firm 1 to Firm 2, Firm 1 to Firm 3, Firm 1 to Firm 4, Firm 2 to Firm 3, Firm 2 to Firm 4, and finally Firm 3 to Firm 4. The total number of comparisons would be $\\binom{4}{2}=\\frac{4!}{(4-2)!2!}=6$, using the combination formula from the binomial distribution (see Handout 1). In general, the number of pairs of means you would have to test if you were dealing with $g$ groups would be $\\binom{g}{2}=\\frac{g(g-1)}{2}$.\nThe problem is that simply conducting multiple two-sample $t$ tests is highly error prone if you look at the collection of tests as a whole. If you think about it, it makes some sense: each test has probability $\\alpha$ of giving a Type I¬†error, and if you have a collection of tests, each with a probability of error, the chance of making at least one error increases dramatically. Since making errors is generally not what we want to do when we attempt statistical inference, we need another approach. Enter ANOVA.\nANOVA(ANalysis Of Variance) is a broad category of statistical techniques used to determine if there is a difference between two or more population/process means\nIn its simplest form, it is an extension of the two-sample $t$ test to more than two samples. You could certainly use ANOVA for the case of two samples, but it would be an unnecessary complication. Before we move on, we need to get some vocabulary out of the way.\nA factor is a qualitative criterion by which we may segment the population. The populations can be segmented by several criteria.\nIn the appraiser example, there is only one factor, firm. We could add another factor such as \u0026ldquo;firm experience.\u0026rdquo;\nEach factor has several levels (or treatments), which are possible values of the factor. We assume each combination of factor levels defines a population or process.\nThere are four levels in the appraiser example $($Firm $1$,Firm $2$,Firm $3$, and Firm $4)$. If we add \u0026ldquo;firm experience,\u0026rdquo; the levels of that factor could be \u0026ldquo;little,\u0026rdquo; \u0026ldquo;moderate,\u0026rdquo; and \u0026ldquo;significant.\u0026rdquo;\nThe response variable is the numeric variable that we examine within each factor level combination.\nIn the appraiser example, the response variable is the appraisal accuracy.\nThe experimental unit is the entity that each value of the response variable represents.\nIn the appraiser example, the experimental unit is an individual house.\nThe design that examines only one factor with $g$ $(\u0026gt;2)$ levels is known as the one-way ANOVA.\nWe also need to get a bit of notation out of the way. Fortunately, it is a natural extension of the two-sample case to the case of $g$ populations. See Figure [anova_notation]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;anova_notation\u0026rdquo;}.\nAn additional important assumption that we will make is that the variance is the same for each of the $g$¬†groups, equal to $% \\sigma ^{2}$. That is\n$$\\sigma_{1}^{2}=\\sigma_{2}^{2}=\\ldots =\\sigma_{g}^{2}=\\sigma ^{2}$$\nIn the one-way ANOVA, we divide the total variability in the data into two parts: that which is explained by the differences between group means and the overall mean, and that which is not. If a lot of the variability is explained by the difference between the group means and the overall mean, we have evidence that there is a difference (somewhere) in the population means. Figure [anova_concept]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;anova_concept\u0026rdquo;} gives a visual representation of the idea. Here, we see that most of the total variability is accounted for by the differences between the group means and the overall mean (the shaded area). Therefore, we would say that there is a difference (somewhere) among the group means.\nThe mathematical representation of Figure [anova_concept]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;anova_concept\u0026rdquo;} is in terms of \u0026ldquo;sums of squares.\u0026rdquo; The equation is\n$$SSTOT=SST+SSE$$\nwhere $SSTOT$ is the total sum of squares (a measure of total variability in the data), $SST$ is called the sum of squares for treatments (a measure of variability between group means and overall mean), and $SSE$ is called the sum of squares for error, a measure of variability that is left unexplained.\nTo construct the eventual test statistic, we need formulas for finding $SST$ and $SSE$. First, we need the grand mean, the overall average of the data. The formula and notation for that value is the following:\n$$\\overline{\\overline{y}}=\\frac{1}{N}\\sum_{i=1}^{g}\\sum_{j=1}^{n_{i}}y_{ij} \\label{full_data_grand_mean}$$\nwhere $N$ is the total number of data points in the sample. If only the individual group means $\\overline{y}{1},\\overline{y}{2},\\ldots ,% \\overline{y}_{g}$ are known, the grand mean can be found by taking a weighted (by the individual sample sizes) average of the group means, that is,\n$$\\overline{\\overline{y}}=\\frac{n_{1}\\overline{y}_{1}+n_{2}\\overline{y}%_{2}+\\ldots +n_{g}\\overline{y}_{g}}{n_{1}+n_{2}+\\ldots +n_{g}} \\label{grand_mean_group_means_only}$$\nNow we can define the other formulas, starting with $SST$:\n$$SST=\\sum_{i=1}^{g}n_{i}(\\overline{y_{i}}-\\overline{\\overline{y}})^{2} \\label{SST}$$\nIf all of the group sample sizes are the same, then $n_{i}=n$ for all $i$, so the above can be simplified a bit:\n$$SST=n\\sum_{i=1}^{g}(\\overline{y_{i}}-\\overline{\\overline{y}})^{2}\\text{ [if all group sample sizes are equal].} \\label{SST_equal_n}$$\nNext we have the formula for $SSE$:\n$$SSE=\\sum_{i=1}^{g}\\sum_{j=1}^{n_{i}}(y_{ij}-\\overline{y}_{i})^{2} \\label{SSE}$$\nwhich can be simplified if we note that for each $i$, the inner terms, $\\sum_{j=1}^{n_{i}}(y_{ij}-\\overline{y}_{i})^{2}$, are really just the numerators of the sample variances for each group, $s_{i}^{2}$. So we have $(n_{i}-1)s_{i}^{2}=\\sum_{j=1}^{n_{i}}(y_{ij}-\\overline{y}_{i})^{2}$, which means we can simplify the formula for $SSE$ to\n$$SSE=\\sum_{i=1}^{g}(n_{i}-1)s_{i}^{2}\\text{ \\ \\ \\ [simplified version of equation \\ref{SSE}]}$$\nFinally we, have $SSTOT$:\n$$SSTOT=\\sum_{i=1}^{g}\\sum_{j=1}^{n_{i}}(y_{ij}-\\overline{\\overline{y}})^{2} \\label{SSTot}$$\nwhich is just the total of $SST$ and $SSE$. The more of $SSTOT$ that is made up of $SST$, the more we begin to suspect that there are differences in group means.\nThe next concept we need is degrees of freedom, which has to do with the number of independent pieces of information you are combining. Think of the ordinary sample variance formula: $s^{2}=\\frac{% \\sum_{i=1}^{n}(y_{i}-\\overline{y})^{2}}{n-1}$. The reason we divide by $n-1$ is that if you know $n-1$ of the numbers and the sample mean, you can figure out the remaining number. For instance, if I tell you the sample mean is $% \\overline{y}=6$ for a sample of $4$ numbers, and three of them are $1,8$, and $3$, then the fourth one must be $24-1-8-3=12$¬†(verify this for yourself). In ANOVA, degrees of freedom can be broken up by sums of squares:\n$$\\begin{aligned} \\text{Total Degrees of Freedom}\\text{: } \u0026amp;\u0026amp;DF_{TOT}=N-1 \\ \\text{Degrees of Freedom for Treatment}\\text{: } \u0026amp;\u0026amp;DF_{SST}=g-1 \\ \\text{Degrees of Freedom for Error}\\text{: } \u0026amp;\u0026amp;DF_{SSE}=N-g\\end{aligned}$$\nso we have\n$$DF_{TOT}=DF_{SST}+DF_{SSE}$$\nWe will see how this all fits together when we work a few examples. First we have to define the ANOVA test. The same general four-step procedure still applies. What changes are the specifics.\nThe general four-step procedure still applies. What changes are the specifics.¬†Let\u0026rsquo;s see how.\n Specify the hypothesis:  Now instead of one $\\mu$ we work with several $\\mu ^{\\prime }s$, one for each of the $g$ groups. The ANOVA hypothesis, like the test for equal variances, is two sided by design:\n$$\\begin{aligned} H_{0} \u0026amp;:\u0026amp;\\mu_{1}=\\mu_{2}=\\ldots =\\mu_{g} \\ H_{1} \u0026amp;:\u0026amp;\\text{ not all }\\mu_{i}\\text{ are equal }\\end{aligned}$$\nNote that in the null hypothesis, you do not specify what all of the $\\mu ^{\\prime }s$ are equal to; you only state that they are all equal to one another. Note also that the complement of $H_{0}$¬†is not that all the means are different, but just that at least two are different* (or equivalently, not all are equal).\n Calculate the test statistic  Under the assumption that the data arise from $g$ normal distributions with possibly different means but identical standard deviations, the test statistic will be an¬†$F$¬†statistic,** which follows an $F$ distribution when the null hypothesis is true. Similar to the Student\u0026rsquo;s $t$ distribution, the $F$ distribution is defined by degrees of freedom. The difference is that we have **numerator degrees of freedom,** which we call **¬†**$\\nu_{1}$,**¬†**and **denominator degrees of freedom**, which we call.**¬†**The $% F_{v_{1},v_{2}}$ distribution, then is the $F$ distribution with $\\nu_{1}$ numerator degrees of freedom and $\\nu_{2}$ denominator degrees of freedom. The $F$ distribution takes on a variety of appearances depending on these two parameters, as you can see in Figure [fdist]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;fdist\u0026rdquo;}.\nTo calculate the test statistic, we compute the following:\n$$f=\\frac{SST/(g-1)}{SSE/(N-g)}=\\frac{MST}{MSE}$$\nwhere $MST$ stands for \u0026ldquo;mean square for treatments\u0026rdquo; and $MSE$ stands for \u0026ldquo;mean square for error.\u0026rdquo;\n Determine the rejection region (or compute the $p$-value)  Like any test for means, unless we have software, we usually go with the rejection region approach. The the rejection region for a test with the $% \\alpha$ level of significance would be computed as follows:\n$${f_{g-1},_{N-g}:f_{g-1},_{N-g}\u0026gt;f_{g-1,N-g,\\alpha }}$$\nIf we have access to software, we can compute the $p$-value as follows: $$p-val=P(F_{g-1,N-g}\u0026gt;f)$$\nNotice that there is only one rejection region for this test; it is a right-tailed test. We only reject for large values of $f$.\n Make a conclusion  If the $p$-value $\\leq \\alpha$, we reject $H_{0}$. If the $p$-value $% \u0026gt;\\alpha$, we fail to reject $H_{0}$. Equivalently, if $f$ lies within the rejection region, we reject $H_{0}$. If $f$ lies outside the rejection region, we fail to reject $H_{0}$.\nIn practice, the sums of squares, mean squares, $F$ statistic, $p$-value, and other information are often given in an ANOVA table similar to that in Figure [anovatable]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;anovatable\u0026rdquo;}, which is the form given in Microsoft Excel. \u0026ldquo;Between groups\u0026rdquo; refers to the explained variability in group means and \u0026ldquo;within groups\u0026rdquo; refers to the unexplained variability. We will use this table extensively in the examples.\nLet\u0026rsquo;s work an example putting all of this together.\nThe following statistics were calculated for an experiment:\na). Develop the ANOVA table and fill in the information below.\nb). State the ANOVA hypothesis\nc). State the conclusion.\nA study was done to determine if the amount of time people spend on the internet varies by martial status. The data, from the $2012$ General Social Survey, are summarized below.\na). Develop the ANOVA table and fill in the information below.\nb). State the ANOVA hypothesis\nc). State the conclusion in the context of the problem.\nThe ANOVA table from the example in the PowerPoint regarding the average time students in different majors spend studying per week is reproduced below.\na). How many groups were examined?\nb). Is this a balanced design?\nc). How many total observations were there?\nd). Show how the F statistic was calculated\nf). What important assumption seems to be violated here?\nBonferroni Method of Multiple Comparisons Recall that the ANOVA hypotheses are\n$$H_{0}: \\mu_{1}=\\mu_{2}=\\ldots =\\mu_{g}\\text{ vs. }H_{1}:\\text{not all }\\mu ^{\\prime }s\\text{ are equal}$$\nSuppose you have rejected the ANOVA hypothesis. You can then only conclude that not all of the group means are identical. However, you don\u0026rsquo;t know which ones differ from each other, which is a natural question to ask. As stated at the beginning of Handout 11, simply conducting several $t$ tests in the usual way will increase your chance of making at least one type I¬†error, so we have to adjust our approach slightly.\nMultiple comparison techniques allow one to make conclusions about a collection, or family, of hypotheses while keeping the overall error rate (familywise error rate) at a given (small) level, $\\alpha_{family}.$\nMany methods exist, and I wrote my disseration on one . We will only cover one here because it is the simplest. The basic idea of all methods is that we increase the burden of proof over a regular test or interval. We do this because each test has an $\\alpha$ probability of being incorrect.\nThe Bonferroni adjustment forms confidence intervals and conducts tests at the $\\frac{\\alpha_{family}}{k}$ level, where $k$ is the number of tests or intervals you want to construct and control the error rate for.\nTo compare all of the groups to one another, just set $k=\\binom{g}{2}=\\frac{% g(g-1)}{2}.$ For example, if you have $5$ groups and you wish to compare all group means, you would set $k=\\frac{5(5-1)}{2}=10.$ If $\\alpha_{family}=0.05$, then you would form intervals and conduct tests at the $% \\frac{0.05}{10}=0.005$ level. However, you just wanted to compare, say, $4$ pairs of means, you would set $k=4$ and you would form intervals and conduct tests at the $\\frac{0.05}{4}=0.0125$ level.\nConfidence intervals for differences between means $\\mu_{i}$ and $\\mu_{j}$ can be constructed by the following method\n$$\\overline{y_{i}}-\\overline{y_{j}}\\pm t_{\\frac{_{\\alpha_{family}}}{2k}% },_{N-g}\\times \\sqrt{MSE\\left( \\frac{1}{n_{i}}+\\frac{1}{n_{j}}\\right) }$$\nWhat this interval allows us to conclude is that, with $(1-\\alpha_{family})100%$ confidence **in the entire set of confidence intervals we calculate**, the difference in the two group means $\\mu_{1}-\\mu_{j}$ lies within the upper and lower bounds. So, if we want $95%$ confidence in the entire set of intervals, we would set $\\alpha_{family}=0.05.$ The $\u0026quot;2\u0026quot;$ in the denominator of the above equation comes from the fact that when we form a two-sided confidence interval, we divide the $\\alpha$ by 2 to obtain the correct value of $t.$\nWe can also define a hypothesis test using the Bonferroni method. What changes are the specifics.¬†Let\u0026rsquo;s see how.\n  Specify the hypothesis:\nIn general, we want to test the relationship between two means, $\\mu_{i}$ and $\\mu_{j}.$\n$$\\begin{aligned} \\text{(a) }H_{0} \u0026amp;:\u0026amp;\\mu_{i}-\\mu_{j}\\leq d\\text{ vs. }H_{1}:\\mu_{i}-\\mu _{j}\u0026gt;d\\text{ } \\ \\text{(b) }H_{0} \u0026amp;:\u0026amp;\\mu_{i}-\\mu_{j}\\geq d\\text{ vs. }H_{1}:\\mu_{i}-\\mu _{j}\u0026lt;d \\ \\text{(c) }H_{0} \u0026amp;:\u0026amp;\\mu_{i}-\\mu_{j}=d\\text{ vs. }H_{1}:\\mu_{i}-\\mu _{j}\\neq d\\end{aligned}$$\n  Calculate the test statistic\nThe test statistic will be a $t$ statistic.\n$$t_{0}=\\frac{\\overline{y}_{i}-\\overline{y}_{j}-d}{\\sqrt{MSE(\\frac{1}{n_{i}}+% \\frac{1}{n_{j}})}}$$\n  Compute the p-value or rejection region\n$$\\begin{aligned} \\text{(a) }p \u0026amp;=\u0026amp;P(t_{N-g}\u0026gt;t_{0}) \\ \\text{(b) }p \u0026amp;=\u0026amp;P(t_{N-g}\u0026lt;t_{0}) \\ \\text{(c) }p \u0026amp;=\u0026amp;2\\min [P(t_{N-g}\u0026gt;t_{0}),P(t_{N-g}\u0026lt;t_{0})]\\end{aligned}$$\nAlternatively, we can define the rejection region as follows:\n$$\\begin{aligned} \\text{(a)}{\\text{ }t_{N-g} \u0026amp;:\u0026amp;t_{N-g}\u0026gt;\\text{ }t_{\\alpha_{family}/k,N-g}} \\ \\text{(b) }{t_{N-g} \u0026amp;:\u0026amp;t_{N-g}\u0026lt;-t_{\\alpha_{family}/k,N-g}} \\ \\text{(c) }{t_{N-g} \u0026amp;:\u0026amp;|t_{N-g}|\u0026gt;\\text{ }t_{\\alpha_{family}/2k,N-g}}\\end{aligned}$$\n  Make a conclusion\n  If the $p$-value $\\leq \\frac{\\alpha_{family}}{k}$, we reject $H_{0}.$ If the $p$-value $\u0026gt;\\frac{\\alpha_{family}}{k}$, we fail to reject $H_{0}.$ Equivalently, if $t_{0}$ lies within the rejection region, we reject $H_{0}$ . If $t_{0}$ lies outside the rejection region, we fail to reject $H_{0}.$\nIn general, getting the values of $t$ requires a computer since rarely will $% \\frac{\\alpha_{family}}{2k}$ be a value in a table. To get the values of $t$ using Excel, you can follow Figure [bonf]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;bonf\u0026rdquo;} below. You would substitute in a number for \u0026ldquo;alpha/k\u0026rdquo; and \u0026ldquo;N-g.\u0026rdquo;\nExcel already knows to divide by $2$**¬†since it assumes you want a value of** $t$**¬†for a two-sided confidence interval. Therefore, you just simply specify** $\\frac{\\alpha_{family}}{k}.$\nThe following statistics were calculated for an experiment:\nForm all $95%$ confidence intervals between the four groups so that we can be $95%$ confident in the entire set of intervals. Use the fact that $% t_{0.05/(2\\ast 6),49}=2.75.$\nThere are a total of six groups to compare ($\\binom{4}{2}=\\frac{4(3)}{2}=6).$ From the ANOVA table, If we want to be $95%$ confident in the entire set of intervals, we use $\\alpha_{family}=\\frac{0.05}{6}=0.0083.$ The value of $t$ we need is $t_{0.0083/2,49}=2.75$ since there are $53$ total observations and $4$ groups, giving $N-g=53-4=49.$ MSE can be calculated as $102.47$ (check this for yourself).\nNow we can compare the groups. The calculation for the first comparison, between Group 1 and Group 2, is shown here. The other calculations are similar; all that changes are the two means you are comparing and the two sample sizes under the square root.\n$(30-35)\\pm 2.75\\sqrt{102.47\\left( \\frac{1}{10}+\\frac{1}{14}\\right) }% =[-16.5,6.5].$ Since this interval contains $0$, at the $\\alpha_{family}=0.05$ level, we would fail to reject $H_{0}:\\mu_{1}-\\mu_{2}$; that is, we could not conclude that there was a difference in Group 1 and Group 2.\n   Group Interval     $1$ vs. $2$ $[-16.5,6.5]$ (no difference)   $1$ vs. $3$ $[-15.2,9.2]$(no difference)   $1$ vs. $4$ $[-21,1]$(no difference)   $2$ vs. $3$ $[-9.2,13.2]$(no difference)   $2$ vs. $4$ $[-15,5]$(no difference)   $3$ vs. $4$ $[-17.7,3.7]$(no difference)    Thus, we cannot conclude at the familywise level of $0.05$ (or state with the familywise confidence level of $0.95)$ that there is a difference in any of the groups.\nA study was done to determine if the amount of time people spend on the internet varies by martial status. The data, from the $2012$ General Social Survey, are summarized below.\nForm all $99%$ confidence intervals for comparing Divorced to the other groups such that we can be $99%$ confident in the entire set of intervals. Use the fact that $t_{0.01/(2\\ast 3),1807}=2.939.$\nThere are a total of three groups to compare (Divorced vs. Married, Divorced vs. Never Married, and Divorced vs. Separated). If we want to be $99%$ confident in the entire set of intervals, we use $\\alpha_{family}=\\frac{0.01% }{3}=0.0033.$ The value of $t$ we need is $t_{0.0033/2,1807}=2.939$, since there are $1811$ total observations and $4$ groups, giving $N-g=1811-4=1807.$ MSE can be calculated as $210.67$ (check this for yourself). The first interval, Divorced Vs. Married, is calculated as follows:\n$$(8.4-9.4)\\pm 2.939\\sqrt{210.67\\left( \\frac{1}{317}+\\frac{1}{900}\\right) }% =[-3.8,1.8]$$\n   Group Interval     Divorced vs. Married $[-3.8,1.8]$(no difference)   Divorced vs. Never Married $[-8.1,-2.1]$(difference)   Divorced vs. Separated $[-8.2,3.2]$(no difference)    Thus, we can conclude at the familywise level of $0.01$ (or state with the familywise confidence level of $0.99)$ that there is a difference in the average amount of time spent online per week between divorced persons and persons who have never married. Specifically, we conclude that persons who have never married spend, on average, between $2$ and $8$ more hours per week online than those who have been divorced.\n","date":1595949567,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595949567,"objectID":"1b279c594d7a4a0d7575a18641c03aa1","permalink":"/courses/bana3363/11-anova-basics/","publishdate":"2020-07-28T15:19:27.628999Z","relpermalink":"/courses/bana3363/11-anova-basics/","section":"courses","summary":"ANOVA Basics In the last couple of chapters, we have discussed how to make inferences about the differences in two populations in terms of means and proportions. What if we wanted to compare more than two populations simultaneously?","tags":null,"title":"ANOVA Basics","type":"docs"},{"authors":null,"categories":null,"content":"","date":1595949567,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595949567,"objectID":"91c7ba4a250602996e77daf8297acb7e","permalink":"/courses/bana3363/12-anova-mc/","publishdate":"2020-07-28T15:19:27.628999Z","relpermalink":"/courses/bana3363/12-anova-mc/","section":"courses","summary":"","tags":null,"title":"ANOVA - Bonferroni Method of Multiple Comparisons","type":"docs"},{"authors":null,"categories":null,"content":"Chi Squared Goodness-of-Fit Test Beginning with our discussion of ANOVA, we have moved away from making inferences about single parameters such as $\\mu$ and $p$ and have instead been examining a¬†general or overall model. The ANOVA $F$ test is testing whether the means of $g\u0026gt;1$ groups $\\mu_{1},\\ldots ,\\mu_{g}$ are identical against the alternative hypothesis that at least one of the means is different from the rest. The ANOVA $F$ test is used to examine numerical data divided into groups.\nTo examine nominal or categorical data (i.e., data that we cannot directly summarize using mathematical measures such as the sample mean and standard deviation), we could examine a graph. The primary tools used to graph nominal or categorical variables are bar charts and pie charts. For example, the 2012 General Social Survey (GSS) asked respondents to what degree they agreed with the statement \u0026ldquo;Both men and women contribute to household income.\u0026rdquo; The results are summarized below in Figure [husweef]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;husweef\u0026rdquo;}. We can easily see that most people agree or strongly agree with the statement.\nWhat if we wanted to determine whether attitudes on this issue have changed over time, say, since 1994? One way we could analyze this is to make side-by-side bar charts and examine the patterns for the two years. If the patterns differed, we would have evidence of a shift. However, the problem with the graphical approach is that it is based solely on individual judgement. What one person might think is a pattern shift worth investigating another person might dismiss as unimportant. Therefore, we need another tool to remove some (not all) of the subjectivity. A hypothesis test is one such tool.\nBefore we continue, we need to discuss the concept of a multinomial experiment.\nA multinomial experiment is one that is conducted such that i). the experiment consists of a fixed number of \u0026ldquo;trials,\u0026rdquo; $n$; ii). the outcome of each trial can be classified into one and only one of $k\u0026gt;2$ categories; iii). the probability $p_{i}$ of an outcome falling into category $i$ is constant for each trial; iv). the sum of all the probabilities is one, that is, $\\dsum p_{i}=1;$ v). each trial is independent of the other trials.\nA binomial experiment is one you are familiar with. There are $n$ trials, the outcome of each trial can be classified as a \u0026ldquo;success\u0026rdquo; or \u0026ldquo;failure,\u0026rdquo; and the probability of success remains the same on each trial. With a binomial experiment, the probability of success is $p$, which means that the probability of failure is $1-p$ because, by design, there are only two outcomes, and one of them has to happen. I could simply rename the probability of success as $p_{1}$ and the probability of failure $p_{2}=1-p_{1}.$ Then $p_{1}+p_{2}=p_{1}+(1-p_{1})=1.$ In a binomial experiment, we further assume that each of the $n$ trials is independent (see Handout 1). Therefore, we can view the binomial experiment as a special case of the multinomial experiment when $k=2.$ If we let the random variable $X$ be the number of successes out of $n$ trials, we say that \u0026ldquo;$X$ has a binomial distribution with parameters $n$ and $p.\u0026ldquo;$\nA multinomial experiment simply adds more categories. Instead of only two (\u0026ldquo;success\u0026rdquo; and \u0026ldquo;failure\u0026rdquo;), we have three, four, five, six, or, in general, $k$ different categories. To see the idea, suppose the experiment is to ask a random sample of $n$ Americans to report their job classification as one of the following: ${$working full time, working part time, keeping house, in school, other$}.$ Then, there are five probabilities $p_{1},p_{2},p_{3},p_{4}$, and $p_{5}$, one for every possible category. The probability that someone answers \u0026ldquo;working full time\u0026rdquo; is $p_{1}$, the probability that someone answers \u0026ldquo;working part time\u0026rdquo; is $p_{2}$, and so on. By including the \u0026ldquo;other\u0026rdquo; category, we ensure that each person we ask can be classified into exactly one of the five categories. Therefore, $p_{1}+p_{2}+p_{3}+p_{4}+p_{5}=1.$ Just like with the binomial model, however, we can say that $p_{5}=1-(p_{1}+p_{2}+p_{3}+p_{4})$, so if we know four of the probabilities, we can get the fifth by subtraction. This fact will play a part in understanding the test to be presented.\nThe generalization of the binomial distribution to $k\u0026gt;2$ categories is called the multinomial distribution, which is defined now.\nIf $X_{1}$ is the number of outcomes classified in Category $1$, $X_{2}$ is the number of outcomes classified in Category $2,\\ldots , and X_{k}$ is the number of outcomes classified in Category $k$, then\n$$P(X_{1}=x_{1},X_{2}=x_{2},\\ldots ,X_{k}=x_{k})=\\frac{n!}{x_{1}!x_{2}!\\ldots x_{k}!}p_{1}^{x_{1}}p_{2}^{x_{2}}\\cdots p_{k}^{x_{k}}$$\nHere is a simple example.\nIn a certain town, $40%$ of the eligible voters prefer candidate A, $10%$ prefer candidate B, and the remaining $50%$ have no preference. You randomly sample $10$ eligible voters. What is the probability that $4$ will prefer candidate A, $1$ will prefer candidate B, and the remaining $5$ will have no preference?\nAccording to the Houston Chronicle, the race/ethnicity distribution in 2010 for Harris county is as shown in Figure [race]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;race\u0026rdquo;}.\nSuppose that a jury of twelve members is chosen from the adult population in Harris County in such a way that each resident has an equal probability of being selected independently of every other resident. What is the probability of obtaining a jury with $5$ people who are non-hispanic white, $2$ people who are Hispanic, $3$ people who are African American, $1$ person who is Asian, and $1$ person who belongs to the \u0026ldquo;other\u0026rdquo; category?\nThe connection between the multinomial distribution and Figure [husweef]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;husweef\u0026rdquo;} is that we can imagine that, in 2012, there were \u0026ldquo;true\u0026quot;proportions $p_{1}$, $p_{2},\\ldots ,p_{5}$ corresponding the probability that an adult $18$ and over would respond with one of the categories shown. Therefore, we can view Figure [husweef]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;husweef\u0026rdquo;} as the outcome of a (large) multinomial trial where the bar heights are the number of people who fell into each category.\nIn practice, we don\u0026rsquo;t know these true values, but we can estimate them using data. The idea is exactly the same as estimating $p$ using the sample proportion, except now we have $k$ different sample proportions. Specifically, we can say that $\\widehat{p}{1}$ is the number of people who \u0026ldquo;strongly agree,\u0026rdquo; call it $n{1}$, out of the total number of people surveyed, $n.$ That is $\\widehat{p}{1}=\\frac{n{1}}{n}$. Similarly, $\\widehat{p}{2}$ is the number of people who \u0026ldquo;Agree,\u0026rdquo; call it $n{2}$, out of the total number of people surveyed. That is, $\\widehat{p}{2}=\\frac{n{2}% }{n}.$ In this data set, there were $1,267$ observations, and $249$ people \u0026ldquo;strongly agreed.\u0026rdquo; Therefore, $\\widehat{p}_{1}=\\frac{249}{1267}=0.197.$ The sample proportions are shown in the following table.\n   Category Sample Proportion     Strongly Agree $0.197$   Agree $0.452$   Neutral $0.242$   Disagree $0.096$   Strongly Disagree $0.013$    These sample proportions form an estimated multinomial distribution. It is estimated because it was formed using data, not the true parameters. The true distribution could be summarized in a table. The true probabilities are, of course, unknown.\n   Category Pr(Category)     Strongly Agree $p_{1}$   Agree $p_{2}$   Neutral $p_{3}$   Disagree $p_{4}$   Strongly Disagree $p_{5}$    We can use the sample proportions that we have to determine the \u0026ldquo;goodness of fit\u0026rdquo; between a theorized distribution and an observed distribution. If the observed distribution is \u0026ldquo;too different\u0026rdquo; from the theorized distribution, the \u0026ldquo;fit\u0026rdquo; will be poor and we would have objective evidence to claim that the two distributions are different. The mechanism by which this works is the subject of the chi-squared goodness-of-fit test. It is a hypothesis test just like any other, but, as always, the specifics differ. Here is a rundown of the test.\n  Specify the hypothesis:\nNow, instead of one or two proportions we have several. The null hypothesis specifies that the different $p^{\\prime }s$ are equal to specified values, which we will denote as $p_{0i}.$ Note that they *do not* have to be equal to one another.\n$$\\begin{aligned} H_{0} \u0026amp;:\u0026amp;p_{1}=p_{01},p_{2}=p_{02},\\ldots ,p_{k}=p_{0k} \\ H_{1} \u0026amp;:\u0026amp;\\text{ at least one }p\\text{ is not equal to its specified value}\\end{aligned}$$\n  Calculate the test statistic\nThe test statistic is a bit different:\n$$\\chi ^{2}=\\frac{\\dsum (f_{i}-e_{i})^{2}}{e_{i}}$$\n  where $f_{i}$ is the \u0026ldquo;observed frequency\u0026rdquo; in category $i$ and $e_{i}$ is the \u0026ldquo;expected frequency\u0026rdquo; in category $i.$ The observed frequencies are given by the data. To find the $e_{i}$, note that if we have a total of $n$ observations, and there is probability $p_{i0}$ that an observation falls into category $i$ under the null hypothesis, it is natural to say that the expected number of observations falling into category $i$ is\n$$e_{i}=np_{i0}.$$\nNote the logic behind the test statistic. What it essentially is doing is comparing the actual frequencies we have to what we should see if the null hypothesis were true. If $\\chi ^{2}$ is \u0026ldquo;too large,\u0026rdquo; this implies that the estimated distribution is far from the theorized distribution, and therefore we doubt that the theorized distribution is the one that gave rise to the data that we have.\nThe test statistic has what is called a \u0026ldquo;chi-squared distribution.\u0026quot; This is a new distribution to us.\nA chi-squared random variable with $k$ degrees of freedom, denoted $\\chi_{k}^{2}$, arises as the sum of $k$ independent squared standard normal random variables, $Z_{i}^{2}$.\nThe distribution takes on various shapes according to $k$ as you can see in Figure [chisq]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;chisq\u0026rdquo;}. Importantly, it always ranges from $0$ to $\\infty .$\n Compute the p-value or rejection region  $$pval=P(\\chi_{k-1}^{2}\u0026gt;\\chi ^{2})$$\nLike the test for means, unless we have software, we usually go with the rejection region approach. The the rejection region (i.e., the \u0026ldquo;critical values\u0026rdquo;) for a test with the $\\alpha$ level of significance would be computed as follows:\n$${\\chi_{k-1}^{2}:\\chi_{k-1}^{2}\u0026gt;\\chi_{k-1,\\alpha }^{2}}$$\nThe reason we use $k-1$ rather than $k$ is that there is a restriction on the $p_{i}:$ they must sum to $1.$ So, if we know $k-1$ of the probabilities, we can get the $k^{th}$ one.\nAs with the ANOVA $F$ test, there is only one \u0026ldquo;guard\u0026rdquo; for the rejection region, since the test is one-sided. This reflects the nature of the test statistic above. If $\\chi ^{2}$ is small, we are inclined to believe that the observed distribution could have been produced by the theorized distribution. If $\\chi ^{2}$ is large, we would conclude the opposite. So we only look for \u0026ldquo;large\u0026rdquo; values.\n Make a conclusion  If the $p$-value $\\leq \\alpha $, we reject $H_{0}.$ If the $p$-value $\u0026gt;\\alpha $, we fail to reject $H_{0}.$ Equivalently, if $\\chi ^{2}$ lies within the rejection region, we reject $H_{0}$. If $\\chi ^{2}$ lies outside the rejection region, we fail to reject $H_{0}.$\nThis test is known as an asymptotic test because it is only valid if the sample size is \u0026ldquo;large enough.\u0026rdquo; and the $e_{i}$ are all \u0026ldquo;large enough.\u0026rdquo; There is some debate about how \u0026ldquo;large\u0026rdquo; these have to be in practice for the test to work properly, but one simple rule that has been shown to work is to require that $n\\geq$ $10,\\frac{n^{2}}{c}\\geq 10$, and all $e_{i}\\geq 0.25$. Usually the last requirement will be the problem. An easy fix is to simply combine categories.\nLet\u0026rsquo;s work an example putting all of this together.\nIn $1994$, the distribution of responses to the statement \u0026ldquo;Both husband and wife should contribute to household income\u0026rdquo; was as follows:\n   Category Pr(Category)     Strongly Agree $0.197$   Agree $0.379$   Neutral $0.296$   Disagree $0.115$   Strongly Disagree $0.013$    $1,267$*¬†people were surveyed in* $2012$*¬†and the numbers of responses in each category are shown below. Do we have evidence at the* $\\alpha =0.05$*¬†level of significance that opinion on this issue has changed since* $1994$*?*\nAccording to the Houston Chronicle, the race/ethnicity distribution in 2010 for Harris county is as shown in Figure [race]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;race\u0026rdquo;}. From January through May of 2013, a total of $63,207$ people were called for jury service. The distribution by race is as shown in Figure [jury_call]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;jury_call\u0026rdquo;}.\nDo we have evidence at the $\\alpha =0.10$ level that juries in Harris county do not reflect the true demographics of the county?\nAccording to the American Academy of Ophthalmology , the distribution of eye color in the United States is as follows:\n   Color Percent of U. S. Pop     Blue /Grey Irises $32%$   Blue / Grey / Green Irises with Brown / Yellow Specks $15$   Green / Light Brown Irises with Minimal Specks $12$   Brown Irises with Specks $16$   Dark Brown Irises $25$    Record the number of people in our class who fall into each category in the table below. Do we have evidence at the $\\alpha =0.05$¬†level to conclude that our class has the same distribution of eye color as the U. S. population? Note: You should verify that the assumptions are met first.\n   Color Percent of U. S. Pop     Blue /Grey Irises $32%$   Blue / Grey / Green Irises with Brown / Yellow Specks $15$   Green / Light Brown Irises with Minimal Specks $12$   Brown Irises with Specks $16$   Dark Brown Irises $25$    ","date":1595949567,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595949567,"objectID":"abf0750eeb92f6dd447700e7e3fac423","permalink":"/courses/bana3363/13-chi-squared-goodness-of-fit-test/","publishdate":"2020-07-28T15:19:27.628999Z","relpermalink":"/courses/bana3363/13-chi-squared-goodness-of-fit-test/","section":"courses","summary":"Chi Squared Goodness-of-Fit Test Beginning with our discussion of ANOVA, we have moved away from making inferences about single parameters such as $\\mu$ and $p$ and have instead been examining a¬†general or overall model.","tags":null,"title":"Chi Squared Goodness-of-Fit Test","type":"docs"},{"authors":null,"categories":null,"content":"Confidence Interval for the Difference in Means (Matched Pairs) The confidence interval that we examined in the last handout applies when we are working with two independent samples. Often, however, it makes sense to examine the effect of a treatment on samples that are in some sense related. These samples are called matched pairs. One way a matched pair can form is if each unit that is sampled (a person or object) is studied before and after a treatment is applied. The two sets of data\u0026ndash;measurements before and after treatment\u0026ndash;will not be independent because the same entity is being measured both times. Another way to form matched pairs is to make groups that are related genetically (e.g. twins), by proximity to one another (e.g., neighbors), by time of measurement (e.g., months of the year), by relationship to one another (e.g., husband/wife), or in some other fashion (e.g., socioeconomic status).\nWhen we have a matched pairs design, the analysis is actually easier than for the two independent samples case. What we do, essentially, is make a new variable that represents the differences between the groups on the measure of interest, and then perform one-sample techniques for $\\mu_{d},$ the population mean difference. We need to calculate $\\overline{x_{D}},$ the sample mean difference, and $s_{D},$ the standard deviation of the differences. Both of these are calculated using the number of pairs, $n_{d}.$ The formula for the confidence interval is given now.\nA $(1-\\alpha )100%$ confidence interval for the mean difference between two related groups, denoted $\\mu_{d},$ is\n$$\\overline{x_{D}}\\pm t_{\\alpha /2,n_{d}}\\frac{s_{D}}{\\sqrt{n_{d}}}$$\nHere is an example.\nA video game design firm wants to determine if there is a difference in the accuracy of gamers that can be attributed to the assignment of player actions (shooting, jumping, changing weapons, etc.) to certain button layouts. A sample of 6 professional game testers each tried two types of layouts in a particular level under a particular attack scenario. The performance (measured as the percentage of enemies defeated) for each person is shown in Figure [vidya_game]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;vidya_game\u0026rdquo;}. Calculate a $90%$ confidence interval for the true average difference in performance between the two layouts. What can you conclude?\n\\FRAME{ftbphFU}{4.0395in}{2.5079in}{0pt}{\\Qcb{Video game performance data}}{% \\Qlb{vidya_game}}{Figure}{\\special{language \u0026ldquo;Scientific Word\u0026rdquo;;type \u0026ldquo;GRAPHIC\u0026rdquo;;maintain-aspect-ratio TRUE;display \u0026ldquo;USEDEF\u0026rdquo;;valid_file \u0026ldquo;T\u0026rdquo;;width 4.0395in;height 2.5079in;depth 0pt;original-width 8.6248in;original-height 5.3333in;cropleft \u0026ldquo;0\u0026rdquo;;croptop \u0026ldquo;1\u0026rdquo;;cropright \u0026ldquo;1\u0026rdquo;;cropbottom \u0026ldquo;0\u0026rdquo;;tempfilename \u0026lsquo;N2QP2Q00.wmf\u0026rsquo;;tempfile-properties \u0026ldquo;XPR\u0026rdquo;;}}\nIn a recent study, the effect that cell phone use had on reaction time while driving was studied. A sample of $20$ drivers was selected and each one was asked to drive through an obstacle course. Each person\u0026rsquo;s average reaction time (in seconds) to stimuli along the course was recorded both during a drive with no cell phone use and during a drive where the subject was engaged in a casual conversation with a friend. The data are summarized in Figure [phone]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;phone\u0026rdquo;} below.\nWhy might a matched pairs design be a good choice for studying this question? Compute a $99%$ confidence interval for the true average difference in reaction time between using a cell phone and not using a cell phone.\n\\FRAME{ftbphFU}{2.0211in}{2.8556in}{0pt}{\\Qcb{Cell phone driving reaction time data.}}{\\Qlb{phone}}{Figure}{\\special{language \u0026ldquo;Scientific Word\u0026rdquo;;type \u0026ldquo;GRAPHIC\u0026rdquo;;maintain-aspect-ratio TRUE;display \u0026ldquo;USEDEF\u0026rdquo;;valid_file \u0026ldquo;T\u0026rdquo;;width 2.0211in;height 2.8556in;depth 0pt;original-width 2.7086in;original-height 3.8441in;cropleft \u0026ldquo;0\u0026rdquo;;croptop \u0026ldquo;1\u0026rdquo;;cropright \u0026ldquo;1\u0026rdquo;;cropbottom \u0026ldquo;0\u0026rdquo;;tempfilename \u0026lsquo;MU3NOL08.wmf\u0026rsquo;;tempfile-properties \u0026ldquo;XPR\u0026rdquo;;}}\nA consumer group wants to determine if husbands and wives spend differing amounts of money on each other on Valentine\u0026rsquo;s Day. A sample of seven married couples who had been married at least one year was taken, and the data (in dollars spent) are shown in the table below. Why might a matched pairs design be a good choice for studying this question? Compute a $95%$ confidence interval for the true average difference between husbands and wives in the amount of money spent on Valentine\u0026rsquo;s Day .\n\\FRAME{ftbphFU}{1.8498in}{1.5783in}{0pt}{\\Qcb{Money spent by husbands and wives on Valentine\u0026rsquo;s Day gifts.}}{\\Qlb{valentime}}{Figure}{\\special{language \u0026ldquo;Scientific Word\u0026rdquo;;type \u0026ldquo;GRAPHIC\u0026rdquo;;maintain-aspect-ratio TRUE;display \u0026ldquo;USEDEF\u0026rdquo;;valid_file \u0026ldquo;T\u0026rdquo;;width 1.8498in;height 1.5783in;depth 0pt;original-width 1.8127in;original-height 1.542in;cropleft \u0026ldquo;0\u0026rdquo;;croptop \u0026ldquo;1\u0026rdquo;;cropright \u0026ldquo;1\u0026rdquo;;cropbottom \u0026ldquo;0\u0026rdquo;;tempfilename \u0026lsquo;MU3NOM09.wmf\u0026rsquo;;tempfile-properties \u0026ldquo;XPR\u0026rdquo;;}}\nHow objectively do we view ourselves in terms of attractiveness? A website offers to give a person an attractiveness score (from $1$ to $10$) based on an uploaded photo. A random sample of five people uploaded a photo the website. Before their scores were given, they were asked to predict what the computer would say. The data are shown below. Compute a $90%$ confidence interval for the true average difference between a person\u0026rsquo;s predicted rating and their actual rating.\n\\FRAME{ftbphFU}{1.6924in}{1.2004in}{0pt}{\\Qcb{Predicted and actual attractiveness scores.}}{\\Qlb{attraction}}{Figure}{\\special{language \u0026ldquo;Scientific Word\u0026rdquo;;type \u0026ldquo;GRAPHIC\u0026rdquo;;maintain-aspect-ratio TRUE;display \u0026ldquo;USEDEF\u0026rdquo;;valid_file \u0026ldquo;T\u0026rdquo;;width 1.6924in;height 1.2004in;depth 0pt;original-width 1.6561in;original-height 1.1666in;cropleft \u0026ldquo;0\u0026rdquo;;croptop \u0026ldquo;1\u0026rdquo;;cropright \u0026ldquo;1\u0026rdquo;;cropbottom \u0026ldquo;0\u0026rdquo;;tempfilename \u0026lsquo;MU3NOM0A.wmf\u0026rsquo;;tempfile-properties \u0026ldquo;XPR\u0026rdquo;;}}\nThe general four-step procedure for hypothesis testing still applies. What changes are the specifics.¬†Let\u0026rsquo;s see how.\n  Specify the hypothesis:\n$$\\begin{aligned} \\text{(a) }H_{0} \u0026amp;:\u0026amp;\\mu_{d}\\leq d\\text{ vs. }H_{1}:\\mu_{d}\u0026gt;d \\ \\text{(b) }H_{0} \u0026amp;:\u0026amp;\\mu_{d}\\geq d\\text{ vs. }H_{1}:\\mu_{d}\u0026lt;d\\text{ } \\ \\text{(c) }H_{0} \u0026amp;:\u0026amp;\\mu_{d}=d\\text{ vs. }H_{1}:\\mu_{d}\\neq d\\end{aligned}$$\n  Calculate the test statistic\nWe will assume from now on that we never know the true standard deviation of the population, so our statistic will be a $t.$\n$$t_{0}=\\frac{\\overline{x_{d}}-d}{\\frac{s_{d}}{\\sqrt{n_{d}}}}$$\n  Compute the rejection region\nThe the rejection region (i.e., the \u0026quot;critical values\u0026quot;) for a test with the $% \\alpha$ level of significance would be computed as follows:\n$$\\begin{aligned} \\text{(a)}{\\text{ }t_{n_{d}-1} \u0026amp;:\u0026amp;t_{n_{d}-1}\u0026gt;\\text{ }t_{\\alpha ,_{n_{d}-1}}} \\ \\text{(b) }{t_{n_{d}-1} \u0026amp;:\u0026amp;t_{n_{d}-1}\u0026lt;-t_{\\alpha ,n_{d}-1}} \\ \\text{(c) }{t_{n_{d}-1} \u0026amp;:\u0026amp;|t_{n_{d}-1}|\u0026gt;\\text{ }t_{\\alpha /2,n_{d}-1}}\\end{aligned}$$\nwhere $t_{n_{d}-1}$is a Student\u0026rsquo;s $t$ random variable with $n_{d}-1$ degrees of freedom.\n  Make a conclusion\n  If the $p$-value $\\leq \\alpha ,$ we reject $H_{0}.$ If the $p$-value $% \u0026gt;\\alpha ,$ we fail to reject $H_{0}.$ Equivalently, if $t_{0}$ lies within the rejection region, we reject $H_{0}$. If $t_{0}$ lies outside the rejection region, we fail to reject $H_{0}.$\nPerform a hypothesis test at the $\\alpha =0.05$ level for all of the examples above.\n","date":1595949567,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595949567,"objectID":"48264524a338300b2fde0ac5244f570b","permalink":"/courses/bana3363/8-matched-pairs/","publishdate":"2020-07-28T15:19:27.628999Z","relpermalink":"/courses/bana3363/8-matched-pairs/","section":"courses","summary":"Confidence Interval for the Difference in Means (Matched Pairs) The confidence interval that we examined in the last handout applies when we are working with two independent samples. Often, however, it makes sense to examine the effect of a treatment on samples that are in some sense related.","tags":null,"title":"Matched Pairs","type":"docs"},{"authors":null,"categories":null,"content":"Another Example of Goodness-of-Fit We begin with another example of the chi-squared goodness-of-fit test. The sources for the data are the National Highway Traffic Safety Administration and the Texas Department of Transportation.\nThe percentage of fatal accidents by day of the week for the United States as a whole is shown in Figure [accidents]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;accidents\u0026rdquo;} below, and the number of accidents in Texas by day of the week is shown in Figure [texas_crah]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;texas_crah\u0026rdquo;}. Can we conclude at the $\\alpha =0.05$ level that Texas differs from the rest of the United States in the distribution of crashes over the days of the week?\n\\FRAME{ftbphFU}{5.5019in}{1.4235in}{0pt}{\\Qcb{Distribution of fatal accidents by day of the week.}}{\\Qlb{accidents}}{Figure}{\\special{language \u0026ldquo;Scientific Word\u0026rdquo;;type \u0026ldquo;GRAPHIC\u0026rdquo;;maintain-aspect-ratio TRUE;display \u0026ldquo;USEDEF\u0026rdquo;;valid_file \u0026ldquo;T\u0026rdquo;;width 5.5019in;height 1.4235in;depth 0pt;original-width 9.8442in;original-height 2.5209in;cropleft \u0026ldquo;0\u0026rdquo;;croptop \u0026ldquo;1\u0026rdquo;;cropright \u0026ldquo;1\u0026rdquo;;cropbottom \u0026ldquo;0\u0026rdquo;;tempfilename \u0026lsquo;MVWFYL04.wmf\u0026rsquo;;tempfile-properties \u0026ldquo;XPR\u0026rdquo;;}}\n\\FRAME{ftbphFU}{5.5495in}{2.3229in}{0pt}{\\Qcb{Number of fatal crashes in Texas for 2012 by day of the week.}}{\\Qlb{texas_crah}}{Figure}{\\special% {language \u0026ldquo;Scientific Word\u0026rdquo;;type \u0026ldquo;GRAPHIC\u0026rdquo;;maintain-aspect-ratio TRUE;display \u0026ldquo;USEDEF\u0026rdquo;;valid_file \u0026ldquo;T\u0026rdquo;;width 5.5495in;height 2.3229in;depth 0pt;original-width 8.8851in;original-height 3.6979in;cropleft \u0026ldquo;0\u0026rdquo;;croptop \u0026ldquo;1\u0026rdquo;;cropright \u0026ldquo;1\u0026rdquo;;cropbottom \u0026ldquo;0\u0026rdquo;;tempfilename \u0026lsquo;MVWG5406.wmf\u0026rsquo;;tempfile-properties \u0026ldquo;XPR\u0026rdquo;;}}\nChi-Squared Test of Independence A common question in research is what, if any, relationship two variables have with one another. If the two variables are numeric, then the most common measure of dependence is correlation, which we will discuss later. Correlation specifically measures the degree of linear relationship between two numeric variables. The central question in this handout is: how can we examine the relationship between two nominal/categorical variables?\nJust as with the chi-squared goodness-of-fit test, we can employ two basic (and complementary) techniques: graphs and a hypothesis test. Graphs such as side-by-side bar charts allow us to examine the distribution of one variable (called¬†the¬†response variable) for each level of another variable that we believe somehow influences the response variable; this latter variable is called the**¬†explanatory (independent) variable. If there is a relationship, the pattern of the response variable will change at different levels of the explanatory variable.\nLet\u0026rsquo;s look again at the question on the 2012 General Social Survey (GSS) that asked respondents to what degree they agreed with the statement \u0026ldquo;Both men and women contribute to household income.\u0026rdquo; The results are summarized below in Figure [husweef]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;husweef\u0026rdquo;}. We can easily see that most people agree or strongly agree with the statement.\nThis graph shows the responses of all $1,974$ people in the 2012 survey added across all age groups,income levels, ages, and political affiliations. What if we were interested in examining whether there was a relationship between belief in both men and women contributing to household income and gender? Figure [two_inc_gen]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;two_inc_gen\u0026rdquo;}¬†shows the distribution of agreement on the question by gender. What we look for when we examine a side-by-side bar chart is a change in the pattern of responses by the levels of the independent variable. The heights of the bars will usually differ, as there are typically different sample sizes in the two categories of the independent variable. In the figure below, we do not see a clear change to the pattern of responses between men and women, and would therefore¬†(informally) conclude that there is little or no relationship between gender and belief in both men and women contributing to household income.\nHow can we assess this relationship formally? As you might have gathered, we can accomplish this by the use of a hypothesis test. In order to understand the test, we have to recall the definition of joint and marginal probabilities.\nThe joint probability distribution for two discrete random variables is a specification of the possible values that the two variables can take on simultaneously and their simultaneous, or \u0026ldquo;joint,\u0026rdquo; probabilities.\nAs a simple example, suppose the random variable $X$ can take on the values of $1,2$, or $3$, and the random variable $Y$ can take on the values $1$ and $2.$ Then a joint probability distribution for $X$ and $Y$ might be as follows:\n   $X$ $\\mathbf{1}$ $\\mathbf{2}$ $\\mathbf{3}$     $\\mathbf{1}$ $.24$ $.12$ $.06$   $\\mathbf{2}$ $.30$ $.14$ $.14$    The numbers inside the table are the joint probabilities, the probabilities that $X$ will take on a certain value $x$ and $Y$ will take on a certain value $y,$ which we write as $P(X=x,Y=y).$ For instance, we could say that the probability that $X$ $=$ $1$ and $Y=2$ is $% P(X=1,Y=2)=0.12,$ since this number is in the intersection of the $\u0026quot;1\u0026quot;$ row and and $\u0026quot;2\u0026quot;$ column. Similarly, $P(X=2,Y=1)=0.28.$ Let\u0026rsquo;s check our understanding.\n[[joint_prob_ex]]{#joint_prob_ex label=\u0026quot;joint_prob_ex\u0026rdquo;}Find the following joint probabilities.\na). $P(X=1,Y=1)$\nb). $P(X=1,Y=2)$\nc). $P(X=1,Y=3)$\nd). $P(X=2,Y=1)$\ne). $P(X=2,Y=2)$\nf). $P(X=2,Y=3)$\nFrom a joint distribution, we can always obtain the distribution of each variable individually by adding across the levels of the other variable. These distributions are called marginal distributions because they can be computed in the margins of a table like the one above. Here is the table above, with the totals of the rows and columns in the margins:\n   $X$ $\\mathbf{1}$ $\\mathbf{2}$ $\\mathbf{3}$ Total     $\\mathbf{1}$ $.24$ $.12$ $.06$ $\\mathbf{.42}$   $\\mathbf{2}$ $.30$ $.14$ $.14$ $\\mathbf{.58}$   Total $\\mathbf{.54}$ $\\mathbf{.26}$ $\\mathbf{.20}$ $\\mathbf{%1}$    The marginal distributions for $X$ and $Y$ can then be listed separately. The marginal distribution of $X$ is\n   $x$ $P(X=x)$     $1$ $.42$   $2$ $.58$   ,     and the marginal distribution of $Y$ is\n   $y$ $P(Y=y)$     $1$ $.54$   $2$ $.26$   $3$ $.20$    .\nWe can now define the concept of independence.\n[[independence_def]]{#independence_def label=\u0026quot;independence_def\u0026rdquo;}Two (discrete) random variables $X$ and $Y$ are independent if (and only if) $P(X=x,Y=y)=P(X=x)P(Y=y)$ for all possible $x$ and $y.$ That is, $X$ and $Y$ are independent if every one of the joint probabilities equals the product of its associated marginal probabilities.\nAre $X$ and $Y$ independent in the table above? To establish independence, we need to check all of the probabilities in Exercise [joint_prob_ex]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;joint_prob_ex\u0026rdquo;} to see if the relation $P(X=x,Y=y)=P(X=x)P(Y=y)$ holds for each one. If we find even one instance where this relation does not hold, then we can say that $X$ and $Y$ are not independent (or, more simply, we call them dependent). Let\u0026rsquo;s find $P(X=1)P(Y=1)$ and compare to $P(X=1,Y=1).$ From the tables above we see that\n$$P(X=1)P(Y=1)=(0.42)(0.58)=0.2436$$\nand that\n$$P(X=1,Y=1)=0.24$$\nSince $0.2436\\neq 0.24,$ we can conclude that $X$ and $Y$ are not independent (that is, they are dependent). We need not check the rest of the joint probabilities.\nOf course, in practice, we do not have tables of probabilities given to us. The contingency tables we see are formed from observed data on two variables. If we have a sufficient number of observations, we can estimate joint and marginal probabilities. Since the probabilities will be estimated, the requirement of Definition [independence_def]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;independence_def\u0026rdquo;}, that the joint probabilities be exactly equal to the product of the marginal probabilities, will be too strict. The question we will need to ask is not if there is a difference between the two probabilities, but rather how large the difference is. The basis of the chi-squared test of independence is how closely the estimated probabilities conform to the requirements of Definition [independence_def]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;independence_def\u0026rdquo;} when given a table of data from which these probabilities are estimated.\nLet us now define the test.\n  Specify the hypothesis:\nTo set up the test, we assume we have two nominal/categorical random variables $X$ and $Y.$ There are $r\u0026gt;1$ categories in $X$ and $c\u0026gt;1$ categories in $Y.$ The general contingency table looks like the following.\n  \u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;- \u0026ndash; $Y$ $y_{1}$ $y_{2}$ $\\ldots$ $y_{c}$ $x_{1}$ $p_{12}$ $\\ldots$ $p_{1c}$ $X$ $x_{2}$ $p_{21}$ $\\ldots$ $% p_{2c}$ $\\vdots$ $\\ldots$ $\\ldots$ $% \\vdots$ $x_{r}$ $p_{r2}$ $\\ldots$ $p_{rc}$ \u0026mdash;\u0026ndash; \u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;- \u0026mdash;\u0026mdash;\u0026mdash;- \u0026ndash;\nHere, $p_{ij}$ is the joint probability that the variable $X$ will take on the category $x_{i}$ and $Y$ will take on category $j.$ For example, $p_{21}$ is the probability that $X$ takes on category $x_{2}$ and $Y$ takes on category $y_{1},$ that is, $p_{21}=P(X=x_{2},Y=y_{1}).$ The null hypothesis is that $X$ and $Y$ are independent, which, according to Definition [independence_def]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;independence_def\u0026rdquo;}, means that the joint probabilities should equal the marginal probabilities. So, if the null hypothesis is true, then $% p_{21}=P(X=x_{2},Y=y_{1})=P(X=x_{2})P(Y=y_{1}),$ and similarly for all $% (x,y)$ combinations. So, we have the null hypothesis:\n$$\\begin{aligned} H_{0} \u0026amp;:\u0026amp;p_{ij}=P(X=x_{i})P(Y=y_{j})\\text{ for all }i,j \\ H_{1} \u0026amp;:\u0026amp;\\text{ }p_{ij}\\neq P(X=x_{i})P(Y=y_{j})\\text{ for some }i,j\\end{aligned}$$\nFor our purposes, it will be sufficient to write the following (equivalent, but less precise) statements:\n$$\\begin{aligned} H_{0} \u0026amp;:\u0026amp;\\text{ \\TEXTsymbol{\u0026lt;}Variable }X\u0026gt;\\text{ is independent of \\TEXTsymbol{\u0026lt;} Variable }Y\u0026gt; \\ H_{1} \u0026amp;:\u0026amp;\\text{ The two variables are dependent}\\end{aligned}$$\nwhere we would substitute the name of the variables for the text in the brackets.\n  Calculate the test statistic\nThe test statistic is a similar to the goodness-of-fit statistic:\n$$\\chi^{2}=\\frac{\\sum_{i=1}^{r}\\sum_{j=1}^{c}(f_{ij}-e_{ij})^{2}}{e_{ij}}$$\n  where $f_{ij}$ is the \u0026ldquo;observed frequency\u0026rdquo; in cell $(i,j)$ and $% e_{ij}$ is the \u0026ldquo;expected number of observations\u0026rdquo; in cell $(i,j)$ under the null hypothesis$.$ The observed frequencies $f_{ij}$ are given by the data in the table. If the null hypothesis is true, the expected number of observations to fall in cell $(i,j)$ would be the total sample size, $n,$ times the marginal probabilities for category $x_{i}$ and category $y_{j}.$ To find the $e_{ij},$ note that we can estimate the marginal probability of category $x_{i}$ by defining $\\widehat{p}_{i,x}=\\dsum_{j=1}^{c}\\frac{f_{ij}}{% n}$, that is, the total number of observations in row $i$ divided by the total number of observations in the data set. Similarly, we can estimate the marginal probability of category $\\widehat{p}_{j,y}$ by taking $\\widehat{p}% _{j,y}=\\dsum_{i=1}^{r}\\frac{f_{ij}}{n},$ that is, the total number of observations in column $j$ divided by the total number of observations in the data set. Then the expected number of observations in cell $(i,j)$ is $$e_{ij}=n\\left( \\widehat{p}_{i,x}\\right) \\left( \\widehat{p}_{j,y}\\right)$$\nNote the logic behind the test statistic is the same as for the goodness-of-fit test. What it essentially is doing is comparing the actual frequencies we have to what we should see if the null hypothesis were true. If $\\chi^{2}$ is \u0026ldquo;too large,\u0026rdquo; this implies that the estimated distribution is far from the theorized distribution, and therefore we doubt that the theorized distribution is the one that gave rise to the data that we have.\n  Compute the p-value or rejection region\n$$p\\text{-value}=P(\\chi _{(r-1)(c-1)}^{2}\u0026gt;\\chi^{2})$$\nThe the degrees of freedom value here is found by taking $(r-1)(c-1)$, or the number of rows minus one times the number of columns minus one. Like the test for means, unless we have software, we usually go with the rejection region approach. The the rejection region (i.e., the \u0026ldquo;critical values\u0026rdquo;) for a test with the $\\alpha$ level of significance would be computed as follows:\n$${\\chi _{(r-1)(c-1)}^{2}:\\chi _{(r-1)(c-1)}^{2}\u0026gt;\\chi _{(r-1)(c-1),\\alpha }^{2}}$$\n  As with the ANOVA $F$ test, there is only one \u0026ldquo;guard\u0026rdquo; for the rejection region, since the test is one-sided. This reflects the nature of the test statistic above. If $\\chi^{2}$ is small, we are inclined to believe that the observed distribution could have been produced by the theorized distribution. If $\\chi^{2}$ is large, we would conclude the opposite. So we only look for \u0026ldquo;large\u0026rdquo; values.\n Make a conclusion  If the $p$-value $\\leq \\alpha$, we reject $H_{0}.$ If the $p$-value $% \u0026gt;\\alpha$, we fail to reject $H_{0}.$ Equivalently, if $\\chi^{2}$ lies within the rejection region, we reject $H_{0}$. If $\\chi^{2}$ lies outside the rejection region, we fail to reject $H_{0}$.\nThis test is known as an asymptotic test because it is only valid if the sample size is \u0026ldquo;large enough.\u0026rdquo; and the $e_{i}$ are all \u0026ldquo;large enough.\u0026rdquo; There is some debate about how \u0026ldquo;large\u0026rdquo; these have to be in practice for the test to work properly, but the general recommendation is that all of the $e_{ij}$ must be above $5$ for small tables, or for large tables, $80%$ are above $5.$ If this is not the case, we combine some categories.\nAll of the above makes this test look a lot more complicated than it is. Let\u0026rsquo;s work an example putting all of this together.\nIn the $2012$ General Social Survey, respondents were asked the question \u0026ldquo;In general, can people be trusted?\u0026rdquo; A researcher is interested in seeing if whether someone has been divorced influences their response to this question. The data are shown in the table below. Can we conclude at the $% \\alpha =0.01$ level that being divorced affects one\u0026rsquo;s ability to trust? To help organize your answer, fill out the \u0026ldquo;Expected\u0026rdquo; table.\nRefer to Figure [two_inc_gen]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;two_inc_gen\u0026rdquo;}, the distribution of agreement with the question \u0026ldquo;Both men and women should contribute to household income. The data for the total sample size of $1,267$ is shown below. Can we conclude at the $% \\alpha =0.05$ level that gender and agreement on the two-income question are dependent? To help organize your answer, fill out the \u0026ldquo;Expected\u0026rdquo; table.\n","date":1595949567,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595949567,"objectID":"fea5a80ab5849214177ca2ae553527c3","permalink":"/courses/bana3363/14-chi-squared-test-of-independence/","publishdate":"2020-07-28T15:19:27.628999Z","relpermalink":"/courses/bana3363/14-chi-squared-test-of-independence/","section":"courses","summary":"Another Example of Goodness-of-Fit We begin with another example of the chi-squared goodness-of-fit test. The sources for the data are the National Highway Traffic Safety Administration and the Texas Department of Transportation.","tags":null,"title":"Chi Squared Test of Independence","type":"docs"},{"authors":null,"categories":null,"content":"Dependence Between Numeric Variables (Correlation) There are an infinite number of possible relationships between numeric variables. There could be quadratic relationships, cubic relationships, logarithmic relationships, and so forth. In much of statistics, the focus is on the linear relationship because it is generally easy to specify, it captures the notion of dependence in many real-world situations, and many other relationships can be at least approximated by a line in some small, focused area. The measure of linear relationship between two numeric variables is known as correlation.\nThe population correlation coefficient $\\rho ,$ pronounced \u0026ldquo;row,\u0026rdquo; describes the linear association between two random variables $X$ and $Y$. It is calculated as\n$$\\rho =\\frac{E(XY)-E(X)E(Y)}{\\sqrt{V(X)}\\sqrt{V(Y)}}$$\nwhere $E$ stands for \u0026ldquo;expected value\u0026rdquo; and $V$ stands for variance.\nThe correlation coefficient $\\rho$ is a population parameter, like $% \\mu$ and $p,$ and must therefore be estimated with data. This leads to the next definition.\nThe sample correlation coefficient $r$ is a random variable that describes the relationship between two sets of numeric data that are considered in pairs $(y_{1},x_{1}),(y_{2},x_{2}),\\ldots ,(y_{n},x_{n})$. It is given by\n$$r=\\frac{\\dsum_{i}x_{i}y_{i}-n(\\overline{x})(\\overline{y})}{(n-1)s_{x}s_{y}}$$\nwhere $s_{x\\text{ }}$ and $s_{y}$ are the standard deviations of $x$ and $y$ , respectively.\nThe following are facts about correlation:\n  Always lies between $-1$ and $1$ (inclusive)\n  The strength of correlation is measured by $|r|$, the absolute value of the correlation coefficient\n  Correlations of $1$ (or $-1$) indicate perfect positive (or negative) linear relationship\n  Correlation of $0$ indicates no linear relationship\n  Suppose we wanted to create a model of gasoline prices that factors in crude oil prices. The data would appear as shown in Figure [data]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;data\u0026rdquo;} in a spreadsheet program. Using Excel\u0026rsquo;s chart feature, we could make a scatter plot as shown in Figure [gas_scatter]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;gas_scatter\u0026rdquo;}. The correlation between crude oil price and gasoline price can be shown to be $r=0.987,$ which is an extremely strong positive linear relationship as can be seen.\nThe concept of correlation plays a key role in estimating the simple linear regression model, as we will see in the next section.\nIntroduction to Regression The broad set of techniques that fall under the \u0026ldquo;regression analysis\u0026rdquo; title drive most practical research studies, from drug trials in the pharmaceutical industry to new product test marketing. Businesses face tough competition in the information age. Not only must they navigate an increasingly fragmented customer base, manage their reputations through traditional and social media channels, innovate with products and services at an increasing rate, and negotiate with many suppliers and distributors, but they must also make short- and long-term decisions about personnel, production, new markets to enter, and strategies to increase (or at least maintain) market share against competitors. Sites such as Kickstarter 1 and Fundable2 specialize in a type of online venture capital process called \u0026ldquo;crowdfunding,\u0026rdquo; where ordinary individuals can contribute to a project in exchange for a variety of rewards, from a simple acknowledgement as a supporter, to a functioning prototype of the company\u0026rsquo;s product, to a percentage of sales. You will no doubt hear a good deal more on this topic in your marketing and management classes.\nManagers must be able to make decisions that incorporate a number of factors. Increasingly, these decisions have been influenced by mathematical modeling. A new field called business analytics3 has arisen that seeks to use advanced mathematics and large data sets to discover patterns and relationships among consumer choices that can help optimize product and service delivery. If you have ever wondered why grocery stores have rewards cards, the answer is simple: it allows the store to track your purchases in order to present you with customized deals and to understand how your purchases relate to those of other customers. If you use Netflix to stream movies, you should know that everything you do\u0026ndash;the programs you search for, the number of searches you perform, the time you spend browsing your recommendations, the device you are using to watch the program, the genre of the programs you view most often, and more\u0026ndash;is being monitored4. The data are fed into complex models designed by mathematicians and statisticians, and the patterns extracted from the data imply that, for instance, Hot Tub Time Machine and Les Miserables might appeal to the same user.\nMathematical models can be divided into two three broad classes: deterministic, probabilistic, and regression models. The last type is actually a blend of the first two. Let\u0026rsquo;s define these terms.\nA deterministic model is a mathematical equation for which, if a set of inputs $\\mathbf{x}$ are known, the output $y$ is known with certainty.\nDeterministic models are what we are most familiar with in elementary mathematics. They are why mathematics is seen as \u0026ldquo;objective,\u0026rdquo; in the sense that there is often only one \u0026ldquo;right answer\u0026rdquo; to a problem.¬†One simple example of a deterministic model that you should be familiar with is the formula for future value:$\\ FV=P(1+r/n)^{nt}$. This formula is deterministic because, given a present value $P,$ an interest rate $r$, the compounding frequency $n$, and the investment period $t$, we can calculate what the future value of the investment will be. That is, given $P,r,n,$ and $t,$ $FV$ is known with certainty. Another example of a deterministic model is the formula for a company\u0026rsquo;s total cost: $T=F+Vq$, where $F$ is a company\u0026rsquo;s fixed cost, $V$ is the variable cost per unit, and $q$ is the quantity produced. Given $F$, $V$, and $q$, we can compute the company\u0026rsquo;s total cost. Remembering the formula for distance ($D=st,$ where $s=speed),$ the time it takes to drive $45$ miles to campus, if you drive $s$ miles per hour is $t=% \\frac{45}{s}$.\nThese models are simple and leave much room for improvement. For instance, the future value formula assumes the interest rate remains fixed over time, which will rarely be the case. The total cost equation, meanwhile, ignores the fact that, for some level of production $q^{\\ast },$ the company will need to expand and change its fixed costs to $F^{\\ast }$ to keep up. More sophisticated models could be built to account for these factors. However, even when we incorporate these other variables, it is unlikely that we will be able to perfectly, to an infinite number of decimal places, the total cost of making a product or the future value of an investment.\nVariability is a basic characteristic of the natural world. Every time you drive $45$ miles to get to campus, it takes you a different amount of time to arrive here because your speed fluctuates by small amounts over the course of your trip. Everyone lives to a different age. People spend different amounts of money on each shopping trip. Differing amounts of soda are in a can that states that it contains $12$ ounces. With deterministic models, variability is ignored, so they are technically wrong.\nA probabilistic model assumes that the values of a variable arise from a random sample from a probability distribution (for example, the normal distribution).\nThese are the models that we have been working with all semester! For instance, if we were to calculate a confidence interval for the population mean amount of money that Americans give to charity each year, we¬†might have said:¬†\u0026ldquo;Assume you have a random sample of $n=1000$ people and their average yearly charitable contribution was $\\overline{y}=1000$ and $s=300.\u0026ldquo;$ The histogram of the charitable contributions might appear as in Figure [charity_cont]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;charity_cont\u0026rdquo;}.\nIt is undeniable that the amount of money that people contribute to charity varies from person to person and even from time to time for the same person over different years. Assuming that \u0026ldquo;charitable contributions\u0026rdquo; is a random variable is a useful first approach to understanding the natural world, but we can do much better by bringing in additional information.\nIt is sensible that charitable contributions depend on many factors, one of which is a person\u0026rsquo;s income (other variables might be education level, political affiliation, gender, etc.). We can model the idea that charitable contribution is a random variable, but one that is influenced by another (possibly nonrandom) variable. We can get at this idea by making a reasonable assumption:\nfor any given income, there is an entire distribution of possible \u0026gt; charitable contributions that a person with that income could choose \u0026gt; to make .\nIt would make sense that, as income increases, at least the mean contribution should increase, but how?\nOne possibility is that the mean changes linearly with income. That is, if income is $x$ dollars per year, the average charitable contribution is $% \\beta_{0}+\\beta_{1}x,$ where $\\beta_{0}$ (read¬†as \u0026ldquo;beta zero\u0026rdquo;) is the intercept of the line and $\\beta_{1}$ (read as \u0026ldquo;beta one\u0026rdquo;) is the slope. These two $\\beta$ values are **population parameters**, like $\\mu$ and $p$, that we can use data to estimate. This is shown in Figure [reg_model]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;reg_model\u0026rdquo;}. The linear part that specifies how the mean changes with increasing income is **deterministic** (if we know $x$, $\\beta_{0},$ and $\\beta_{1},$ we know the mean with certainty), but the actual value is random, modeled as a draw from a normal distribution with mean $\\beta_{0}+\\beta_{1}x$. Now we have the next definition.\nA regression model is a blend of deterministic and probabilistic models that specifies how the distribution of some random variable $Y$ changes for a function of other (possibly nonrandom) variables $% f(x_{1},x_{2},\\ldots ,x_{p})$.\nHow do we figure out what that function $f(x_{1},x_{2},\\ldots ,x_{p})$ is? The best way is to simply graph the $\\ Y$ variable with each of the $x$ variables using a **scatter plot**, a graph that plots each pair of points $(y_{i},x_{ij})$ in the ordinary $x-y$ plane. Looking at the scatter, you can see if a line might fit the data fairly well, or if another function such as a quadratic might work better. The simple linear regression model, which we now define, claims that the distribution of $Y$ depends only on one other variable, $x$.\nFor a random sample of bivariate data $(Y_{1},x_{1}),(Y_{2},x_{2}),\\ldots ,(Y_{n},x_{n}),$ where the $Y_{i}$ are random variables and the $x_{i}$ are constants, the **simple linear regression model** relating $Y$ to $x$ is given by\n$$\\begin{aligned} Y_{i} \u0026amp;=\u0026amp;\\beta_{0}+\\beta_{1}x_{i}+\\varepsilon_{i},\\text{ }i=1,2,\\ldots n% \\text{ [sample form]} \\label{simple_model_eq} \\ Y \u0026amp;=\u0026amp;\\beta_{0}+\\beta_{1}x+\\varepsilon \\text{ [population form]} \\label{pop_version}\\end{aligned}$$\nwhere $\\beta_{0}$*¬†(read as \u0026ldquo;beta zero\u0026rdquo;) and* $\\beta_{1}$ *¬†(read as \u0026ldquo;beta one\u0026rdquo;) are fixed (but unknown) parameters and* $% \\varepsilon_{i}$*¬†(read as \u0026ldquo;epsilon\u0026rdquo;) are independent normally distributed random variables with mean* $0$*¬†and standard deviation* $\\sigma$*¬†that are uncorrelated with* $x$.\nIn Figure [reg_model]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;reg_model\u0026rdquo;}, the only $x$ variable is income, but there are many other factors that affect charity that we could add to the model. What this equation says is that the actual value of $Y$ that we observe is made up of a deterministic (fixed and known) $x$ and a random (unknown) component $\\varepsilon$. The $\\varepsilon$ value is made up of the countless other factors that influence $Y$ that we have not explicitly included as an $x$ variable in the model. We require $\\varepsilon$ to be in the model because we can never know $Y$ with absolute certainty. However, we hope that the relationship between $Y$ and $x$ is strong enough to account for a good bit of the change in $Y,$ because we can quantify this change by estimating $% \\beta_{0}$ and $\\beta_{1}$ (more on this below).\nThe implication of equation [pop_version]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;pop_version\u0026rdquo;} is that the mean of $Y$ is $% E(Y|x)=$ $\\beta_{0}+\\beta_{1}x$ because, by the rules of expected value¬†(see Handout $1$),\n$$\\begin{aligned} E(Y|x) \u0026amp;=\u0026amp;E(\\beta_{0}+\\beta_{1}x+\\varepsilon ) \\ \u0026amp;=\u0026amp;\\beta_{0}+\\beta_{1}x+E(\\varepsilon )\\text{ [since }\\beta_{0}+\\beta_{1}x\\text{ is a constant]} \\ \u0026amp;=\u0026amp;\\beta_{0}+\\beta_{1}x\\text{ [since we assume }E(\\varepsilon )=0]\\end{aligned}$$\nWe put the $\u0026quot;|x\u0026quot;$ part in the expected value to emphasize that the mean of $% Y$ depends on $x$. A shorthand notation for this is $\\widehat{Y}$, read as $% \u0026ldquo;Y$ hat.\u0026rdquo; We also have the following important fact:\nBecause $\\varepsilon$ is assumed to have a normal distribution, $Y$ also has a normal distribution, but with mean $\\beta_{0}+\\beta_{1}x.\\label% {simple_ols_theorem}$\nWe can interpret $\\beta_{0}$ and $\\beta_{1}$ as follows:\n  $\\beta_{0}$ is the mean of $Y$ when $x=0$.\n  $\\beta_{1}$ is the increase in the mean of $Y$ for a one-unit increase in $x$.\n  As stated above, the values of $\\beta_{0}$ and $\\beta_{1}$ are parameters **¬†**that we must use data to estimate. It is not immediately obvious what the \u0026ldquo;best\u0026rdquo; way to do this is, but a reasonable choice would be to find the estimated values $b_{0}$ and $b_{1}$ such that the line $b_{0}+b_{1}x$ goes through most of the data points. Consider Figure [gas_scatter]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;gas_scatter\u0026rdquo;}. It is fairly easy to imagine drawing a line through the scatterplot in such a way that the data fall close to the line. It\u0026rsquo;s a bit more challenging to do this with a data set such as the one shown in Figure [c02]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;c02\u0026rdquo;}, but it still appears as if a line might be an okay fit.\nNotice that, even though we know that gasoline is directly related to crude oil through refining, we cannot perfectly predict gasoline prices using crude oil prices. However, using crude oil prices will let us make a much better prediction than if we just worked with the gasoline data alone. Figure [gasonoly]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;gasonoly\u0026rdquo;} displays a histogram of only the gas prices, which would (falsely) lead us to believe that our best prediction of gasoline price would be in the $$0.50-$1$ range, which is clearly not realistic.\nOne way to fit a line is to choose two points, say, $(x_{1},y_{1})$ and $% (x_{2},y_{2}),$ out of all the points in the scatterplot and find the slope and $y$ intercept. But which points would you pick? Your choice of two points might not match mine, and ours might still be different from a third person\u0026rsquo;s. Clearly, we need a more objective way of fitting the line. The most common method used in practice is known as the**¬†method of ordinary least squares (OLS)**, which we now define.\nThe ordinary least squares (OLS) estimates, $b_{0}$ and $b_{1},$ estimate $\\beta_{0}$ and $\\beta_{1}$ and are chosen such that the \u0026ldquo;sum of squared errors\u0026rdquo;, SSE, is made as small as possible. That is, we choose $b_{0}$ and $b_{1}$ so that\n$$SSE=\\sum (y_{i}-b_{0}-b_{1}x_{i})^{2}$$\nis minimized.\nIf you know about multivariable calculus, the approach is clear: find the partial derivatives of $SSE$ with respect to $b_{0}$ and $b_{1},$ set them equal to $0$, and solve for $b_{0}$ and $b_{1}$. In this course, we will simply state the solution as a theorem without proof.\nThe values of $b_{0}$ and $b_{1}$ that minimize $\\sum (y_{i}-b_{0}-b_{1}x_{i})^{2}$ are $b_{1}=r\\frac{s_{y}}{s_{x}}$ and $b_{0}=% \\overline{y}-b_{1}(\\overline{x}),$ where $s_{y}$ and $s_{x}$ are the standard deviations of the observed data ${y_{i}}$ and ${x_{i}}$, respectively, and $r$ is the sample correlation coefficient between $% {y_{i}}$ and ${x_{i}}$.\nOnce we have estimated $b_{0}$ and $b_{1},$ we can estimate $y$ using the equation\n$$\\widehat{y}=b_{0}+b_{1}x$$\nWe can simply plug in any value of $x$ and we get an estimate of $y$. What happened to the error term $\\varepsilon ?$ Since the error term is not known, we simply use its mean of $0$ to estimate it. What does our estimate of $y$ represent? $\\widehat{y\\text{ }}$**¬†is the estimated mean of the random variable** $Y$**¬†at the value of** $x!$ Thus, through equation [simple_model_eq]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;simple_model_eq\u0026rdquo;}, we have now defined a model that allows the distribution of $Y$ to change with $x$.\nThe scatter plot of gasoline and oil prices with the least-square line is shown in Figure [scatter_and_reg]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;scatter_and_reg\u0026rdquo;}. The plot was made using Excel\u0026rsquo;s scatter plot and \u0026ldquo;Add trendline\u0026rdquo; options. The estimated equation is\n$$\\widehat{y}=0.0268x+0.0315$$\nwhere $x$ is the price of oil per barrel. From this equation referring to [simple_ols_theorem]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;simple_ols_theorem\u0026rdquo;}, we estimate that for each $$1$ increase in the price of oil per barrel, the average price of gasoline increases by about $$0.0268,$ or about three cents. Further, looking at $% b_{0},$ we can say (rather pointlessly) that the average price of gasoline when oil is $$0$ per barrel is $$0.0315$. Of course, oil will always cost something per barrel, so $b_{0}$ does not have a realistic interpretation here.\nUsing the Simple Linear Regression Model After the model has been fit, the next logical question is whether the model is useful or not. One way of assessing this is to determine if $x$ has significant predictive ability for $Y$. Since the effect of $x$ on $Y$ is measured by the unknown parameter $\\beta_{1},$ the natural approach is to conduct a hypothesis test or construct a confidence interval for $\\beta_{1}$ . First, we will address the confidence interval, since we can essentially define the test by determining whether a specified null hypothesis value of $% \\beta_{1}$ lies in the interval or not, much like we have done already. Before we can define the interval, we have to define a few terms related to the estimate $b_{1}$, which is what we use to construct the interval.\nThe estimated value of $\\beta_{1},$ known as $b_{1}$, is a random variable with a mean equal to $\\beta_{1}$ and a standard deviation (called a **standard error)** of $\\sqrt{\\frac{\\sigma ^{2}}{\\dsum (x_{i}-\\overline{x})^{2}}% }$ . Since we rarely assume $\\sigma ^{2}$ is known, we set $s.e.(b_{1})=% \\sqrt{\\frac{SSE/(n-2)}{\\dsum (x_{i}-\\overline{x})^{2}}}=\\sqrt{\\frac{SSE/(n-2)% }{(n-1)s_{x}^{2}}},$ where $s_{x}$ is the standard deviation of the $x$ data.\nA consequence of this fact leads to the definition of the $(1-\\alpha )100%$ confidence interval for $\\beta_{1}:$\nA $(1-\\alpha)100%$ confidence interval for $\\beta_{1}$ is given by\n$$b_{1}\\pm t_{\\alpha /2,n-2}\\sqrt{\\frac{SSE/(n-2)}{(n-1)s_{x}^{2}}}$$\nwhere $t_{n-2,\\alpha /2}$¬†is the value of Student\u0026rsquo;s $t$ distribution with* $n-2$¬†degrees of freedom and* $\\alpha /2$¬†probability to the right.\nWe use $n-2$ because we had to estimate two values, $b_{0}$ and $b_{1},$ using the data. The hypothesis testing procedure follows a familiar pattern:\n Specify the hypotheses  The parameter of interest is $\\beta_{1},$ so the hypotheses are statements about its value.\n$$\\begin{aligned} \\text{(a) }H_{0} \u0026amp;:\u0026amp;\\beta_{1}\\leq c\\text{ vs. }H_{0}:\\beta_{1}\u0026gt;c \\ \\text{(b) }H_{0} \u0026amp;:\u0026amp;\\beta_{1}\\geq c\\text{ vs. }H_{0}:\\beta_{1}\u0026lt;c \\ \\text{(c) }H_{0} \u0026amp;:\u0026amp;\\beta_{1}=c\\text{ vs. }H_{0}:\\beta_{1}\\neq c\\end{aligned}$$\nIn practice, $c$ is usually $0,$ but it could be anything. In regression programs such as Excel, the hypothesis tested by default is\n$$H_{0}:\\beta_{1}=0\\text{ vs. }H_{0}:\\beta_{1}\\neq 0$$\n Calculate the test statistic.  The test statistic is a $t$ statistic:\n$$t=\\frac{b_{1}-c}{s.e.(b_{1})}$$\n Determine the rejection region (or compute the $p$-value).  The test statistic is a Student\u0026rsquo;s $t$ statistic, so the critical values that mark the rejection regions are as follows:\n$$\\begin{aligned} \\text{(a)}{\\text{ }t_{n-2} \u0026amp;:\u0026amp;t_{n-2}\u0026gt;\\text{ }t_{n-2,\\alpha }} \\ \\text{(b) }{\\text{ }t_{n-2} \u0026amp;:\u0026amp;t_{n-2}\u0026lt;-\\text{ }t_{n-2,\\alpha }} \\ \\text{(c) }{\\text{ }t_{n-2} \u0026amp;:\u0026amp;|t_{n-2}|\u0026gt;\\text{ }t_{n-2,\\alpha /2}\\end{aligned}$$\nAlternatively, with software we can calculate the $p$-value as follows:\n$$\\begin{aligned} \\text{(a) }p \u0026amp;=\u0026amp;P(t_{n-2}\u0026gt;t) \\ \\text{(b) }p \u0026amp;=\u0026amp;P(t_{n-2}\u0026lt;t) \\ \\text{(c) }p \u0026amp;=\u0026amp;2\\min [P(t_{n-2}\u0026lt;t_{0}),P(t_{n-2}\u0026gt;t)]\\end{aligned}$$\n Make a conclusion  If the $p$-value $\\leq \\alpha ,$ we reject $H_{0}$. If the $p$-value $% \u0026gt;\\alpha ,$ we fail to reject $H_{0}$. Equivalently, if $t$ lies within the rejection region, we reject $H_{0}$. If $t$ lies outside the rejection region, we fail to reject $H_{0}$.\nAs was the case when we tested means from one and two populations, the following theorem is true:\nIf the $(1-\\alpha )100%$ confidence interval for $\\beta_{1}$ **contains** $c$, then a test of $H_{0}:\\beta_{1}=c$ vs. $H_{0}:\\beta_{1}\\neq c$ at the $\\alpha$ level will **fail to reject** $H_{0}$.\nIn practice, software programs such as Excel are used to fit regression models. Excel has a built-in tool to do this in the same location as the ANOVA program. The output even looks similar. As an example, let\u0026rsquo;s fit a simple linear regression model to the crude oil/gasoline price data which are graphed in Figure [gas_scatter]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;gas_scatter\u0026rdquo;}. Fitting the model using Excel gives output shown below in Figure [reg_ouytpu]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;reg_ouytpu\u0026rdquo;}. Let\u0026rsquo;s examine the output.\nUsing the output above, answer the following questions:\na). What is the estimated model?\nb). Interpret the coefficients $b_{0}$¬†and $b_{0}$ in context.\nc). What is the $95%$¬†confidence interval for $\\beta_{1}?$\nd). What are the hypotheses that the $\\mathit{p}$-value refers to?\ne). Based on the $p$-value and on the confidence interval, what conclusion can you make about the relationship between crude oil price and gasoline price?\nWhat is the relationship between the square footage of a house and its selling price? Figure [housescatter]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;housescatter\u0026rdquo;}¬†is a scatterplot of over $2,000$ selling prices of houses in Ames, Iowa, and their associated square footages. This plot was made using a free statistics program called R 5. The regression output from Excel is shown in Figure\na). What is the estimated model?\nb). Interpret the coefficients $b_{0}$¬†and $b_{0}$ in context.\nc). Calculate the $95%$¬†confidence interval for $\\beta_{1}$.\nd). What are the hypotheses that the $\\mathit{p}$-value refers to?\ne). Based on the $p$-value and on the confidence interval, what conclusion can you make about the relationship between the selling price of a house and its square footage?\nDoes the amount of education that one\u0026rsquo;s father has influence the amount of education his children have? A sample of $20$ people were asked to report the number of years of formal schooling they had along with the number of years their fathers had. The data are shown in the table below.\n Dad_Schooling Child_Schooling\n16 18\n15 15\n12 14\n12 20\n12 13\n12 16\n12 12\n12 14\n12 16\n14 16\n12 20\n12 14\n20 17\n16 13\n9 15\n11 12\n12 12\n10 11\n16 18\n8 10\n a). Use Excel to fit the simple linear regression model to the data above.\nb). Interpret the coefficients $b_{0}$¬†and $b_{0}$ in context.\nc). Calculate a $90%$¬†confidence interval for $\\beta_{1}$.\nd). What are the hypotheses that the $\\mathit{p}$-value refers to?\ne). Based on the $p$-value and on the confidence interval, what conclusion can you make about the relationship between a child\u0026rsquo;s education level and his or her father\u0026rsquo;s education level?\n   www.kickstarter.com/ \u0026#x21a9;\u0026#xfe0e;\n  www.fundable.com/ \u0026#x21a9;\u0026#xfe0e;\n http://www.indeed.com/jobs?q=business+analytics\u0026amp;l \u0026#x21a9;\u0026#xfe0e;\n http://www.wired.com/underwire/2013/08/qq_netflix-algorithm/?mbid=social10558894 \u0026#x21a9;\u0026#xfe0e;\n Available here: www.r-project.org \u0026#x21a9;\u0026#xfe0e;\n   ","date":1595949567,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595949567,"objectID":"34c2e34a9d3c8e37db56c2779120be1c","permalink":"/courses/bana3363/15-intro-to-regression/","publishdate":"2020-07-28T15:19:27.628999Z","relpermalink":"/courses/bana3363/15-intro-to-regression/","section":"courses","summary":"Dependence Between Numeric Variables (Correlation) There are an infinite number of possible relationships between numeric variables. There could be quadratic relationships, cubic relationships, logarithmic relationships, and so forth. In much of statistics, the focus is on the linear relationship because it is generally easy to specify, it captures the notion of dependence in many real-world situations, and many other relationships can be at least approximated by a line in some small, focused area.","tags":null,"title":"Intro to Regression","type":"docs"},{"authors":null,"categories":null,"content":"Evaluating the Simple Linear Regression Model The essential components of regression analysis are the following:\n  We assume that two variables, $x$ and $Y,$ are somehow related.\n  Specifically, we assume the probability distribution of $Y$ (which is called the response or dependent variable) changes as $x$ (which is called the predictor or independent variable) changes.\n  Specifically, we assume that the mean (or expected value) of $Y,$ denoted as $E(Y),$ changes as $x$ changes. To indicate this we write $E(Y|x)$ read as \u0026ldquo;the expected value of $Y$ given $x.\u0026ldquo;$\n  We can also indicate $E(Y|x)$ using the notation $\\widehat{Y},$ read as $\u0026quot;y$ hat.\u0026rdquo;\n  Specifically, with simple regression, we assume that $Y=\\beta {0}+\\beta{1}x+\\varepsilon$, where $\\varepsilon$ has a normal distribution with mean $0$ and standard deviation $\\sigma $.\n  Given a set of paired data points $(Y_{1},x_{1}),$ $% (Y_{2},x_{2}),\\ldots ,(Y_{n},x_{n}),$ we can fit the simple linear regression model $\\widehat{y}=b_{0}+b_{1}x$ by finding the values $b_{0}$ and $b_{1}$ that makes the \u0026ldquo;sum of squared errors\u0026rdquo; $SSE=$ $\\sum (y_{i}-(b_{0}+b_{1}x_{i}))^{2}$ as small as possible. As you will recall, this is the **OLS (ordinary least squares) criterion.**\n  Once the model is fit, we should examine how good the fit is. Several items should be checked, but they fall under three general categories: $1)$. Does the model make sense? $2)$. Are the assumptions satisfied? and $3)$. Is the model practically useful? Statisticians have devised many ways to address these questions, some qualitative, some quantitative. Many of the quantitative methods are focused on examining $(2)$, since the validity of the predictions from the model rest upon the assumptions being at least reasonably satisfied. However, even if the assumptions are badly violated, the model could be made to be useful by making adjustments to the model-fitting process.\nAssessing the Fit of the Model Whether the fitted model $\\widehat{y}=b_{0}+b_{1}x$ makes sense or not depends on your knowledge of the subject you are trying to learn about, and specifically on your idea of how the two variables should be related. For example, it seems sensible that as income increases, the total amount of charitable contributions should increase as well. The relationship might be linear or non-linear. Two possible relationships are shown in Figures [reg_model]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;reg_model\u0026rdquo;} and [quadreg]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;quadreg\u0026rdquo;} below. However, other relationships are possible. A recent study has shown that the very wealthy are less generous in that they give a smaller percentage of their income to charity1.\nThe most important way to determine the relationship between variables is to simply graph the data. Scatter plots such as those in Figures [gas_scatter]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;gas_scatter\u0026rdquo;} and [c02]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;c02\u0026rdquo;} confirm what we suspect:¬†higher oil prices lead to higher gas prices, and the more cars on the road, the higher the CO$_{2}$ output. But what about the relationship between the square footage of a house and its selling price? Figure [housescatter]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;housescatter\u0026rdquo;}¬†is a scatterplot of over $2,000$ selling prices of houses in Ames, Iowa, and their associated square footage. Is the relationship linear or quadratic? In this case, it is a little harder to see.\n\\FRAME{ftbphFU}{3.8017in}{2.9698in}{0pt}{\\Qcb{One possible relationship between charitable contributions and income is a linear relationship, where as income increases, average contributions increase at a constant rate.}}{% \\Qlb{reg_model}}{Figure}{\\special{language \u0026ldquo;Scientific Word\u0026rdquo;;type \u0026ldquo;GRAPHIC\u0026rdquo;;maintain-aspect-ratio TRUE;display \u0026ldquo;USEDEF\u0026rdquo;;valid_file \u0026ldquo;T\u0026rdquo;;width 3.8017in;height 2.9698in;depth 0pt;original-width 10.1149in;original-height 7.8854in;cropleft \u0026ldquo;0\u0026rdquo;;croptop \u0026ldquo;1\u0026rdquo;;cropright \u0026ldquo;1\u0026rdquo;;cropbottom \u0026ldquo;0\u0026rdquo;;tempfilename \u0026lsquo;N7HNF80F.wmf\u0026rsquo;;tempfile-properties \u0026ldquo;XPR\u0026rdquo;;}}\n\\FRAME{ftbphFU}{3.8242in}{2.9032in}{0pt}{\\Qcb{Another possible relationship between charitable contributions and income is a quadratic relationship, where as income increases, average contributions increase at an increasing rate.}}{\\Qlb{quadreg}}{Figure}{\\special{language \u0026ldquo;Scientific Word\u0026rdquo;;type \u0026ldquo;GRAPHIC\u0026rdquo;;maintain-aspect-ratio TRUE;display \u0026ldquo;USEDEF\u0026rdquo;;valid_file \u0026ldquo;T\u0026rdquo;;width 3.8242in;height 2.9032in;depth 0pt;original-width 10.4063in;original-height 7.8854in;cropleft \u0026ldquo;0\u0026rdquo;;croptop \u0026ldquo;1\u0026rdquo;;cropright \u0026ldquo;1\u0026rdquo;;cropbottom \u0026ldquo;0\u0026rdquo;;tempfilename \u0026lsquo;N7HNF80G.wmf\u0026rsquo;;tempfile-properties \u0026ldquo;XPR\u0026rdquo;;}}\n\\FRAME{ftbphFU}{4.1096in}{2.2667in}{0pt}{\\Qcb{Scatterplot of gasoline prices ($y$ axis) and crude oil prices ($x$ axis).}}{\\Qlb{gas_scatter}}{Figure}{% \\special{language \u0026ldquo;Scientific Word\u0026rdquo;;type \u0026ldquo;GRAPHIC\u0026rdquo;;maintain-aspect-ratio TRUE;display \u0026ldquo;USEDEF\u0026rdquo;;valid_file \u0026ldquo;T\u0026rdquo;;width 4.1096in;height 2.2667in;depth 0pt;original-width 6.9254in;original-height 3.8043in;cropleft \u0026ldquo;0\u0026rdquo;;croptop \u0026ldquo;1\u0026rdquo;;cropright \u0026ldquo;1\u0026rdquo;;cropbottom \u0026ldquo;0\u0026rdquo;;tempfilename \u0026lsquo;N7HNF80H.wmf\u0026rsquo;;tempfile-properties \u0026ldquo;XPR\u0026rdquo;;}}\n\\FRAME{ftbphFU}{4.1831in}{2.2416in}{0pt}{\\Qcb{CO$_{2}$ output and number of motor vehicles per $1000$ people.}}{\\Qlb{c02}}{Figure}{\\special{language \u0026ldquo;Scientific Word\u0026rdquo;;type \u0026ldquo;GRAPHIC\u0026rdquo;;maintain-aspect-ratio TRUE;display \u0026ldquo;USEDEF\u0026rdquo;;valid_file \u0026ldquo;T\u0026rdquo;;width 4.1831in;height 2.2416in;depth 0pt;original-width 7.3232in;original-height 3.9064in;cropleft \u0026ldquo;0\u0026rdquo;;croptop \u0026ldquo;1\u0026rdquo;;cropright \u0026ldquo;1\u0026rdquo;;cropbottom \u0026ldquo;0\u0026rdquo;;tempfilename \u0026lsquo;N7HNF80I.wmf\u0026rsquo;;tempfile-properties \u0026ldquo;XPR\u0026rdquo;;}} \\FRAME{ftbphFU}{3.9401in}{2.9265in}{0pt}{\\Qcb{Scatterplot of sales prices of houses in Ames, Iowa, and total square footage.}}{\\Qlb{housescatter}}{Figure% }{\\special{language \u0026ldquo;Scientific Word\u0026rdquo;;type \u0026ldquo;GRAPHIC\u0026rdquo;;maintain-aspect-ratio TRUE;display \u0026ldquo;USEDEF\u0026rdquo;;valid_file \u0026ldquo;T\u0026rdquo;;width 3.9401in;height 2.9265in;depth 0pt;original-width 6.6565in;original-height 4.9372in;cropleft \u0026ldquo;0\u0026rdquo;;croptop \u0026ldquo;1\u0026rdquo;;cropright \u0026ldquo;1\u0026rdquo;;cropbottom \u0026ldquo;0\u0026rdquo;;tempfilename \u0026lsquo;N7HNF80J.wmf\u0026rsquo;;tempfile-properties \u0026ldquo;XPR\u0026rdquo;;}}\nOne way of measuring the fit of a model is known as $R^{2},$ read as $\u0026quot;R$ square,\u0026rdquo; which we now define.\n$\\mathbf{R}^{2}$, also known as the coefficient of determination is the proportion of total variability in $Y$ that is accounted for, or \u0026ldquo;explained,\u0026rdquo; by the regression fit between $Y$ and $x$.\nThe idea of \u0026ldquo;explained variability\u0026rdquo; should remind you of the ANOVA discussion. ANOVA is, in fact, a type of regression model where the $x$ variable is a categorical rather than a numeric variable. Remember that the fundamental assumption of statistics is that variability exists in the world. It is inescapable. So the question is not whether we can eliminate variability,*¬†*but rather*¬†how we can account for it.* In the three data sets plotted above, we clearly see variability in the $Y$ variable. Some of that variability can potentially be \u0026ldquo;explained\u0026rdquo; by a regression line fit to the data. The rest we regard as \u0026ldquo;unexplained.\u0026rdquo;\nFigure [rsq]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;rsq\u0026rdquo;} shows the idea. Panel (a) shows that the total variability in the data can be measured by the differences between each actual observation and the sample mean $\\overline{y}$. We already know a measure for that:¬†the sample standard deviation $s$. However, the variability is really measured just by the numerator of that equation, which we defined as $% \u0026ldquo;SSTOT,\u0026ldquo;$ which stands for \u0026ldquo;total sum of squares.\u0026rdquo;¬†This is defined as\n$$SSTOT=\\sum (y_{i}-\\overline{y})^{2}$$\n\\FRAME{ftbphFU}{6.6712in}{2.7735in}{0pt}{\\Qcb{Decomposition of variability into \u0026ldquo;explained\u0026rdquo; and \u0026ldquo;unexplained\u0026rdquo; components.}}{\\Qlb{rsq}}{Figure}{\\special% {language \u0026ldquo;Scientific Word\u0026rdquo;;type \u0026ldquo;GRAPHIC\u0026rdquo;;maintain-aspect-ratio TRUE;display \u0026ldquo;USEDEF\u0026rdquo;;valid_file \u0026ldquo;T\u0026rdquo;;width 6.6712in;height 2.7735in;depth 0pt;original-width 7.2809in;original-height 3.0104in;cropleft \u0026ldquo;0\u0026rdquo;;croptop \u0026ldquo;1\u0026rdquo;;cropright \u0026ldquo;1\u0026rdquo;;cropbottom \u0026ldquo;0\u0026rdquo;;tempfilename \u0026lsquo;N7HNF80K.wmf\u0026rsquo;;tempfile-properties \u0026ldquo;XPR\u0026rdquo;;}}\nIn panel (c), we see that some of this variability can be \u0026ldquo;explained\u0026rdquo; by the regression line through the differences between the values $\\widehat{y_{i}}$ and the sample mean $\\overline{y}$. This \u0026ldquo;explained\u0026rdquo; variability is measured by a sum of squares as well, which we call $SSR$, which stands for \u0026ldquo;sum of squares for regression.\u0026rdquo; This is defined as:\n$$SSR=\\sum (\\widehat{y_{i}}-\\overline{y})^{2}$$\nThe remaining variability is accounted for by the differences in the actual values, $y_{i},$ and the predicted values $\\widehat{y_{i}}$. This is also a sum of squares, called , $SSE,$ which stands for \u0026ldquo;sum of squares for error.\u0026rdquo; It is defined as:\n$$SSE=\\sum (y_{i}-\\widehat{y_{i}})^{2}$$\nAs implied in the figure, the following relationship holds:\n$$SSTOT=SSR+SSE$$\nIf a \u0026ldquo;large\u0026rdquo; proportion of variability in the data can be accounted for by the model then we have evidence that the model is useful. The statistic that quantifies the proportion of variability accounted for by the model is known as $R^{2}$, which we now define.\n$R^{2},$ read as \u0026ldquo;r squared\u0026rdquo; and also known as the coefficient of determination, measures the proportion of total variability in the $Y$ data that is accounted for by the regression fit. The formula is\n$$R^{2}=\\frac{SSR}{SSTOT}=1-\\frac{SSE}{SSTOT}$$\nHere is a useful fact relating $R^{2}$ to correlation.\nIn the simple linear regression model $Y=\\beta_{0}+\\beta_{1}x+\\varepsilon ,$ $R^{2}$ is equal to the square of the correlation coefficient $r$. That is $R^{2}=r^{2}$.\nHow much variability is accounted for by the regression model is only one indication of its usefulness. It is possible to have a fairly large $R^{2}$ in a model that is not practically useful. A $95%$ confidence interval for $% \\beta_{1}$, for instance, could be too wide to make any reasonable judgment on the relationship between a unit increase in $x$ on the mean of $Y$. Another problem is that a high $R^{2}$ is possible when the underlying assumptions of the regression procedure are violated. Fortunately, in this case, adjustments to the model or transformations of the data are usually possible in order to make the assumptions more reasonably met. In the next section, we give an overview of some common regression assumption violations.\nViolations of Regression Assumptions The basic ways that a simple linear regression model can go wrong are 1). the true $E(Y|x)$ is not linear, but some other function; 2). The error terms $\\varepsilon_{i}$ are not independent of one another or of $x;$ 3). The error terms $\\varepsilon_{i}$ do not have the same variance $% V(\\varepsilon )=$ $\\sigma ^{2}$ for all values of $x$ ; 4). Several of these assumptions are violated. Let\u0026rsquo;s expand upon points¬†$(2)$ and $(3)$.\nThe error terms $\\varepsilon_{i}$ arise due to the imperfect fit between the regression line $\\beta_{0}+\\beta_{1}x$ and the actual value $Y$. For each value of $Y_{i}$ and $x_{i},$ there is an associated $\\varepsilon_{i}$ that \u0026ldquo;absorbs\u0026rdquo; the effects of all of the variables that we didn\u0026rsquo;t include in the model. One of the key assumptions is that the variance of the error term $V(\\varepsilon )$ *does not change* as $x$ changes. That is, $% V(\\varepsilon_{i})=V(\\varepsilon_{j})$ for any $i$ and $\\ j$. We further assume that the correlation between $\\varepsilon_{i}$ and $\\varepsilon_{j}$ is exactly $0,$ that is, the error in predicting $Y_{i}$ using $x_{i}$ is unrelated to the error in predicting $Y_{j}$ using $x_{j}$, for any $i$ and $% j$. If these assumptions about the $\\varepsilon_{i}$ are not satisfied, inferential statements we make using the fitted model¬†(for example, calculating a confidence interval or test for $\\beta_{1}$, or predicting a new observation of $Y)$ are not strictly valid, and may thus be highly misleading.\nThe most common method by which we evaluate the assumptions of the regression model is by examining what are called \u0026ldquo;residuals.\u0026rdquo; First, we need to define what a residual is.\nA regression residual $e_{i}$**¬†**is the difference between the actual value $y_{i}$ and the fitted value $\\widehat{y_{i}}$. For a fitted regression model, the residual is\n$$e_{i}=y_{i}-\\widehat{y_{i}}$$\nIn the case of the simple linear regression model, plotting the residuals by the $x$ variable is a natural choice. A scatterplot of $(e_{i},x_{i})$ is called a **residual plot**. Examining residual plots can help identify problems with the model fit. How should the residual plot look if all of the assumptions are reasonably met? The short answer is: *look for the absence of a pattern.* Figure [good_resid]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;good_resid\u0026rdquo;} illustrates this idea. Notice that the residuals seem to scatter around $0$ with no apparent pattern. We would say this model is a reasonably good fit to the data.\n\\FRAME{ftbphFU}{6.1315in}{3.0666in}{0pt}{\\Qcb{Residual plot for a model where the assumptions are reasonably satisfied. In this model, \u0026ldquo;time\u0026rdquo; is the $x$ variable.}}{\\Qlb{good_resid}}{Figure}{\\special{language \u0026ldquo;Scientific Word\u0026rdquo;;type \u0026ldquo;GRAPHIC\u0026rdquo;;maintain-aspect-ratio TRUE;display \u0026ldquo;USEDEF\u0026rdquo;;valid_file \u0026ldquo;T\u0026rdquo;;width 6.1315in;height 3.0666in;depth 0pt;original-width 9.6868in;original-height 4.8317in;cropleft \u0026ldquo;0\u0026rdquo;;croptop \u0026ldquo;1\u0026rdquo;;cropright \u0026ldquo;1\u0026rdquo;;cropbottom \u0026ldquo;0\u0026rdquo;;tempfilename \u0026lsquo;N7HREN0P.wmf\u0026rsquo;;tempfile-properties \u0026ldquo;XPR\u0026rdquo;;}}\nA violation of the assumption of constant standard deviation (known formally as heteroscedasticity) is shown in Figure [hetero]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;hetero\u0026rdquo;}. Notice that as the $x$ variable (\u0026ldquo;time\u0026rdquo; in this case) gets larger, the residuals begin to \u0026ldquo;fan out\u0026rdquo; in a megaphone pattern, indicating an increase in the variability. In practice, an analyst would employ a variable transformation to the data to correct for this. Discussion of how this transformation is accomplished is beyond the scope of this handout 2.\nAn example of a model in which the assumption that $E(Y|x)$ is linear is not met is shown next in Figure [misspesified]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;misspesified\u0026rdquo;}. Such a model is called a misspecified model, and many times such models will result in error terms that are correlated. Moreover, misspecified models are problematic in their own right simply because any predictions we get from using the model will be wrong because the wrong relationship was postulated between $Y$ and $% x$. In the example below, it can be shown that fitting a parabola $% E(Y|x)=\\beta_{0}+\\beta_{1}x+\\beta_{2}x^{2}$ rather than a line will lead to a residual plot that looks like Figure [good_resid]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;good_resid\u0026rdquo;} above.\n\\FRAME{ftbphFU}{6.0623in}{3.4108in}{0pt}{\\Qcb{Example of a model in which $% E(Y|x)$ was quadratic rather than linear.}}{\\Qlb{misspesified}}{Figure}{% \\special{language \u0026ldquo;Scientific Word\u0026rdquo;;type \u0026ldquo;GRAPHIC\u0026rdquo;;maintain-aspect-ratio TRUE;display \u0026ldquo;USEDEF\u0026rdquo;;valid_file \u0026ldquo;T\u0026rdquo;;width 6.0623in;height 3.4108in;depth 0pt;original-width 9.6522in;original-height 5.4146in;cropleft \u0026ldquo;0\u0026rdquo;;croptop \u0026ldquo;1\u0026rdquo;;cropright \u0026ldquo;1\u0026rdquo;;cropbottom \u0026ldquo;0\u0026rdquo;;tempfilename \u0026lsquo;N7HREO0R.wmf\u0026rsquo;;tempfile-properties \u0026ldquo;XPR\u0026rdquo;;}}\nFor many of the real-world data sets we will use, the assumptions of the simple model will be violated. For instance, Figure [crude_resid]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;crude_resid\u0026rdquo;} indicates that the standard deviation of the residual terms increases as the price of oil increases. The pattern is that of the \u0026ldquo;megaphone\u0026rdquo; we saw earlier. It turns out that for this model, the fitted values $\\widehat{y}%_{i}$ (i.e., the predictions of $y$ for each price per barrel of oil) don\u0026rsquo;t differ by much when the problem of non-constant variability is corrected¬†(this correction isn\u0026rsquo;t shown here and is beyond the scope of this handout.) Therefore, we will proceed in the examples as if the model assumptions are reasonably met, although in \u0026ldquo;real life,\u0026rdquo; as you should always do in any situation, you will need to check assumptions.\n\\FRAME{ftbphFU}{5.495in}{2.5728in}{0pt}{\\Qcb{Residual plot of the regression of the price of gasoline on the price of oil. This plot shows non-constant error standard deviation.}}{\\Qlb{crude_resid}}{Figure}{\\special{language \u0026ldquo;Scientific Word\u0026rdquo;;type \u0026ldquo;GRAPHIC\u0026rdquo;;maintain-aspect-ratio TRUE;display \u0026ldquo;USEDEF\u0026rdquo;;valid_file \u0026ldquo;T\u0026rdquo;;width 5.495in;height 2.5728in;depth 0pt;original-width 5.4371in;original-height 2.5313in;cropleft \u0026ldquo;0\u0026rdquo;;croptop \u0026ldquo;1\u0026rdquo;;cropright \u0026ldquo;1\u0026rdquo;;cropbottom \u0026ldquo;0\u0026rdquo;;tempfilename \u0026lsquo;N7HREO0S.wmf\u0026rsquo;;tempfile-properties \u0026ldquo;XPR\u0026rdquo;;}}\nOnce you have fit the model, there are two things you might want to do with it: make an inference about $E(Y|x)$ for a specific $x$ and predict a new value of $Y,$ call it $Y^{new}$. We will discuss these two issues in the next section.\nEstimation of Mean of Y and Prediction of New Y It turns out that estimating the mean of $Y$ for a specific value of $x$ and predicting a new value of $Y$ for a specific value of $x$ are done the same way: plug the specific value of $x$, call it $x_{p}$, into the fitted model. What differs is constructing the interval. In general, the confidence interval for $E(Y|x_{p})$ will be narrower than the **prediction interval** for the value of $Y^{new}$. This fact makes sense because estimating a specific value of a distribution should logically be less precise than estimating its mean.\nHere are the two formulas\nA $(1-\\alpha )100%$ confidence interval for $E(Y|x_{p})$ for the simple linear regression model is\n$$\\widehat{y}\\pm t_{n-2,\\alpha /2}\\sqrt{\\frac{SSE}{n-2}\\left[ \\frac{1}{n}+% \\frac{(x_{p}-\\overline{x})^{2}}{(n-1)s_{x}^{2}}\\right] }$$\nwhere $x_{p}$ is the specific value of $x$ for which we want to estimate the mean of $Y$.\nA $(1-\\alpha )100%$ prediction interval for a new observation $% Y^{new}$ at a specific value $x_{p}$, is\n$$\\widehat{y}\\pm t_{n-2,\\alpha /2}\\sqrt{\\frac{SSE}{n-2}\\left[ 1+\\frac{1}{n}+% \\frac{(x_{p}-\\overline{x})^{2}}{(n-1)s_{x}^{2}}\\right] }$$\nNotice that the only difference between the two is the addition of the $\u0026quot;1\u0026quot;$ in the prediction interval formula.\nLet\u0026rsquo;s work some examples to put all of the concepts of this handout into practice.\nFor the following values of SSR, SSE, and SSTOT, calculate $R^{2}$.\na). $SSE=159$, $SSTOT=309.3$\nb). $SSR=1682,$¬†$SSTOT=1982$\nc). $SSR=149,$¬†$SSE=592$\nThe fitted regression model relating motor vehicles per $1000$ people to $% CO_{2}$ output for a sample of countries using the data displayed in Figure [c02]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;c02\u0026rdquo;} is shown below$.$.The average number of motor vehicles per $1000$ people $\\overline{x}=260.1$ with a standard deviation $s_{x}=236$.\na). Show how $R^{2}$¬†was calculated.\nb). Interpret $R^{2}$¬†in the context of the problem.\nc). Compute a $99%$¬†confidence interval for the true mean CO$_{2}$ output in a country when it has $400$¬†motor vehicles per $1000$¬†people.\nd). Compute a $99%$¬†confidence interval for the true mean CO$_{2}$¬†output in a country when it has $400$¬†motor vehicles per $1000$¬†people.\n\\FRAME{ftbphFU}{5.9836in}{3.1445in}{0pt}{\\Qcb{Regression fit relating $% CO_{2}$ output $(y)$ to motor vehicles per $1000$ people $(x)$.}}{\\Qlb{% co2_reg}}{Figure}{\\special{language \u0026ldquo;Scientific Word\u0026rdquo;;type \u0026ldquo;GRAPHIC\u0026rdquo;;maintain-aspect-ratio TRUE;display \u0026ldquo;USEDEF\u0026rdquo;;valid_file \u0026ldquo;T\u0026rdquo;;width 5.9836in;height 3.1445in;depth 0pt;original-width 7.3025in;original-height 3.8233in;cropleft \u0026ldquo;0\u0026rdquo;;croptop \u0026ldquo;1\u0026rdquo;;cropright \u0026ldquo;1\u0026rdquo;;cropbottom \u0026ldquo;0\u0026rdquo;;tempfilename \u0026lsquo;N7LIL300.wmf\u0026rsquo;;tempfile-properties \u0026ldquo;XPR\u0026rdquo;;}}\nAn economist is studying the nature of street vendors in Mexico. She has gathered the following data on $15$ vendors:¬†age, number of hours worked per day, and annual earnings. Using Excel, fit a simple linear regression model to predict annual earnings using hours worked per day as the $x$ variable.\n   Earnings Age Hours     2841 29 12   1876 21 8   2934 62 10   1552 18 10   3065 40 11   3670 50 11   2005 65 5   3215 44 8   1930 17 8   2010 70 6   3111 20 9   2882 29 9   1683 15 5   1817 14 7   4066 33 12    a). Show how $R^{2}$¬†was calculated.\nb). Interpret $R^{2}$¬†in the context of the problem\nc). Compute a $95%$¬†confidence interval for the true mean earnings of a vendor who works $\\mathit{6}$¬†hours per day using the fact that the average work hours $\\overline{x}=$¬†$8.7$ with a standard deviation $s_{x}=2.3$.¬†d).Compute a $95%$¬†prediction interval for the earnings of a street vendor who works $\\mathit{6}$¬†hours per day.\nUse the output in Figure [house_example]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;house_example\u0026rdquo;} and the fact that the average age of a house is $36.4$ years with a standard deviation of $30.3$ years to answer the following questions.\na). Show how $R^{2}$¬†was calculated.\nb). Interpret $R^{2}$¬†in the context of the problem.\nc). Compute a $99%$¬†confidence interval for the average price of a $10$-year-old home.\nd).Compute a $99%$¬†prediction interval for the price of a $10$-year-old home.\n\\FRAME{ftbphFU}{5.6861in}{3.5405in}{0pt}{\\Qcb{Excel regression output for a model of selling price $(Y,$ in thousands of dollars$)$ to age $(x=Age)$.}}{% \\Qlb{house_example}}{Figure}{\\special{language \u0026ldquo;Scientific Word\u0026rdquo;;type \u0026ldquo;GRAPHIC\u0026rdquo;;maintain-aspect-ratio TRUE;display \u0026ldquo;USEDEF\u0026rdquo;;valid_file \u0026ldquo;T\u0026rdquo;;width 5.6861in;height 3.5405in;depth 0pt;original-width 6.1566in;original-height 3.8233in;cropleft \u0026ldquo;0\u0026rdquo;;croptop \u0026ldquo;1\u0026rdquo;;cropright \u0026ldquo;1\u0026rdquo;;cropbottom \u0026ldquo;0\u0026rdquo;;tempfilename \u0026lsquo;N7LLO700.wmf\u0026rsquo;;tempfile-properties \u0026ldquo;XPR\u0026rdquo;;}}\n  http://www.npr.org/2013/09/03/218627288/why-being-wealthy-doesnt-lead-to-more-giving \u0026#x21a9;\u0026#xfe0e;\n A common solution to the unequal variance problem is to perform a regression using $x$ and $y^{\\ast }=\\ln (y)$, the natural logarithm of $y$. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1595949567,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595949567,"objectID":"b39c050b02330b9ac68097ed7c18dc92","permalink":"/courses/bana3363/16-simple-linear-regression/","publishdate":"2020-07-28T15:19:27.628999Z","relpermalink":"/courses/bana3363/16-simple-linear-regression/","section":"courses","summary":"Evaluating the Simple Linear Regression Model The essential components of regression analysis are the following:\n  We assume that two variables, $x$ and $Y,$ are somehow related.\n  Specifically, we assume the probability distribution of $Y$ (which is called the response or dependent variable) changes as $x$ (which is called the predictor or independent variable) changes.","tags":null,"title":"Simple Linear Regression","type":"docs"},{"authors":null,"categories":null,"content":"Evaluating the Simple Linear Regression Model\u0026ndash;Part 1 The essential components of regression analysis are the following:\n  We assume that two variables, $x$ and $Y,$ are somehow related.\n  Specifically, we assume the probability distribution of $Y$ (which is called the response or dependent variable) changes as $x$ (which is called the predictor or independent variable) changes.\n  Specifically, we assume that the mean (or expected value) of $Y,$ denoted as $E(Y),$ changes as $x$ changes. To indicate this we write $E(Y|x)$ read as \u0026ldquo;the expected value of $Y$ given $x.\u0026ldquo;$\n  We can also indicate $E(Y|x)$ using the notation $\\widehat{Y},$ read as $\u0026quot;y$ hat.\u0026rdquo;\n  Specifically, with simple regression, we assume that $Y=\\beta_{0}+\\beta_{1}x+\\varepsilon$, where $\\varepsilon$ has a normal distribution with mean $0$ and standard deviation $\\sigma.$\n  Given a set of paired data points ${(Y_{i},x_{i})},$ we can fit the simple linear regression model $\\widehat{y}=b_{0}+b_{1}x$ by finding the values $b_{0}$ and $b_{1}$ that makes the \u0026ldquo;sum of squared errors\u0026rdquo; $SSE= \\sum (y_{i}-(b_{0}+b_{1}x_{i}))^{2}$ as small as possible. This is the **OLS (ordinary least squares) criterion.**\n  Once the model is fit, we should examine how good the fit is. Several items should be checked, but they fall under three general categories: $1)$. Does the model make sense? $2).$ Are the assumptions satisfied?, and $3).$ Is the model practically useful? Statisticians have devised many ways to address these questions, some qualitative, some quantitative. Many of the quantitative methods are focused on examining $(2)$, since the validity of the predictions from the model rest upon the assumptions being at least reasonably satisfied. However, even if the assumptions are badly violated, the model might be useful and sensible. In practice, then, adjustments to the model-fitting process would be performed to make $(2)$ more reasonable. We won\u0026rsquo;t be worried too much about adjusting anything at this point. We will focus on $(1)$ in this handout.\nWhether the fitted model $\\widehat{y}=b_{0}+b_{1}x$ makes sense or not depends on your knowledge of the subject you are trying to learn about, and specifically on your idea of how the two variables should be related. For example, it seems sensible that as income increases,. the total amount of charitable contributions should increase as well. The relationship might be linear or non-linear. Two possible relationships are shown in Figures [reg_model]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;reg_model\u0026rdquo;} and [quadreg]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;quadreg\u0026rdquo;} below. However, other relationships are possible. A recent study has shown that the very wealthy are less generous in that they give a smaller percentage of their income to charity.\nThe most important way to determine the relationship between variables is to simply graph the data. Scatterplots such as those in Figures [gas_scatter]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;gas_scatter\u0026rdquo;} and [c02]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;c02\u0026rdquo;} confirm what we suspect:¬†higher oil prices lead to higher gas prices, and the more cars on the road, the higher the CO$_{2}$ output. But what about the relationship between the square footage of a house and its selling price? Figure [housescatter]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;housescatter\u0026rdquo;}¬†is a scatterplot of over $2,000$ selling prices of houses in Ames, Iowa, and their associated square footage. Is the relationship linear or quadratic? In this case, it is a little harder to see.\nOne way of measuring the fit of a model is known as $R^{2},$ read as $\u0026quot;R$ square,\u0026rdquo; which we now define.\n$R^{2}$, also known as \u0026ldquo;the coefficient of determination\u0026rdquo; is the proportion of total variability in $Y$ that is accounted for, or \u0026ldquo;explained,\u0026rdquo; by the regression fit between $Y$ and $x.$\n\u0026ldquo;Accounting for variability\u0026rdquo; seems to be an odd concept. After all, shouldn\u0026rsquo;t we be aiming for no variability in the data? Remember that the fundamental assumption of statistics is that variability exists in the world. It is inescapable. So the question is not whether we can eliminate variability,*¬†but rather how we can model it.* In the three data sets plotted in the figures above, we clearly see variability in the $Y$ variable. Some of that variability can potentially be \u0026ldquo;explained\u0026rdquo; by a regression line fit to the data. The rest we regard as \u0026ldquo;unexplained.\u0026rdquo;\nFigure [rsq]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;rsq\u0026rdquo;} shows the idea. Panel (a) shows that the total variability in the data can be measured by the differences between each actual observation and the sample mean $\\overline{y}.$ We already know a measure for that:¬†the sample standard deviation $s.$ However, the variability is really measured just by the numerator of that equation, which we defined as $% \u0026ldquo;SSTOT,\u0026ldquo;$ which stands for \u0026ldquo;total sum of squares.\u0026rdquo;¬†This is defined as\n$$SSTOT=\\sum (y_{i}-\\overline{y})^{2}$$\nIn panel (c), we see that some of this variability can be \u0026ldquo;explained\u0026rdquo; by the regression line through the differences between the values $\\widehat{y_{i}}$ and the sample mean $\\overline{y}.$ This \u0026ldquo;explained\u0026rdquo; variability is measured by a sum of squares as well, which we call $SSR$, which stands for \u0026ldquo;sum of squares for regression.\u0026rdquo; This is defined as:\n$$SSR=\\sum (\\widehat{y_{i}}-\\overline{y})^{2}$$\nThe remaining variability is accounted for by the differences in the actual values, $y_{i},$ and the predicted values $\\widehat{y_{i}}.$ This is also a sum of squares, called , $SSE,$ which stands for \u0026ldquo;sum of squares for error.\u0026rdquo; It is defined as:\n$$SSE=\\sum (y_{i}-\\widehat{y_{i}})^{2}$$\nAs implied in the figure, the following relationship holds:\n$$SSTOT=SSR+SSE$$\nThis should remind you of the ANOVA decomposition. In fact, regression is ANOVA, except that the $x$ variable is not called¬†a \u0026ldquo;treatment,\u0026rdquo; and is numeric rather than categorical. If a \u0026ldquo;large\u0026rdquo; proportion of variability in the data can be accounted for by the model then we have evidence that the model is useful. The statistic that quantifies the proportion of variability accounted for by the model is known as $R^{2}$, which we now define.\n$R^{2},$ read as \u0026ldquo;r squared\u0026rdquo; and also known as the coefficient of determination, measures the proportion of total variability in the $Y$ data that is accounted for by the regression fit. The formula is\n$$R^{2}=\\frac{SSR}{SSTOT}=1-\\frac{SSE}{SSTOT}$$\nHere is a useful fact relating $R^{2}$ to correlation.\nIn the simple linear regression model $Y=\\beta_{0}+\\beta_{1}x+\\varepsilon ,$ $R^{2}$ is equal to the square of the correlation coefficient $r.$ That is $R^{2}=r^{2}.$\nHere are some exercises to practice these concepts.\nFor the following values of SSR, SSE, and SSTOT, calculate $R^{2}.$\na). $SSE=159$, $SSTOT=309.3$\nb). $SSR=1682,$ $SSTOT=1982$\nc). $SSR=149,$ $SSE=592$\na). Draw a scatter plot of the data. Does a linear model make sense?\nb). Fit the simple linear regression model $Y=\\beta_{0}+\\beta_{1}x+\\varepsilon$ to the following data by finding $b_{0}$ and $b_{1}.$\nb). Compute and interpret $R^{2}.$\n$\\begin{array}{cc} x \u0026amp; y \\ 2 \u0026amp; 4 \\ 3 \u0026amp; 7 \\ 4 \u0026amp; 11 \\ 7 \u0026amp; 14% \\end{array}%$\nUsing the output in Figure [reg_ouytpu]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;reg_ouytpu\u0026rdquo;}, answer the following questions:\na). Show how $R^{2}$ was calculated.\nb). Interpret $R^{2}$ in the context of the problem\nUse the output in Figure [house_out]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;house_out\u0026rdquo;} to answer the following questions.\na). Show how $R^{2}$ was calculated.\nb). Interpret $R^{2}$ in the context of the problem.\nb). Construct a $95%$ confidence interval for $\\beta_{1}.$\nd). Interpret the estimated parameters $b_{0}$ and $b_{1}.$\n1). Evaluating the Simple Linear Regression Model\u0026ndash;Part 2\n2). Confidence Interval for $E(Y|x)$**¬†and Prediction of New Response** $Y^{new}$\nIn the last handout we examined $R^{2}$, the coefficient of determination, as a measure of how good the fit of a regression model was to the data. Another important element to examine are the assumptions in the regression model itself. Remember the key points (bullet point $4$ is an expansion from Handout 17):\n  We assume the probability distribution of $Y$ changes as $x$ changes.\n  Specifically, we assume that the mean (or expected value) of *¬†*$Y,$ denoted as $E(Y),$ changes as $x$ changes. To indicate this we write $E(Y|x)$ read as \u0026ldquo;the expected value of $Y$ given $x.\u0026ldquo;$\n  We can also indicate $E(Y|x)$ using the notation $\\widehat{Y},$ read as $\u0026quot;y$ hat.\u0026rdquo;\n  With simple regression, we assume that $Y_{i}=\\beta_{0}+\\beta_{1}x_{i}+\\varepsilon_{i}$, where the $\\varepsilon_{i}$ are independent random variables from a normal distribution with mean $0$ and standard deviation $\\sigma .$\n  The basic ways that a simple linear regression model can go wrong are 1). the true $E(Y|x)$ is not linear, but some other function; 2). The error terms $\\varepsilon_{i}$ are not independent; 3). The error terms $% \\varepsilon_{i}$ do not have the same standard deviation $\\sigma ;$ 4). Several of these assumptions are violated. Let\u0026rsquo;s expand upon points¬†(2) and (3). The error terms $\\varepsilon_{i}$ arise due to the imperfect fit between the regression line $\\beta_{0}+\\beta_{1}x$ and the actual value $Y.$ For each value of $Y_{i}$ and $x_{i},$ there is an associated $\\varepsilon_{i}$ that \u0026ldquo;absorbs\u0026rdquo; the effects of all of the variables that we didn\u0026rsquo;t include in the model. One of the key assumptions is that the variance of the error term $V(\\varepsilon )$ *does not change* as $x$ changes. That is, $V(\\varepsilon_{i})=V(\\varepsilon_{j})$ for any $i\\neq j.$ We further assume that the correlation between $\\varepsilon_{i}$ and $\\varepsilon_{j}$ is exactly $0,$ that is, the error in predicting $Y_{i}$ using $x_{i}$ is unrelated to the error in predicting $Y_{j}$ using $x_{j}$, for any $i$ and $% j.$ If these assumptions about the $\\varepsilon_{i}$ are not satisfied, inferential statements we make using the fitted model¬†(for example, calculating a confidence interval or test for $\\beta_{1}$, or predicting a new observation of $Y)$ are not strictly valid, and may thus be highly misleading.\nThe most common method by which we evaluate the assumptions of the regression model is by examining what are called \u0026ldquo;residuals.\u0026rdquo; First, we need to define what a residual is.\nA regression residual $e_{i}$**¬†**is the difference between the actual value $Y_{i}$ and the fitted value $\\widehat{Y_{i}}.$ For a fitted regression model, the residual is\n$$e_{i}=y_{i}-\\widehat{y_{i}}$$\nA scatterplot of $(e_{i},\\widehat{y_{i}})$ is called a **residual plot** . Examining residual plots can help identify problems with the model fit. How should the residual plot look if all of the assumptions are reasonably met? The short answer is: *look for the absence of a pattern.* Figure [good_resid]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;good_resid\u0026rdquo;} illustrates this idea. Notice that the residuals seem to scatter around $0$ with no apparent pattern. We would say this model is a reasonably good fit to the data.\nA violation of the assumption of constant standard deviation (known formally as heteroscedasticity) is shown in Figure [hetero]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;hetero\u0026rdquo;}. Notice that as the $x$ variable (\u0026ldquo;time\u0026rdquo; in this case) gets larger, the residuals begin to \u0026ldquo;fan out\u0026rdquo; in a megaphone pattern, indicating an increase in the variability. In practice, an analyst would employ a variable transformation to the data to correct for this. Discussion of how this transformation is accomplished is beyond the scope of this course.\nAn example of a model in which the assumption that $E(Y|x)$ is linear is not met is shown next in Figure [misspesified]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;misspesified\u0026rdquo;}. Such a model is called a misspecified model, and many times such models will result in error terms that are correlated. Moreover, misspecified models are problematic in their own right simply because any predictions we get from using the model will be wrong because the wrong relationship was postulated between $Y$ and $% x.$ In the example below, it can be shown that fitting a parabola $% E(Y|x)=\\beta_{0}+\\beta_{1}x+\\beta_{2}^{2}$ rather than a line will lead to a residual plot that looks like Figure [good_resid]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;good_resid\u0026rdquo;} above.\nFor many of the real-world data sets we will use, the assumptions of the simple model will be violated. For instance, Figure [crude_resid]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;crude_resid\u0026rdquo;} indicates that the standard deviation of the residual terms increases as the price of oil increases. The pattern is that of the \u0026ldquo;megaphone\u0026rdquo; we saw earlier. It turns out that for this model, the fitted values $\\widehat{y}_{i}$ (i.e., the predictions of $y$ for each price per barrel of oil) don\u0026rsquo;t differ by much when the problem of non-constant variability is corrected. Such correction is beyond the scope of the course in any event. Therefore, we will proceed in the examples as if the model assumptions are reasonably met, although in \u0026ldquo;real life,\u0026rdquo; as you should always do in any situation, you will need to check assumptions.\nOnce you have fit the model, there are two things you might want to do with it: make an inference about $E(Y|x)$ for a specific $x$ and predict a new value of $Y,$ call it $Y^{new}.$ We will discuss these two issues in the next section.\nEstimation of Mean of Y and Prediction of New Y It turns out that estimating the mean of $Y$ for a specific value of $x$ and predicting a new value of $Y$ for a specific value of $x$ are done the same way: plug the specific value of $x$, call it $x_{0}$, into the fitted model. What differs is constructing the interval. In general, the confidence interval for $E(Y|x_{0})$ will be narrower than the **prediction interval** for the value of $Y^{new}.$ This fact makes sense because estimating a specific value of a distribution should logically be less precise than estimating its mean.\nHere are the two formulas\nA $(1-\\alpha )100%$ confidence interval for $E(Y|x_{0})$ for the simple linear regression model is\n$$\\widehat{y}\\pm t_{n-2,\\alpha /2}\\sqrt{\\frac{SSE}{n-2}\\left[ \\frac{1}{n}+% \\frac{(x_{0}-\\overline{x})^{2}}{(n-1)s_{x}^{2}}\\right] }$$\nwhere $x_{0}$ is the specific value of $x$ for which we want to estimate the mean of $Y.$\nA $(1-\\alpha )100%$ prediction interval for a new observation $% Y^{new}$ at a specific value of $x,x_{0}$, is\n$$\\widehat{y}\\pm t_{n-2,\\alpha /2}\\sqrt{\\frac{SSE}{n-2}\\left[ 1+\\frac{1}{n}+% \\frac{(x_{0}-\\overline{x})^{2}}{(n-1)s_{x}^{2}}\\right] }$$\nNotice that the only difference between the two is the addition of the $\u0026quot;1\u0026quot;$ in the prediction interval formula. This makes the interval wider to account for the fact that we are trying to estimate a particular value of $Y$ , which is a more uncertain task than estimating the mean of $Y.$\nLet\u0026rsquo;s work some examples to put these formulas into practice.\nUsing the output in Figure [reg_ouytpu]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;reg_ouytpu\u0026rdquo;} and the fact that the average crude oil price was $$38.13$ with a standard deviation of $$27.85,$ do the following:\na). Compute a $90%$ confidence interval for the average price of gasoline when oil costs $$80$ per barrel.\nb). Compute a $90%$ prediction interval for the average price of gasoline when oil costs $$80$ per barrel.\nUse the output in Figure [house_out]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;house_out\u0026rdquo;} and the fact that the average age of a house is $36.4$ years with a standard deviation of $30.3$ years to answer the following questions.\na). Compute a $99%$ confidence interval for the average price of a $10$ -year-old home.\nb). Compute a $99%$ prediction interval for the price of a $10$-year-old home.\n","date":1595949567,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595949567,"objectID":"fbcd30546562abf2247f6f680f2d6c52","permalink":"/courses/bana3363/17-evaluating-linear-regression/","publishdate":"2020-07-28T15:19:27.628999Z","relpermalink":"/courses/bana3363/17-evaluating-linear-regression/","section":"courses","summary":"Evaluating the Simple Linear Regression Model\u0026ndash;Part 1 The essential components of regression analysis are the following:\n  We assume that two variables, $x$ and $Y,$ are somehow related.\n  Specifically, we assume the probability distribution of $Y$ (which is called the response or dependent variable) changes as $x$ (which is called the predictor or independent variable) changes.","tags":null,"title":"Evaluating Linear Regression","type":"docs"},{"authors":null,"categories":null,"content":"Multiple Linear Regression The point of regression analysis is to use the relationship between a random variable $Y$ and another variable $x$ to make better predictions about $Y$ than could be made by simply using the overall average $\\overline{y}.$ In the \u0026ldquo;real world,\u0026rdquo; variables are related to one another, and thus a model that allows the distribution of one variable to change with another is one way to model this relationship. However, it would be rather silly to conclude that $Y$ is influenced only by a single variable $x.$ Rather, it is much more sensible that $Y$ is influenced by a collection of $x$ variables, say $\\mathbf{x=(}x_{1},x_{2},\\ldots ,x_{p})$ where $p\u0026gt;2.$\nFor example, if $Y$ is a person\u0026rsquo;s cholesterol level, a person\u0026rsquo;s age would be a logical choice for an $x$ variable. We might call it $x_{1}.$ But cholesterol might also be related to gender $(x_{2})$, weight $(x_{3})$, blood pressure $(x_{4})$, and many other variables. Being able to model $Y$ as a function of all of these variables simultaneously would allow doctors and researchers to examine the effects of each of these variables on cholesterol level in the presence of the other variables, and make predictions that could help patients make decisions about their lifestyles.\nFortunately, regression analysis extends naturally from one independent variable to many independent variables. The basic concept is essentially the same, except we add more $x$ variables to the mix:\n  We assume the probability distribution of $Y$ changes as the collection of $x$ variables $\\mathbf{x=(}x_{1},x_{2},\\ldots ,x_{p})$ changes\n  Specifically, we assume that the mean (or expected value) of $Y$, denoted as $E(Y)$, changes as $\\mathbf{x}$ changes. To indicate this we write $E(Y|\\mathbf{x})$ read as \u0026ldquo;the expected value of $Y$ given $\\mathbf{x}$.\u0026rdquo;\n  When multiple $x$ variables are involved, however, we can define many types of functional relationships between $Y$ and $\\mathbf{x.}$ The simplest one is known as the¬†\u0026ldquo;first-order multiple linear regression model,\u0026rdquo; which we now define:\nA first-order multiple linear regression model is specified by\n$$Y=\\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+\\ldots +\\beta_{p}x_{p}+\\varepsilon$$\nwhere the $\\beta$ terms are fixed (but unknown) parameters and $\\varepsilon$ has a normal distribution with mean $0$ and standard deviation $\\sigma$.\nThe implication of the multiple linear regression model is that the mean of $% Y$ is $E(Y|\\mathbf{x})=\\widehat{Y}=$ $\\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+\\ldots +\\beta_{p}x_{p}$ because, by the rules of expected value¬†(see Handout $1$),\n$$\\begin{aligned} E(Y|\\mathbf{x}) \u0026amp;=\u0026amp;E(\\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+\\ldots +\\beta_{p}x_{p}+\\varepsilon ) \\ \u0026amp;=\u0026amp;\\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+\\ldots +\\beta_{p}x_{p}+E(\\varepsilon )\\text{ [since }\\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+\\ldots +\\beta_{p}x_{p}\\text{ is a constant]} \\ \u0026amp;=\u0026amp;\\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+\\ldots +\\beta_{p}x_{p}\\text{ [since we assume }E(\\varepsilon )=0]\\end{aligned}$$\nWe can interpret the parameters in a similar manner as with the simple linear regression model, except that we must now be mindful of the other $x$ variables in the model. Here are the general interpretations.\n  $\\beta_{0}$ is the mean of $Y$ when all the $x$ variables are $0$.\n  $\\beta_{1}$ is the increase in the mean of $Y$ for a one-unit increase in $x_{1}$, holding the other $x$ variables fixed$.$\n  In general, $\\beta_{j}$ is the increase in the mean of $Y$ for a one-unit increase in $x_{j}$, holding the other $x$ variables fixed.\n  The parameters are estimated with ordinary least squares (OLS) just as with the simple linear regression model. However, in the case of multiple $x$ variables, the formulas are not simple to write out by hand. In fact, the values that minimize $SSE=\\sum (y_{i}-b_{0}-b_{1}x_{i1}-b_{2}x_{i2}-\\ldots -b_{p}x_{ip})^{2}$ are found using matrix algebra, which is beyond the scope of this course. Suffice it to say that you will never have to find these values by hand. Multiple regression is nearly always carried out using specialized software such as Excel.\nWith the simple linear regression model, the fitted function $\\widehat{y}% =b_{0}+b_{1}x_{1}$ was a line. With a multiple linear regression model, the fitted function $\\widehat{y}=b_{0}+b_{1}x_{1}+b_{2}x_{2}+\\ldots +b_{p}x_{p}$ is a **plane** (flat surface) in $p+1$ dimensions. For instance if we were to fit a model with two $x$ variables, we would obtain the following function in Figure [regplane]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;regplane\u0026rdquo;}, a plane in three dimensions. With more than two $x$ variables, we obtain a **hyperplane**, a figure that we can no longer easily visualize.\nLet\u0026rsquo;s now discuss how we might use assess and use the multiple linear regression model.\nUsing the Multiple Linear Regression Model After the model has been fit, the next logical question is whether the model is useful or not. One way of assessing this is to determine if there is an overall regression relationship between $Y$ and the $x$ variables. We can examine the overall regression relationship by examining $R^{2}$ and by conducting the model F test. Let\u0026rsquo;s discuss the overall test first.\nModel F Test  Specify the hypothesis. The hypothesis is very similar to the ANOVA hypothesis of Chapter 14.  $$\\begin{aligned} H_{0} \u0026amp;:\u0026amp;\\beta_{1}=\\beta_{2}=\\ldots =\\beta_{p}=0 \\ H_{1} \u0026amp;:\u0026amp;\\text{ Not all }\\beta ^{\\prime }s\\text{ equal 0 [or, at least one }% \\beta \\text{ is not equal to }0]\\end{aligned}$$\n  Calculate the test statistic.\nThe test uses an $F$ statistic\n$$f=\\frac{SSR/p}{SSE/(n-(p+1))}$$\n  Compute the $p$-value\n$$p=P(F_{p,n-(p+1)}\u0026gt;f)$$\nAlternatively, we can define the rejection region as follows:\n$${\\text{ }f_{p,n-(p+1)}:f_{p,n-(p+1)}\u0026gt;f_{p,n-(p+1)},_{\\alpha }}$$\n  Make a conclusion\nIf the $p$-value $\\leq \\alpha $, we reject $H_{0}.$ If the $p$-value $% \u0026gt;\\alpha $, we fail to reject $H_{0}.$ Equivalently, if $f$ lies within the rejection region, we reject $H_{0}$. If $f$ lies outside the rejection region, we fail to reject $H_{0}.$\n  In general, to construct confidence intervals or tests for regression coefficients, we need the standard error and the appropriate value of $t$ from the Student\u0026rsquo;s t distribution. The test for an individual regression coefficient $\\beta_{j}$ is as follows.\nTest and Confidence Interval for Individual $\\beta_{j}$   Specify the hypotheses.\nThe parameter of interest is $\\beta_{j}$, so the hypotheses are statements about its value.\n$$\\begin{aligned} \\text{(a) }H_{0} \u0026amp;:\u0026amp;\\beta_{j}\\leq c\\text{ vs. }H_{0}:\\beta_{j}\u0026gt;c \\ \\text{(b) }H_{0} \u0026amp;:\u0026amp;\\beta_{j}\\geq c\\text{ vs. }H_{0}:\\beta_{j}\u0026lt;c \\ \\text{(c) }H_{0} \u0026amp;:\u0026amp;\\beta_{j}=c\\text{ vs. }H_{0}:\\beta_{j}\\neq c\\end{aligned}$$\nIn practice, $c$ is usually $0$, but it could be anything. In regression programs such as Excel, the hypothesis tested by default is\n$$H_{0}:\\beta_{j}=0\\text{ vs. }H_{0}:\\beta_{j}\\neq 0$$\n  Calculate the test statistic.\nThe test statistic is a $t$ statistic:\n$$t_{0}=\\frac{b_{j}-c}{s.e.(b_{j})}$$\n  Compute the p-value\n$$\\begin{aligned} \\text{(a) }p \u0026amp;=\u0026amp;P(t_{n-(p+1)}\u0026gt;t_{0}) \\ \\text{(b) }p \u0026amp;=\u0026amp;P(t_{n-(p+1)}\u0026lt;t_{0}) \\ \\text{(c) }p \u0026amp;=\u0026amp;2\\min [P(t_{n-(p+1)}\u0026lt;t_{0}),P(t_{n-(p+1)}\u0026gt;t_{0})]\\end{aligned}$$\nAlternatively, we can define the rejection region as follows:\n$$\\begin{aligned} \\text{(a)}{\\text{ }t_{n-(p+1)} \u0026amp;:\u0026amp;t_{n-(p+1)}\u0026gt;t_{n-(p+1),\\alpha }} \\ \\text{(b)}{\\text{ }t_{n-(p+1)} \u0026amp;:\u0026amp;t_{n-(p+1)}\u0026lt;-t_{n-(p+1),\\alpha }} \\ \\text{(c)}{t_{n-(p+1)} \u0026amp;:\u0026amp;|t_{n-(p+1)}|\u0026gt;t_{n-(p+1),\\alpha /2}}\\end{aligned}$$\n  Make a conclusion\nIf the $p$-value $\\leq \\alpha $, we reject $H_{0}.$ If the $p$-value $% \u0026gt;\\alpha $, we fail to reject $H_{0}.$ Equivalently, if $t_{0}$ lies within the rejection region, we reject $H_{0}$. If $t_{0}$ lies outside the rejection region, we fail to reject $H_{0}.$\n  An important issue arises in multiple regression with respect to $R^{2}$ that we summarize in the following theorem.\nThe confidence interval for an individual coefficient is given in the following definition.\nA $(1-\\alpha )100%$ confidence interval for $\\beta_{j}$ is given by\n$$b_{j}\\pm t_{n-(p+1),\\alpha /2}s.e.(b_{j})$$\nwhere $t_{n-(p+1),\\alpha /2}$ is the value of Student\u0026rsquo;s $t$ distribution with $n-(p+1)$ degrees of freedom and $\\alpha /2$ probability to the right, and $s.e.(b_{j})$ is the standard error of $b_{j}.$\nThe interpretation of a confidence interval in the multiple regression context must mention the other variables being held constant.\nAn important issue arises in multiple regression with respect to $R^{2}$ that we summarize in the following theorem.\n$R^{2}$ will never decrease with the addition of $x$ variables. In other words, it is possible to \u0026ldquo;force\u0026rdquo; $R^{2}$ to $1$ by adding additional $x$ variables, whether they are useful for predicting $Y$ or not.\nTherefore, what is needed is a measure of fit that penalizes us for adding additional variables that don\u0026rsquo;t really help in predicting $Y.$ Statisticians have defined many such measures, but the one most used in practice (and which is reported in Excel) is called adjusted $R^{2}.$\nAdjusted $R^{2}$ (denoted as $R_{adj}^{2})$ is a penalizing measure of fit for a multiple regression model defined by\n$$R_{adj}^{2}=1-\\left( \\frac{n-1}{n-(p+1)}\\right) (1-R^{2})$$\nSome facts about $R_{adj}^{2}$ that make it useful for assessing model fit are:¬†1). it is possible for $R_{adj}^{2}$ to decrease if the addition of an independent variable is not useful for predicting $Y$, and 2). it is possible for $R_{adj}^{2}$ to be negative. When fitting a multiple regression model, $R_{adj}^{2}$ is the preferred measure of fit over $R^{2}.$\nLet\u0026rsquo;s work an example or two using Excel.\nWe can think of many factors that might influence the final sale of a house, such as age, square footage, # of bathrooms, # of bedrooms, amenities, etc. Let\u0026rsquo;s fit a model to¬†age, square footage, and # of bathrooms using the Ames housing data set from earlier handouts. Using the excel output in Figure [reg_output]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;reg_output\u0026rdquo;}, answer the following questions.\na). What would be the estimated selling price of a house that is $10$¬†years old, and for which there are $2$¬†bathrooms and $% 1400$¬†square feet?\nb). How would you interpret the coefficients?\nc). How would you interpret the coefficient for Bath?\nd). Show how adjusted $R^{2}$¬†was calculated. Why should we look at this value rather than $R^{2}?$\ne). Conduct the overall $F$¬†test (specify hypotheses, report test statistic, report rejection region, report conclusion).\nf). Report and interpret the confidence interval for Bath.\nData on $4,137$ college students was obtained and a first-order multiple linear regression model was fit in order to determine the effects of various factors on college GPA, measured on the usual four-point scale. The results are shown in the Excel output below in Figure [col_gpa]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;col_gpa\u0026rdquo;}.\na). Write down the estimated model.\nb). What would be the estimated GPA of a student with an SAT score of $1190$¬†who graduated $3^{rd\\text{ \\ }}$ in a class of $542$¬†students?\nc). How would you interpret the coefficients?\nd). Calculate and interpret a $95%$¬†confidence interval for $\\beta_{3}$,¬†the coefficient for high school rank.\ne). Test the hypothesis that $\\beta_{2}\\neq 0$¬†using the four-step procedure.\nAn economist is studying the nature of street vending in Mexico. She has gathered the following data for $15$ vendors: age, hours worked per day, and annual earnings. Fit a multiple linear regression model using the data below.\n   Earnings Age Hours     2841 29 12   1876 21 8   2934 62 10   1552 18 10   3065 40 11   3670 50 11   2005 65 5   3215 44 8   1930 17 8   2010 70 6   3111 20 9   2882 29 9   1683 15 5   1817 14 7   4066 33 12    a). Write down the estimated model.\nb). What would be the estimated average earnings for a $19$ -year-old vendor working $5$¬†hours per day?\nc). How would you interpret the coefficients?\nd). Test the hypothesis that $\\beta_{1}$,¬†the coefficient for age, is not equal to $0$¬†using the four-step procedure.\ne). Calculate and interpret a $99%$¬†confidence interval for $\\beta_{2}$,¬†the coefficient for hours.\nMultiple Linear Regression The point of regression analysis is to use the relationship between a random variable $Y$ and another variable $x$ to make better predictions about $Y$ than could be made by simply using the overall average $\\overline{y}.$ In the \u0026ldquo;real world,\u0026rdquo; variables are related to one another, and thus a model that allows the distribution of one variable to change with another is one way to model this relationship. However, it would be rather silly to conclude that $Y$ is influenced only by a single variable $x.$ Rather, it is much more sensible that $Y$ is influenced by a collection of $x$ variables, say $\\mathbf{x=(}x_{1},x_{2},\\ldots ,x_{p})$ where $p\u0026gt;2.$ For example, if $Y$ is a person\u0026rsquo;s cholesterol level, a person\u0026rsquo;s age would be a logical choice for an $x$ variable. We might call it $x_{1}.$ But cholesterol might also be related to gender $(x_{2})$, weight $(x_{3})$, blood pressure $(x_{4})$, and many other variables. Being able to model $Y$ as a function of all of these variables simultaneously would allow doctors and researchers to examine the effects of each of these variables on cholesterol level in the presence of the other variables, and make predictions that could help patients make decisions about their lifestyles.\nFortunately, regression analysis extends naturally from one independent variable to many independent variables. The basic concept is essentially the same, except we add more $x$ variables to the mix:\n  We assume the probability distribution of $Y$ changes as the collection of $x$ variables $\\mathbf{x=(}x_{1},x_{2},\\ldots ,x_{p})$ changes\n  Specifically, we assume that the mean (or expected value) of $Y$, denoted as $E(Y)$, changes as $\\mathbf{x}$ changes. To indicate this we write $E(Y|\\mathbf{x})$ read as \u0026ldquo;the expected value of $Y$ given $\\mathbf{x}.\u0026ldquo;$\n  When multiple $x$ variables are involved, however, we can define many types of functional relationships between $Y$ and $\\mathbf{x.}$ The simplest one is known as the¬†\u0026ldquo;first-order multiple linear regression model,\u0026rdquo; which we now define:\nA first-order multiple linear regression model is specified by\n$$Y=\\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+\\ldots +\\beta_{p}x_{p}+\\varepsilon$$\nwhere the $\\beta$ terms are fixed (but unknown) parameters and $\\varepsilon$ has a normal distribution with mean $0$ and standard deviation $\\sigma$.\nThe implication of the multiple linear regression model is that the mean of $% Y$ is $E(Y|\\mathbf{x})=\\widehat{Y}=$ $\\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+\\ldots +\\beta_{p}x_{p}$ because, by the rules of expected value¬†(see Handout $1$),\n$$\\begin{aligned} E(Y|\\mathbf{x}) \u0026amp;=\u0026amp;E(\\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+\\ldots +\\beta_{p}x_{p}+\\varepsilon ) \\ \u0026amp;=\u0026amp;\\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+\\ldots +\\beta_{p}x_{p}+E(\\varepsilon )\\text{ [since }\\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+\\ldots +\\beta_{p}x_{p}\\text{ is a constant]} \\ \u0026amp;=\u0026amp;\\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}x_{2}+\\ldots +\\beta_{p}x_{p}\\text{ [since we assume }E(\\varepsilon )=0]\\end{aligned}$$\nWe can interpret the parameters in a similar manner as with the simple linear regression model, except that we must now be mindful of the other $x$ variables in the model. Here are the general interpretations.\n  $\\beta_{0}$ is the mean of $Y$ when all the $x$ variables are $0$.\n  $\\beta_{1}$ is the increase in the mean of $Y$ for a one-unit increase in $x_{1}$, holding the other $x$ variables fixed$.$\n  In general, $\\beta_{j}$ is the increase in the mean of $Y$ for a one-unit increase in $x_{j}$, holding the other $x$ variables fixed.\n  The parameters are estimated with ordinary least squares (OLS) just as with the simple linear regression model. However, in the case of multiple $x$ variables, the formulas are not simple to write out by hand. In fact, the values that minimize $SSE=\\sum (y_{i}-b_{0}-b_{1}x_{i1}-b_{2}x_{i2}-\\ldots -b_{p}x_{ip})^{2}$ are found using matrix algebra, which is beyond the scope of this course. Suffice it to say that you will never have to find these values by hand. Multiple regression is nearly always carried out using specialized software such as Excel.\nLet\u0026rsquo;s now discuss how we might use assess and use the multiple linear regression model.\nUsing the Multiple Linear Regression Model After the model has been fit, the next logical question is whether the model is useful or not. One way of assessing this is to determine if there is an overall regression relationship between $Y$ and the $x$ variables. We can examine the overall regression relationship by examining $R^{2}$ and by conducting the model F test. Let\u0026rsquo;s discuss the overall test first.\nModel F Test  Specify the hypothesis. The hypothesis is very similar to the ANOVA hypothesis of Chapter 14.  $$\\begin{aligned} H_{0} \u0026amp;:\u0026amp;\\beta_{1}=\\beta_{2}=\\ldots =\\beta_{p}=0 \\ H_{1} \u0026amp;:\u0026amp;\\text{ Not all }\\beta ^{\\prime }s\\text{ equal 0 [or, at least one }% \\beta \\text{ is not equal to }0]\\end{aligned}$$\n  Calculate the test statistic.\nThe test uses an $F$ statistic\n$$f=\\frac{SSR/p}{SSE/(n-(p+1))}$$\n  Compute the $p$-value\n$$p=P(F_{p,n-(p+1)}\u0026gt;f)$$\nAlternatively, we can define the rejection region as follows:\n$${\\text{ }f_{p,n-(p+1)}:f_{p,n-(p+1)}\u0026gt;f_{p,n-(p+1)},_{\\alpha }}$$\n  Make a conclusion\nIf the $p$-value $\\leq \\alpha $, we reject $H_{0}.$ If the $p$-value $% \u0026gt;\\alpha $, we fail to reject $H_{0}.$ Equivalently, if $f$ lies within the rejection region, we reject $H_{0}$. If $f$ lies outside the rejection region, we fail to reject $H_{0}.$\n  In general, to construct confidence intervals or tests for regression coefficients, we need the standard error and the appropriate value of $t$ from the Student\u0026rsquo;s t distribution. The test for an individual regression coefficient $\\beta_{j}$ is as follows.\nTest and Confidence Interval for Individual $\\beta_{j}$   Specify the hypotheses.\nThe parameter of interest is $\\beta_{j}$, so the hypotheses are statements about its value.\n$$\\begin{aligned} \\text{(a) }H_{0} \u0026amp;:\u0026amp;\\beta_{j}\\leq c\\text{ vs. }H_{0}:\\beta_{j}\u0026gt;c \\ \\text{(b) }H_{0} \u0026amp;:\u0026amp;\\beta_{j}\\geq c\\text{ vs. }H_{0}:\\beta_{j}\u0026lt;c \\ \\text{(c) }H_{0} \u0026amp;:\u0026amp;\\beta_{j}=c\\text{ vs. }H_{0}:\\beta_{j}\\neq c\\end{aligned}$$\nIn practice, $c$ is usually $0$, but it could be anything. In regression programs such as Excel, the hypothesis tested by default is\n$$H_{0}:\\beta_{j}=0\\text{ vs. }H_{0}:\\beta_{j}\\neq 0$$\n  Calculate the test statistic.\nThe test statistic is a $t$ statistic:\n$$t_{0}=\\frac{b_{j}-c}{s.e.(b_{j})}$$\n  Compute the p-value\n$$\\begin{aligned} \\text{(a) }p \u0026amp;=\u0026amp;P(t_{n-(p+1)}\u0026gt;t_{0}) \\ \\text{(b) }p \u0026amp;=\u0026amp;P(t_{n-(p+1)}\u0026lt;t_{0}) \\ \\text{(c) }p \u0026amp;=\u0026amp;2\\min [P(t_{n-(p+1)}\u0026lt;t_{0}),P(t_{n-(p+1)}\u0026gt;t_{0})]\\end{aligned}$$\nAlternatively, we can define the rejection region as follows:\n$$\\begin{aligned} \\text{(a)}{\\text{ }t_{n-(p+1)} \u0026amp;:\u0026amp;t_{n-(p+1)}\u0026gt;t_{n-(p+1),\\alpha }} \\ \\text{(b)}{\\text{ }t_{n-(p+1)} \u0026amp;:\u0026amp;t_{n-(p+1)}\u0026lt;-t_{n-(p+1),\\alpha }} \\ \\text{(c)}{t_{n-(p+1)} \u0026amp;:\u0026amp;|t_{n-(p+1)}|\u0026gt;t_{n-(p+1),\\alpha /2}}\\end{aligned}$$\n  Make a conclusion\nIf the $p$-value $\\leq \\alpha $, we reject $H_{0}.$ If the $p$-value $% \u0026gt;\\alpha $, we fail to reject $H_{0}.$ Equivalently, if $t_{0}$ lies within the rejection region, we reject $H_{0}$. If $t_{0}$ lies outside the rejection region, we fail to reject $H_{0}.$\n  An important issue arises in multiple regression with respect to $R^{2}$ that we summarize in the following theorem.\nThe confidence interval for an individual coefficient is given in the following definition.\nA $(1-\\alpha )100%$ confidence interval for $\\beta_{j}$ is given by\n$$b_{j}\\pm t_{n-(p+1),\\alpha /2}s.e.(b_{j})$$\nwhere $t_{n-(p+1),\\alpha /2}$ is the value of Student\u0026rsquo;s $t$ distribution with $n-(p+1)$ degrees of freedom and $\\alpha /2$ probability to the right, and $s.e.(b_{j})$ is the standard error of $b_{j}.$\nThe interpretation of a confidence interval in the multiple regression context must mention the other variables being held constant.\nAn important issue arises in multiple regression with respect to $R^{2}$ that we summarize in the following theorem.\n$R^{2}$ will never decrease with the addition of $x$ variables. In other words, it is possible to \u0026ldquo;force\u0026rdquo; $R^{2}$ to $1$ by adding additional $x$ variables, whether they are useful for predicting $Y$ or not.\nTherefore, what is needed is a measure of fit that penalizes us for adding additional variables that don\u0026rsquo;t really help in predicting $Y.$ Statisticians have defined many such measures, but the one most used in practice (and which is reported in Excel) is called adjusted $R^{2}.$\nAdjusted $R^{2}$ (denoted as $R_{adj}^{2})$ is a penalizing measure of fit for a multiple regression model defined by\n$$R_{adj}^{2}=1-\\left( \\frac{n-1}{n-(p+1)}\\right) (1-R^{2})$$\nSome facts about $R_{adj}^{2}$ that make it useful for assessing model fit are:¬†1). it is possible for $R_{adj}^{2}$ to decrease if the addition of an independent variable is not useful for predicting $Y$, and 2). it is possible for $R_{adj}^{2}$ to be negative. When fitting a multiple regression model, $R_{adj}^{2}$ is the preferred measure of fit over $R^{2}.$\nLet\u0026rsquo;s work an example or two using Excel.\nWe can think of many factors that might influence the final sale of a house, such as age, square footage, # of bathrooms, # of bedrooms, amenities, etc. Let\u0026rsquo;s fit a model to¬†age, square footage, and # of bathrooms using the Ames housing data set from earlier handouts. Using the excel output in Figure [reg_output]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;reg_output\u0026rdquo;}, answer the following questions.\na). What would be the estimated selling price of a house that is $10$¬†years old, and for which there are $2$¬†bathrooms and $% 1400$¬†square feet?\nb). How would you interpret the coefficients?\nc). How would you interpret the coefficient for Bath?\nd). Show how adjusted $R^{2}$¬†was calculated. Why should we look at this value rather than $R^{2}?$\ne). Conduct the overall $F$¬†test (specify hypotheses, report test statistic, report rejection region, report conclusion).\nf). Report and interpret the confidence interval for Bath.\nData on $4,137$ college students was obtained and a first-order multiple linear regression model was fit in order to determine the effects of various factors on college GPA, measured on the usual four-point scale. The results are shown in the Excel output below in Figure [col_gpa]{reference-type=\u0026quot;ref\u0026rdquo; reference=\u0026quot;col_gpa\u0026rdquo;}.\na). Write down the estimated model.\nb). What would be the estimated GPA of a student with an SAT score of $1190$¬†who graduated $3^{rd\\text{ \\ }}$in a class of $% 542$¬†students?\nc). How would you interpret the coefficients?\nd). Calculate and interpret a $95%$¬†confidence interval for $\\beta_{3}$,¬†the coefficient for high school rank.\ne). Test the hypothesis that $\\beta_{2}\\neq 0$¬†using the four-step procedure.\nAn economist is studying the nature of street vending in Mexico. She has gathered the following data for $15$ vendors: age, hours worked per day, and annual earnings. Fit a multiple linear regression model using the data below.\n   Earnings Age Hours     2841 29 12   1876 21 8   2934 62 10   1552 18 10   3065 40 11   3670 50 11   2005 65 5   3215 44 8   1930 17 8   2010 70 6   3111 20 9   2882 29 9   1683 15 5   1817 14 7   4066 33 12    a). Write down the estimated model.\nb). What would be the estimated average earnings for a $19$ -year-old vendor working $5$¬†hours per day?\nc). How would you interpret the coefficients?\nd). Test the hypothesis that $\\beta_{1}$,¬†the coefficient for age, is not equal to $0$¬†using the four-step procedure.\ne). Calculate and interpret a $99%$¬†confidence interval for $\\beta_{2}$,¬†the coefficient for hours.\n","date":1595949567,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595949567,"objectID":"0401cfa495e76854e58a019b5a3ae6c3","permalink":"/courses/bana3363/18-multiple-linear-regression/","publishdate":"2020-07-28T15:19:27.628999Z","relpermalink":"/courses/bana3363/18-multiple-linear-regression/","section":"courses","summary":"Multiple Linear Regression The point of regression analysis is to use the relationship between a random variable $Y$ and another variable $x$ to make better predictions about $Y$ than could be made by simply using the overall average $\\overline{y}.","tags":null,"title":"Multiple Linear Regression","type":"docs"},{"authors":[],"categories":null,"content":"We will build an example website and host it during the presentation. The site will be written using Markdown, a simple, plain text formatting syntax. We will show how you can make web pages and slides using Markdown that because stylish websites. The site will be integrated with Github so that changes we make will automatically update the public facing site.\nParticipants will leave the session able to make websites using Markdown. We will also discuss how Markdown can be used with Jupyter Notebooks and R for incorporating code and math ($\\rm \\LaTeX$) into websites.\n","date":1600945200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600945200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"Slides for Digital Education Summit","tags":[],"title":"Digital Education Summit","type":"talk"},{"authors":[],"categories":[],"content":"Create websites in Markdown  Academic | Documentation\n What problem are we trying to solve?  Write in Markdown not HTML Spend time focused on content not format Separate content from presentation format 3-in-1: Create, Present, and Publish your content Automatically update/deploy new versions Track all pervious versions for version control Free   Why did I chose to do this?  Writing code and math in PPT stinks I wanted to write things in one place, not have to have versions in Word, PowerPoint, and Code editor I wanted a way to do code syntax highlighting   Additional requires I had  Must be able to create PDF of slides Must be able handle math Must be beautiful Must have an active user community   What does good look like?  A website that is interactive and beautiful A simple process for adding, deleting, or changing content Supports images, videos, and mathematical expressions Works with tools like Jupyter Notebooks and R-Markdown Free   Static Site Generator (SSG)  We will use Hugo There are lots of options including:  Jekyll MKDocs Cleaver    I like Hugo because:\n It has lots of nice templates It is wicked fast It has widgets that can be used on multiple sites   Tools required  Terminal  Hugo Markdown editor (like VS Code or Typora) GitHub account and Git installed on computer Netlify account   What is Markdown? Markdown is a lightweight markup language that you can use to add formatting elements to plaintext text documents. Created by John Gruber in 2004, Markdown is now one of the world‚Äôs most popular markup languages.\nUsing Markdown is different than using a WYSIWYG editor. In an application like Microsoft Word, you click buttons to format words and phrases, and the changes are visible immediately. Markdown isn‚Äôt like that. When you create a Markdown-formatted file, you add Markdown syntax to the text to indicate which words and phrases should look different.\n Markdown Guide\n Why use Markdown?  Markdown can be used for everything. Markdown is portable. Markdown is platform independent. Markdown is future proof. Markdown is everywhere.   What does it look like? ## Why use Markdown? 1. Markdown can be used for **everything**. 1. Markdown is *portable*. 1. Markdown is _platform independent_. 1. Markdown is __future proof__. 1. Markdown is __everywhere__.   How does the process work? graph LR; A(Write Markdown) --\u0026gt; B(Convert Markdown to HTML); B --\u0026gt; C(Push HTML to GitHub); C --\u0026gt; D(Render New Version of Site);  graph LR; subgraph Your Computer A(VS Code) --\u0026gt; B(Hugo and Terminal); end subgraph Internet B --\u0026gt; C(Git and GitHub); C --\u0026gt; D(Netlify); end   Netlify  Free for basic features Connects to GitHub and automatically refreshes You can change the default name in the URL You can also bring your own URL  Other options include AWS Amplify and Github Pages\n How do I convert existing material?   Pandoc  pptx2md Online converters   Beware of these Dangers  Markdown spell checkers don\u0026rsquo;t work well Sometimes the formatting gets messed up Markdown comes in multiple \u0026ldquo;flavors\u0026rdquo; Converters are not perfect   Where do we go from here?  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes  Template Documentation   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n","date":1600905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600905600,"objectID":"a4a7b8793ecf3276af564f3b378adb55","permalink":"/slides/digital/","publishdate":"2020-09-24T00:00:00Z","relpermalink":"/slides/digital/","section":"slides","summary":"Using Hugo to make academic websites","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1600560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600560000,"objectID":"208e12931c9abe18c29b26f133db3b01","permalink":"/project/pyfi/","publishdate":"2020-09-20T00:00:00Z","relpermalink":"/project/pyfi/","section":"project","summary":"Course on applying Python to conduct business analytics","tags":["Python"],"title":"Python for Economics and Finance","type":"project"},{"authors":null,"categories":null,"content":"","date":1587945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587945600,"objectID":"d68e28dd1cfcd5316250b148c9b48a9a","permalink":"/project/bana/","publishdate":"2020-04-27T00:00:00Z","relpermalink":"/project/bana/","section":"project","summary":"BANA 3363 Presentation Slides","tags":["Demo"],"title":"Business Analytics","type":"project"},{"authors":null,"categories":null,"content":"","date":1587945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587945600,"objectID":"4f6c355240c71d652b765dc8d776f61c","permalink":"/project/supply-chain/","publishdate":"2020-04-27T00:00:00Z","relpermalink":"/project/supply-chain/","section":"project","summary":"Project with supply chain management","tags":["Demo"],"title":"Supply Chain Management","type":"project"},{"authors":["Kyle Jones"],"categories":["Demo"],"content":"Quiz Here is a simple example of a quiz, written in markdown using hugo shortcodes\n.quiz fieldset { border-color: black; border-width: 10px; margin-bottom: 1em; } .quiz legend { font-size: 105%; font-weight: 600; padding-left: 15px; padding-right: 15px; padding-top: 15px; } .quiz label { display: block; line-height: 1.75em; } .quiz input[type=\"radio\"] { margin-right: 10px; page-break-after: avoid; page-break-before: avoid; } .quiz input[type=\"submit\"] { background: black; color: white; display: block; font-size: 120%; font-weight: 600; height: 2.5em; margin-top: 2em; text-transform: uppercase; width: 100%; } .quiz table { color: white; font-weight: bold; margin: 1em auto 2em auto; width: 100%; } .quiz td { padding: 5px 15px; text-align: left; width: 60px; } .quiz td.missing-label, .quiz td.missing-score { background: #CECBC2; } .quiz td.right-label, .quiz td.right-score { background: #74b559; } .quiz td.wrong-label, .quiz td.wrong-score { background: #D01F3C; }   Quiz header  var choices = \"Mike,Grant,Frank\".split(\",\"); var id = \"test_quiz\"; var question = \"What is buried in Grant\\u0027s Tomb?\"; var answer = 2 ; if (! (id in questions)){ questions[id] = []; } questions[id].push(new Question(question, choices, answer-1));   var choices = \"Cologne,Berlin,Hamburg\".split(\",\"); var id = \"test_quiz\"; var question = \"What is the capital of Germany?\"; var answer = 2 ; if (! (id in questions)){ questions[id] = []; } questions[id].push(new Question(question, choices, answer-1));   var quiz = new Quiz(\"test_quiz\", questions, {\"shuffle\": true});   Click on the submit button to see the result.\nInteractive code     This is an python exercise with a plot   import numpy as np import matplotlib.pyplot as plt x = np.arange(0, 5, 0.1); y = np.sin(x) plt.plot(x, y) plt.show()    Just press 'Run'.   Here is an example using R  # no pec # Calculate 3 + 4 3 + 4 # Calculate 6 + 12  # Calculate 3 + 4 3 + 4 # Calculate 6 + 12 6 + 12 test_output_contains(\u0026quot;18\u0026quot;, incorrect_msg = \u0026quot;Make sure to add `6 + 12` on a new line. Do not start the line with a `#`, otherwise your R code is not executed!\u0026quot;) success_msg(\u0026quot;Awesome! See how the console shows the result of the R code you submitted? Now that you\u0026#39;re familiar with the interface, let\u0026#39;s get down to R business!\u0026quot;)  Just add a line of R code that calculates the sum of 6 and 12, just like the example in the sample code!\n     ","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587081600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"/post/getting-started/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Examples of an embedded quiz and interactive code blocks using shortcodes","tags":["Academic"],"title":"Adding interactive elements","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Kyle Jones Digital Education Summit Academic | Documentation\n What problem are we trying to solve?  Write in Markdown not HTML Spend time focused on content not format Separate content from presentation format 3-in-1: Create, Present, and Publish your content Automatically update/deploy new versions Track all pervious versions for version control Free   Why did I chose to do this?  Writing code and math in PPT stinks I wanted to write things in one place, not have to have versions in Word, PowerPoint, and Code editor I wanted a way to do code syntax highlighting   Additional requires I had  Must be able to create PDF of slides Must be able handle math Must be beautiful Must have an active user community   What does good look like?  A website that is interactive and beautiful A simple process for adding, deleting, or changing content Supports images, videos, and mathematical expressions Works with tools like Jupyter Notebooks and R-Markdown Free   Static Site Generator (SSG)  We will use Hugo There are lots of options including:  Jekyll MKDocs Cleaver     I like Hugo because:\n It has lots of nice templates It is wicked fast It has widgets that can be used on multiple sites   Tools required  Terminal  Hugo Markdown editor (like VS Code or Typora) GitHub account and Git installed on computer Netlify account   What is Markdown? Markdown is a lightweight markup language that you can use to add formatting elements to plaintext text documents.\nUsing Markdown is different than using a WYSIWYG editor. When you create a Markdown-formatted file, you add Markdown syntax to the text to indicate which words and phrases should look different.\n Markdown Guide\n Why use Markdown?  Markdown can be used for everything. Markdown is portable. Markdown is platform independent. Markdown is future proof. Markdown is everywhere.   What does it look like? ## Why use Markdown? 1. Markdown can be used for **everything**. 1. Markdown is *portable*. 1. Markdown is _platform independent_. 1. Markdown is __future proof__. 1. Markdown is __everywhere__.   How does the process work? graph LR; A(Write Markdown) --\u0026gt; B(Convert Markdown to HTML); B --\u0026gt; C(Push HTML to GitHub); C --\u0026gt; D(Render New Version of Site);  graph LR; subgraph Your Computer A(VS Code) --\u0026gt; B(Hugo and Terminal); end subgraph Internet B --\u0026gt; C(Git and GitHub); C --\u0026gt; D(Netlify); end   Netlify  Free for basic features Connects to GitHub and automatically refreshes You can change the default name in the URL You can also bring your own URL  Other options include AWS Amplify and Github Pages\n How do I convert existing material?   Pandoc  pptx2md Online converters   Beware of these Dangers  Markdown spell checkers don\u0026rsquo;t work well Sometimes the formatting gets messed up Markdown comes in multiple \u0026ldquo;flavors\u0026rdquo; Converters are not perfect   Where do we go from here?  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes  Template Documentation   Math In-line math: $z = \\frac{\\bar{x}-\\mu}{\\sigma}$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Questions? ","date":1580860800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580860800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2020-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"Create slides in Markdown with Academic","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you\u0026rsquo;ll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head() ```  renders as\nimport pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head()  Math Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file.\nTo render inline or block math, wrap your LaTeX math with $...$ or $$...$$, respectively.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$  renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left |\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right |^2}$$\nExample inline math $\\nabla F(\\mathbf{x}_{n})$ renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the \\\\\\\\ math linebreak:\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\\\\\ 1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$  renders as\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\\n1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ```  renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2]  An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ```  renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good!  An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ```  renders as\ngantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d  An example class diagram:\n```mermaid classDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() } ```  renders as\nclassDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() }  An example state diagram:\n```mermaid stateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*] ```  renders as\nstateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*]  Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else  renders as\n Write math example Write diagram example Do something else  Tables Represent your data in tables:\n| First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell |  renders as\n   First Header Second Header     Content Cell Content Cell   Content Cell Content Cell    Asides Academic supports a shortcode for asides, also referred to as notices, hints, or alerts. By wrapping a paragraph in {{% alert note %}} ... {{% /alert %}}, it will render as an aside.\n{{% alert note %}} A Markdown aside is useful for displaying notices, hints, or definitions to your readers. {{% /alert %}}  renders as\n A Markdown aside is useful for displaying notices, hints, or definitions to your readers.   Spoilers Add a spoiler to a page to reveal text, such as an answer to a question, after a button is clicked.\n{{\u0026lt; spoiler text=\u0026quot;Click to view the spoiler\u0026quot; \u0026gt;}} You found me! {{\u0026lt; /spoiler \u0026gt;}}  renders as\n Click to view the spoiler  You found me!    Icons Academic enables you to use a wide range of icons from Font Awesome and Academicons in addition to emojis.\nHere are some examples using the icon shortcode to render icons:\n{{\u0026lt; icon name=\u0026quot;terminal\u0026quot; pack=\u0026quot;fas\u0026quot; \u0026gt;}} Terminal {{\u0026lt; icon name=\u0026quot;python\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} Python {{\u0026lt; icon name=\u0026quot;r-project\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} R  renders as\n  Terminal\n Python\n R\nDid you find this page helpful? Consider sharing it üôå ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":["Kyle Jones"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":null,"categories":[],"content":"from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')  print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and JupyterLab  Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb  The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata ( front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post's title date: 2019-09-01 # Put any other Academic metadata here... ---  Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.  Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":["Kyle Jones","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Kyle Jones","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]