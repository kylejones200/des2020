<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Overview | Kyle Jones</title>
    <link>/courses/bana3363/</link>
      <atom:link href="/courses/bana3363/index.xml" rel="self" type="application/rss+xml" />
    <description>Overview</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 09 Sep 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Overview</title>
      <link>/courses/bana3363/</link>
    </image>
    
    <item>
      <title>Basic Statistics Concepts</title>
      <link>/courses/bana3363/1-basic-statistics-concepts/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/bana3363/1-basic-statistics-concepts/</guid>
      <description>&lt;h1 id=&#34;what-is-statistics&#34;&gt;What is statistics?&lt;/h1&gt;
&lt;p&gt;Statistics is the science of collecting, organizing, analyzing, and interpreting data in order to make decisions.&lt;/p&gt;
&lt;p&gt;Statistics is all around us. We hear statsitics on the news and we use statistics to make buisnes decisions.&lt;/p&gt;
&lt;p&gt;There are five general things you can do with statistics:&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Describe ‚Äî characterizing populations and samples using descriptive statistics, statistical intervals, correlation coefficients, graphics, and maps.&lt;/li&gt;
&lt;li&gt;Compare or Test ‚Äî detecting differences between statistical populations or reference values using simple hypothesis tests, and analysis of variance and covariance.&lt;/li&gt;
&lt;li&gt;Identify or Classify ‚Äî classifying and identifying a known or hypothesized entity or group of entities using descriptive statistics; statistical intervals and tests, graphics, and multivariate techniques such as cluster analysis.&lt;/li&gt;
&lt;li&gt;Predict ‚Äî predicting measurements using regression and neural networks, forecasting using time-series modeling techniques, and interpolating spatial data.&lt;/li&gt;
&lt;li&gt;Explain ‚Äî Explaining latent aspects of phenomena using regression, cluster analysis, discriminant analysis, factor analysis, and other data mining techniques.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;How do we do this? This table shows some statistical methods that can be used to accomplish different goals.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Objective&lt;/th&gt;
&lt;th&gt;Common Tools&lt;/th&gt;
&lt;th&gt;Example Applications&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Describe&lt;/td&gt;
&lt;td&gt;graphs; Descriptive Statistics&lt;/td&gt;
&lt;td&gt;Opinion Surveys; Demographic Surveys&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Compare or Test&lt;/td&gt;
&lt;td&gt;Statistical Tests&lt;/td&gt;
&lt;td&gt;Program Evaluation; Pharma/Education Effectiveness&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Identify or Classify&lt;/td&gt;
&lt;td&gt;Classification Trees; Data Mining&lt;/td&gt;
&lt;td&gt;Customer Churn&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Predict&lt;/td&gt;
&lt;td&gt;Regression&lt;/td&gt;
&lt;td&gt;Credit Score; Cost for House&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Explain&lt;/td&gt;
&lt;td&gt;Regression; ANOVA&lt;/td&gt;
&lt;td&gt;Research; Root cause analysis&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;why-do-we-need-statistics&#34;&gt;Why do we need statistics?&lt;/h2&gt;
&lt;p&gt;The world is full of uncertainty. Variability arises from two primary sources: variability in the natural process (inherent variability) and variability due to our imperfect measurement devices (measurement variability).&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;summary-statistics&#34;&gt;Summary Statistics&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Once we have data set (sample), want to determine basic summary statistics&lt;/li&gt;
&lt;li&gt;Quantitative summaries of data, give rough contours&lt;/li&gt;
&lt;li&gt;Useful for giving you a feel for what data actually look like&lt;/li&gt;
&lt;li&gt;Common summary statistics: mean, median, mode, quartiles, max, min&lt;/li&gt;
&lt;li&gt;Note: n = number of observations in our sample&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;variance-and-standard-deviation&#34;&gt;Variance and Standard Deviation&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Standard Deviation (like variance but better)&lt;/li&gt;
&lt;li&gt;However, variance is a squared term&lt;/li&gt;
&lt;li&gt;Difficult to understand ‚Äì squared ages? Squared income? (Difficult substantive meaning)&lt;/li&gt;
&lt;li&gt;Standard deviation: Square root of variance&lt;/li&gt;
&lt;li&gt;Another measure of spread&lt;/li&gt;
&lt;li&gt;Reverts back to original scale&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Samples are data sampled from an underlying population&lt;/li&gt;
&lt;li&gt;Use summary statistics to get a sense of the data&lt;/li&gt;
&lt;li&gt;Common examples: mean, median, variance, standard deviation&lt;/li&gt;
&lt;li&gt;Describe different properties, useful in different contexts&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;measures-of-central-tendency&#34;&gt;Measures of Central Tendency&lt;/h1&gt;
&lt;h2 id=&#34;sample-mean&#34;&gt;Sample mean&lt;/h2&gt;
&lt;p&gt;The sample mean (also called the arithmetic mean or sample average), is found by adding up all observations and dividing by the total number of observations.&lt;/p&gt;
&lt;p&gt;The symbol $\bar{x}$ is read as ‚Äúx-bar‚Äù&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We don‚Äôt always have to use the letter X. We can use any letter we want to indicate a variable (ùëã, ùëå, ùëä, ùëâ, etc.). Whatever we call the variable, a letter with a bar over it indicates the sample mean.&lt;/li&gt;
&lt;li&gt;$\bar{x}$ is a statistic, specifically, a point estimate for the population mean ‚Äúmu‚Äù: $\mu$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;some-key-terms&#34;&gt;Some key terms&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Mean: The Average - The mean of a data set is the sum of the data entries divided by the number of entries.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Median: The Middle - The median of a data set is the value that lies in the middle of the data when the data set is ordered from least to greatest. How do you find it? 1. Order the data entries. 2. Find the middle data entry. 3. Interpret the results in the context of the data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mode: The Most - The mode of the data set is the data entry that occurs with the greatest frequency. A data set can have one mode, more than one mode, or no mode. How do you find it? 1. Write the data in order. 2. Identify the entry, or entries, that occur with the greatest frequency. 3. Interpret the results in the context of the data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Range: Smallest-Largest&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Outlier: A data entry that is far removed from the other entries in the data set.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Descriptive Statistics: The branch of statistics that involves the organization, summarization, and display of data. Example: &amp;ldquo;For unmarried men, approximately 70% were alive at age 65&amp;rdquo; and &amp;ldquo;For married men, 90% were alive at 65.&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Inferential Statistics: The branch of statistics that involves using a sample to draw conclusions about a population. A basic tool in the study of inferential statistics is probability. Inferential statistics is a collection of techniques designed to use sample data to make generally-applicable statements about the natural process or population from which the data arose. Example: Possible Inference: Being married is associated with a longer life for men.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Probability: We typically aren‚Äôt interested in just the sample that we have (although the sample is still interesting).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Example: An insurance company sends out a survey to 100 policy holders asking them to rate their satisfaction with the service.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Managers aren‚Äôt just interested in only those 100 people. They want to be able to say something about the process (or population of customers) that is behind those customer satisfaction ratings&lt;/li&gt;
&lt;li&gt;That is, they want to infer (make a conclusion about) the process, so they can (perhaps) make adjustments to their website, coverage, customer service, etc.&lt;/li&gt;
&lt;li&gt;Probability helps us make the leap from a sample to population (or process) in an objective, mathematically justified way&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Population Mean&lt;/th&gt;
&lt;th&gt;Sample Mean&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$\mu = \frac{\sum x}{N}$&lt;/td&gt;
&lt;td&gt;$\bar{x} = \frac{\sum x}{n}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$N$ is the number of items in the population&lt;/td&gt;
&lt;td&gt;$n$ is the number of items in the sample (and the sample is drawn from the population)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;It has higher accuracy&lt;/td&gt;
&lt;td&gt;It has lower accuracy&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;The population is very large and not known, the population mean is an unknown constant&lt;/td&gt;
&lt;td&gt;It is an efficient and unbiased estimator of the population mean. The expected value of the sample mean is the population mean.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;measures-of-central-tendency-1&#34;&gt;Measures of Central Tendency&lt;/h1&gt;
&lt;p&gt;Estimators and Estimates:  a mathematical function of the sample that tell us how to calculate an estimate of a parameter form a sample. Smaller the variance, most efficient the estimator. Hence, we require to find what are the &amp;ldquo;good&amp;rdquo; estimators.&lt;/p&gt;
&lt;p&gt;Few vital criterial for goodness of an estimator are based on these properties:&lt;/p&gt;
&lt;h2 id=&#34;estimator-parameter-and-excel-command&#34;&gt;Estimator, Parameter, and Excel Command&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Term&lt;/th&gt;
&lt;th&gt;Estimator&lt;/th&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Excel Command&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Mean&lt;/td&gt;
&lt;td&gt;$\bar{x}$&lt;/td&gt;
&lt;td&gt;$\mu$&lt;/td&gt;
&lt;td&gt;&lt;code&gt;=AVERAGE(A1:A10)&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Variance&lt;/td&gt;
&lt;td&gt;$s^2$&lt;/td&gt;
&lt;td&gt;$\sigma^2$&lt;/td&gt;
&lt;td&gt;&lt;code&gt;=STDEV(A1:A:10)&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Median&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;=Median(A1:A:10)&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mode&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;=Mode(A1:A:10)&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Correlation&lt;/td&gt;
&lt;td&gt;R&lt;/td&gt;
&lt;td&gt;$\rho$&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;sample-mean-and-standard-deviation&#34;&gt;Sample Mean and Standard Deviation&lt;/h2&gt;
&lt;p&gt;Given a sample of $n$ numerical data values $x_{1}, x_{2},\ldots ,x_{n},$ the general first step is to calculate summary measures (or **statistics)** to get a feel for the population or process from which the data arises. Many summary measures exist, but the two most important are the sample mean and the sample standard deviation, which we now review.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;sample mean&lt;/strong&gt; of a sample of $n$ data values $x_{1},x_{2},&amp;hellip;,x_{n}$ is a measure of the center of a set of data and is defined as&lt;/p&gt;
&lt;p&gt;$$\overline{x}=\frac{\sum x_{i}}{n}$$&lt;/p&gt;
&lt;p&gt;where $\overline{x}$ is read as &amp;ldquo;x bar.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;sample standard deviation&lt;/strong&gt; of a sample of $n$ data values $x_{1},x_{2},&amp;hellip;,x_{n}$ is a measure of variability in the data and is defined as&lt;/p&gt;
&lt;p&gt;$$s=\sqrt{\frac{\sum (x_{i}-\overline{x})^{2}}{n-1}}$$&lt;/p&gt;
&lt;p&gt;A shortcut formula is&lt;/p&gt;
&lt;p&gt;$$s=\sqrt{\frac{\sum x_{i}^{2}-n\overline{x}^{2}}{n-1}}$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;sample variance&lt;/strong&gt; of a sample of $n$ data points $x_{1},x_{2},&amp;hellip;,x_{n}$ is the square of the standard deviation $s$. That is&lt;/p&gt;
&lt;p&gt;$$\text{Sample Variance }=\text{ }s^{2}$$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Researchers working for a hospitality trade publication surveyed a random sample of $10$ restaurant servers, asking them to report their best guess at their hourly wage (including tips). The data are given below.&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Tips&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$13$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$9.29$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$11.88$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$10.04$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$12.15$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$6.77$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$8.86$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$11.49$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$11.39$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$10.41$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;a). Calculate the sample mean hourly wage for the 10 restaurant servers.&lt;/p&gt;
&lt;p&gt;b). Calculate the sample standard deviation of the hourly wages.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A researcher is interested in the number of years that individuals employed in the financial services industry spend with their current employer. A sample of $11$ individuals ages $25-30$ who worked in the industry was collected. The data are shown below.&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Years&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$6.6$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$6.2$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$3.9$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$6.5$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$6.8$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$5.2$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$5.4$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$5.1$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$4.7$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$8.4$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$4.4$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;¬†a). Calculate the sample mean number of years spent with the current employer.&lt;/p&gt;
&lt;p&gt;b). Calculate the sample standard deviation of years spent with the current employer.&lt;/p&gt;
&lt;h2 id=&#34;numerical-graphical-techniques&#34;&gt;Numerical Graphical Techniques&lt;/h2&gt;
&lt;p&gt;Another early step in the analysis of data is the construction of &lt;strong&gt;graphical summaries&lt;/strong&gt;, which represent the data using a picture designed to highlight key attributes. One of the most important tools for analyzing numerical data is a &lt;strong&gt;histogram.&lt;/strong&gt; A histogram is a type of bar chart that divides the total range of the data into a number of &amp;ldquo;bins&amp;rdquo; of equal width and then sorts the data into the bins based upon those ranges. It answers the questions about 1). center (Where do the numbers tend to concentrate?), 2). spread (How variable is the data?) and 3). shape (In what pattern do the data tend to fall?)&lt;/p&gt;
&lt;p&gt;Figure 
&lt;a href=&#34;#calls&#34;&gt;[calls]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;calls&amp;rdquo;} ¬†below is an example of a histogram made from a set of waiting times (in minutes) for a technical support call center during a specific shift. From this chart, a supervisor could learn quite a lot about the customer service process. It seems most of the time, people are being served fairly quickly and the general pattern is that very short waiting times are common (notice that the tallest bar is above the $0-2$ range, and the bars decrease in height systematically from left to right). This pattern is precisely what a customer service supervisor wants to see! Were the pattern reversed (with small bars over small waiting times and large bars over large waiting times), the call center would need to be investigated to determine why customers are having to wait so long for service.&lt;/p&gt;
&lt;p&gt;Another graphical tool for numerical data is the &lt;strong&gt;box plot.&lt;/strong&gt; This plot typically shows five numbers:¬†the minimum value, the $25^{th}$ percentile, the median (or $50^{th}$ percentile), the $75^{th}$ percentile, and the maximum value&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. The $25^{th}$ percentile is the number $x_{0.25}$ such that (approximately) $25%$ of the data falls below it and (approximately) $75%$ of the data falls above it. In general, the $p^{th}$ percentile is a number $x_{p}$ that splits the data such that (approximately) $p%$ of the data falls below it and $(100-p)%$ of the data falls above it. **Outliers** , data values that are extremely small or large compared to the rest of the data, are typically plotted separately. Figure 
&lt;a href=&#34;#boxplot&#34;&gt;[boxplot]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;boxplot&amp;rdquo;} displays the waiting time data. From this plot, we can see that about $25%$ of the data falls below $0.75$ minutes$,$ about $50%$ falls below $1.5$ minutes, and about $75%$ falls below $4$ minutes. The largest non-outlier waiting time was about $9$ minutes, and one unlucky soul had to wait over $12$ minutes!&lt;/p&gt;
&lt;h2 id=&#34;probability-distributions&#34;&gt;Probability Distributions&lt;/h2&gt;
&lt;p&gt;The mathematical concept of &lt;strong&gt;probability theory&lt;/strong&gt; forms the basis of all statistical inference. It allows us to &amp;ldquo;make the leap&amp;rdquo; from the sample we have to the population or process that we are trying to study. ¬†In order for us to be able to talk about statistics in an efficient way, we must agree upon some definitions. In this section, we will review some concepts that you should be familiar with from earlier classes.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;random variable&lt;/strong&gt; $X$ is a numerical or categorical characteristic of a population or process whose range of possible values is known or assumed but for which the values are subject to random (or &amp;ldquo;chance&amp;rdquo;) variation.&lt;/p&gt;
&lt;p&gt;Any letter of the alphabet can be used to indicate a random variable. The most common choices are $X$, $Z$, and $Y$. The name of the random variable is written in capital letters, while a specific value that the random variable takes on is written in lowercase.&lt;/p&gt;
&lt;p&gt;[[5k]]{#5k label=&amp;quot;5k&amp;rdquo;}Let $X$ be number of people out of $30$ who finish a $5$K race. This is a random variable before the race occurs because we cannot say with certainty how many people will ultimately finish. We know that the possible values are ${0,1,2,\ldots ,30}$. If it turns out that $25$ people finish, then $x=25$.&lt;/p&gt;
&lt;h3 id=&#34;example-accidents&#34;&gt;Example: Accidents&lt;/h3&gt;
&lt;p&gt;Let $V$ be the number of accidents that occur on a particular length of highway on a Tuesday between 5 p.m. and 9 p.m. Before this time period occurs, $V$ is a random variable because we cannot say with certainty how many accidents will occur. We do know that the least number of accidents is $0,$ so we would say the possible values of $V$ are ${0,1,2,\ldots }$&lt;/p&gt;
&lt;h3 id=&#34;example-coffee&#34;&gt;Example: Coffee&lt;/h3&gt;
&lt;p&gt;Let $Y$ be the time (in seconds) you spend in line at a coffee shop on a Wednesday morning. This is a random variable before you visit the coffee shop. The smallest possible wait time is $0,$ while the largest possible time is not clearly defined without more assumptions. If you decide that you will leave, with coffee or without, if the wait is more than $15$ minutes, then the range of possible values is $0\leq y\leq 900$. If on this particular trip you wait $350.96$ seconds, then $y=350.96$, or about $5$ minutes and $51$ seconds.&lt;/p&gt;
&lt;h3 id=&#34;example-weight&#34;&gt;Example: Weight&lt;/h3&gt;
&lt;p&gt;Let $W$ be the number of pounds lost for an adult male over a six-week period of following a particular exercise program. This is a random variable before the six-week period is over because we cannot say for certain what the weight loss will be. We do know that it is possible to gain weight (through building muscle, for example) or to lose weight. Here it is a bit more difficult to assign a range to the random variable without knowing more about the study, but one reasonable choice would be $-35\leq y\leq 55$ ($-35$ implies that it is possible to gain $35$ pounds). If a particular man loses $35.4$ pounds, then $w=35.4$.&lt;/p&gt;
&lt;p&gt;Here are some additional important definitions.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;probability,&lt;/strong&gt; in practical (i.e., non-mathematically formal) terms can be thought of as a chance of something happening. The range of a valid probability for an event is $0$ (no chance of the event occurring) to $1$ (guaranteed for the event to occur). Probabilities cannot be negative.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;discrete random variable&lt;/strong&gt; is a random variable whose entire set of values and associated probabilities can be completely listed or at least counted.&lt;/p&gt;
&lt;p&gt;Examples 
&lt;a href=&#34;#5k&#34;&gt;[5k]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;5k&amp;rdquo;} and 
&lt;a href=&#34;#accidents&#34;&gt;[accidents]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;accidents&amp;rdquo;} above involve discrete random variables because the possibilities can be completely listed (example 
&lt;a href=&#34;#5k&#34;&gt;[5k]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;5k&amp;rdquo;}) or counted 
&lt;a href=&#34;#accidents&#34;&gt;[accidents]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;accidents&amp;rdquo;}.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;continuous random variable&lt;/strong&gt; is a random variable whose entire set of values cannot be listed because the possible values extend, in theory, to an infinite number of decimal places.&lt;/p&gt;
&lt;p&gt;Examples 
&lt;a href=&#34;#coffee&#34;&gt;[coffee]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;coffee&amp;rdquo;} and 
&lt;a href=&#34;#weight&#34;&gt;[weight]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;weight&amp;rdquo;} above involve continuous random variables because time and weight are measurements that can take on, in theory, an infinite number of decimal places depending upon how precise our measurement instruments are.&lt;/p&gt;
&lt;p&gt;Once we have defined a random variable, we need some way to assign the probabilities to its values. In general, how we do this depends on whether the random variable is discrete or continuous. There is some highly technical mathematics involved in that we are going to skip right over in favor of the following definition:&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;probability distribution&lt;/strong&gt; is a specification of the possible values of a random variable and their associated probabilities, made either by listing the possible values and their probabilities or by providing a function that gives probabilities for a collection of possible values of the random variable.&lt;/p&gt;
&lt;p&gt;When we write $P(X=x),$ this stands for &amp;ldquo;the probability that the random variable $X$ equals $x,&amp;ldquo;$ or, more simply, &amp;ldquo;the probability that $X$ equals $x.&amp;ldquo;$ For example, $P(X=4),$ referring to example 
&lt;a href=&#34;#5k&#34;&gt;[5k]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;5k&amp;rdquo;} above, would be read as &amp;ldquo;the probability that $X=4,&amp;ldquo;$ which in context is the probability that $4$ runners finish the $5K$ run.&lt;/p&gt;
&lt;p&gt;In general, how we assign probabilities depends on whether the random variable is discrete or continuous. Discrete probability distributions can be specified as two-column tables listing the possible values of the random variable and their associated probabilities, or by writing a function that outputs probabilities. Continuous random variables must be specified by writing a function $f(y)$ and the outputs of these functions are not probabilities, but &amp;ldquo;relative likelihoods&amp;rdquo; of various values.&lt;/p&gt;
&lt;p&gt;A discrete probability distribution for a random variable $X$ must meet the following criteria:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$\sum_{all\text{ }x}P(X=x)=1$ ¬†[probabilities must sum to $1$]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$0\leq P(X=x)\leq 1$ for all $x$ [probabilities are between $0$ and $1]$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A continuous distribution for a random variable $Y$ must meet the following criteria:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$f(y)\geq 0$ for all possible $y$ ¬†¬†[graph of the function is above the horizontal axis]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Total area under the curve $f(y)=1$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Referring to example 
&lt;a href=&#34;#5k&#34;&gt;[5k]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;5k&amp;rdquo;}, one way to define the probability distribution of $X$ would be to list the values of $x$ and their probabilities as in Table 
&lt;a href=&#34;#TableKey&#34;&gt;[TableKey]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;TableKey&amp;rdquo;} below (the three dots notation, $\vdots$ ,means that the rows between $3$ and $29$ have been left out for space reasons)$$. Note that the probabilities listed are just examples.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;$x$&lt;/th&gt;
&lt;th&gt;$P(X=x)$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$0$&lt;/td&gt;
&lt;td&gt;$0.000001$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$1$&lt;/td&gt;
&lt;td&gt;$0.000003$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$2$&lt;/td&gt;
&lt;td&gt;$0.00004$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$3$&lt;/td&gt;
&lt;td&gt;$0.00005$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$\vdots$&lt;/td&gt;
&lt;td&gt;$\vdots$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$29$&lt;/td&gt;
&lt;td&gt;$0.10$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$30$&lt;/td&gt;
&lt;td&gt;$0.05$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;All of the probabilities above must be between $0$¬†and $1$ , and they must add up to exactly $1$. Another way to define this distribution would be to use a function. If we assume that each runner has the same probability of &amp;ldquo;success&amp;rdquo; $p$¬†(finishing the race) and that they perform independently of other runners, we might use the binomial distribution to describe the probabilities of $x$¬†number of runners finishing:&lt;/p&gt;
&lt;p&gt;$$P(X=x)=\frac{30!}{(30-x)!x!}p^{x}(1-p)^{30-x}$$&lt;/p&gt;
&lt;p&gt;where the symbol $&amp;rdquo;!&amp;ldquo;$¬†denotes the factorial function (for example $5!=5\times 4\times 3\times 2\times 1)$.&lt;/p&gt;
&lt;p&gt;Referring to example 
&lt;a href=&#34;#weight&#34;&gt;[weight]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;weight&amp;rdquo;}, we might assume that the amount of weight lost could be described by a bell-shaped curve with a peak value at $10$ pounds, such as the one in Figure 
&lt;a href=&#34;#wt_dist&#34;&gt;[wt_dist]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;wt_dist&amp;rdquo;}.&lt;/p&gt;
&lt;p&gt;The particular function that produces this graph is&lt;/p&gt;
&lt;p&gt;$$f(y)=\frac{1}{15\sqrt{2\pi }}e^{\frac{-(y-10)^{2}}{450}}$$&lt;/p&gt;
&lt;p&gt;The graph meets the criterion that it must lie above the horizontal axis. Further, if you were to use calculus to find the area under the curve, it would be exactly $1$.¬†The distribution shown in Figure 
&lt;a href=&#34;#wt_dist&#34;&gt;[wt_dist]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;wt_dist&amp;rdquo;} is known as the normal distribution, and we will be seeing it again and again.&lt;/p&gt;
&lt;p&gt;Many times, a histogram of the data can suggest a particular probability distribution. Look again at Figure 
&lt;a href=&#34;#calls&#34;&gt;[calls]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;calls&amp;rdquo;}, which shows the distribution of waiting times at a call center. This &amp;ldquo;downstairs&amp;rdquo; pattern suggests another widely used continuous probability distribution called the exponential distribution. Figure 
&lt;a href=&#34;#exp_dist&#34;&gt;[exp_dist]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;exp_dist&amp;rdquo;} ¬†shows the exponential distribution drawn over the histogram.&lt;/p&gt;
&lt;p&gt;Once we have a random variable and its distribution, we can talk about its properties. The two most common properties to examine are its mean ( &lt;strong&gt;expected&lt;/strong&gt; or average) value and its degree of variability (variance or standard deviation). We will only look at the definitions of these quantities for the discrete case because integral calculus is required to define them for continuous random variables.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;expected value&lt;/strong&gt; (or &lt;strong&gt;mean)&lt;/strong&gt; of a discrete random variable $X,$ denoted by either $E(X)$ or $\mu$ (and pronounced &amp;ldquo;mu&amp;rdquo;)$,$ is defined as&lt;/p&gt;
&lt;p&gt;$$\mu =E(X)=\sum_{all\text{ }x}xP(X=x)$$&lt;/p&gt;
&lt;p&gt;That is, it is a weighted (by the probabilities) sum of the possible values of the random variable.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;variance&lt;/strong&gt; of a discrete random variable $X,$ denoted by either $V(X)$ or $\sigma ^{2}$ (and pronounced &amp;ldquo;sigma squared&amp;rdquo;) is defined as&lt;/p&gt;
&lt;p&gt;$$V(X)=\sum_{all\text{ }x}(x-\mu )^{2}P(X=x)$$&lt;/p&gt;
&lt;p&gt;A shortcut formula is
$$E(X^{2})-[E(X)]^{2}$$&lt;/p&gt;
&lt;p&gt;The standard deviation, denoted as either $StDev(X)$ or $\sigma$ (read as &amp;ldquo;sigma&amp;rdquo;)$,$ is defined as $\sqrt{V(X)}$.&lt;/p&gt;
&lt;p&gt;Basically, $E(X)$ is the population version of $\overline{x}$ and $V(X)$ is the population version of $s^{2}$.&lt;/p&gt;
&lt;p&gt;The following facts will come in handy later.&lt;/p&gt;
&lt;p&gt;(math fact) If $X$ and $Y$ are random variables and $a$ and $b$ are any constants (fixed values), then all of the following statements about expected value and variance are true:&lt;/p&gt;
&lt;p&gt;$$\begin{gathered} E(aX+b)=aE(X)+b \ E(aX+bY)=aE(X)+bE(Y) \ V(aX+b)=a^{2}V(X) \ V(aX+bY)=a^{2}V(X)+b^{2}V(Y)+2ab\rho StDev(X)StDev(Y)\end{gathered}$$&lt;/p&gt;
&lt;p&gt;That is, you get to pull the constants out in front of the mean and variance operators, but with variance you square the constant while with expected value, you just pull it out. The mean of a constant is just the constant, while the variance of a constant is $0$.&lt;/p&gt;
&lt;p&gt;The Greek letter $\rho$ is pronounced &amp;ldquo;row&amp;rdquo; and describes the &lt;strong&gt;correlation&lt;/strong&gt; (or linear relationship) between two numerical variables. We will return to this concept toward the end of the course.&lt;/p&gt;
&lt;p&gt;The following questions address the topics above. Most, if not all, of these questions are review from BANA 2372.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The General Social Survey asked the following question : In the last 12 months, how many times have you been injured on the job?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;a). What must $P(X=12)$ be for this to be a valid probability distribution?&lt;/p&gt;
&lt;p&gt;b). Find $\mu =E(X),$ the mean of the distribution of $X$.&lt;/p&gt;
&lt;p&gt;c). Find $\sigma ^{2}=V(X),$ the variance of the distribution of $X$.&lt;/p&gt;
&lt;p&gt;e). Find $\sigma =StDev(X),$ the standard deviation of the distribution of $X$.&lt;/p&gt;
&lt;p&gt;f). Find $E(10X+2)$ using the rules of expected value.&lt;/p&gt;
&lt;p&gt;g). Find $V(9X-7)$ using the rules of variance.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;(Real-world application). A potential customer for a $50,000$ fire insurance policy has a home in an area that, according to past experience, may sustain a total loss in a given year with a probability of $0.001$ and a $50%$ loss with a probability of $0.01$. There is a $0.989$ chance that the customer will make no claim in the coverage year. The company also offers renter&amp;rsquo;s insurance. This same potential customer wants a $20,000$ policy. Based on historical data the company has for the area, the probability of a total loss due to fire is $0.005,$ the probability of a $50%$ loss is $0.015,$ and the probability of no loss is $0.98$. Let $X$ be the company&amp;rsquo;s loss on the fire insurance policy and $Y$ be the company&amp;rsquo;s loss on the renter&amp;rsquo;s insurance policy.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;a). Find the expected loss for the fire policy and for the renter&amp;rsquo;s insurance policy individually. That is, find $E(X)$ and $E(Y)$.&lt;/p&gt;
&lt;p&gt;b). Find the combined expected loss for the company on the two policies. That is, find $E(X+Y)$.&lt;/p&gt;
&lt;p&gt;c). Suppose the company combines the premiums for the two policies and charges $600$ per year. Based on your answer to (b), is this a good business decision for the company?&lt;/p&gt;
&lt;p&gt;d). Suppose the company offers the fire policy to $20$ customers and the renter&amp;rsquo;s insurance to $10$ customers in the same area. Find the combined expected loss for the company, assuming each customer has identical loss probabilities. That is, Find $E(20X+10Y)$.&lt;/p&gt;
&lt;h2 id=&#34;binomial-distribution&#34;&gt;Binomial Distribution&lt;/h2&gt;
&lt;p&gt;In this section, we review in more detail one of the most important discrete distributions.&lt;/p&gt;
&lt;p&gt;The binomial distribution is used to model the situation in which there are $n$ identical trials or experiments and you want to know the chance of getting $x$ &amp;ldquo;successes&amp;rdquo; or &amp;ldquo;events&amp;rdquo; out of those $n$ trials when the probability of success is known to be $p$. If $X$ has the binomial distribution, then&lt;/p&gt;
&lt;p&gt;$$P(X=x)=\frac{n!}{(n-x)!x!}p^{x}(1-p)^{n-x}$$&lt;/p&gt;
&lt;p&gt;where $!$ is the factorial operation. For example $5!=5\times 4\times 3\times 2\times 1$.&lt;/p&gt;
&lt;p&gt;This can also be written as&lt;/p&gt;
&lt;p&gt;$$P(X=x)=\binom{n}{x}p^{x}(1-p)^{n-x}$$&lt;/p&gt;
&lt;p&gt;where $\binom{n}{x}=\frac{n!}{(n-x)!x!}$ and is read as &amp;ldquo;$n$ choose $x$&amp;rdquo;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;(Real-world application). An operations manager for Umbrella Corporation, a consumer products company, is in charge of determining if one of its current suppliers should be dropped because of an unacceptably high defect rate in the chemical it is sending to the company. The supplier provides the chemical in bottles. Management personnel from both companies have agreed that $1$ bottle out of $100$ (a defect rate of $0.01)$ is an acceptable risk. Suppose the company receives a batch of $50$ bottles and finds that $2$ bottles contain defective product.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;a). What is the probability that Umbrella would find exactly $2$ defective bottles in a shipment of $50$, if the real defect rate is $0.01$ as promised?&lt;/p&gt;
&lt;p&gt;b). What is the probability that Umbrella would find more than $1$ defect?&lt;/p&gt;
&lt;p&gt;c). Using your answer to (b), as a supply chain manager, what is your recommendation for keeping or dropping the supplier?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Suppose you are investigating whether there is a difference in taste between organic milk and regular milk. You recruit $10$ random volunteers and ask each one individually after tasting to tell you which of two unmarked glasses contains the organic milk. Let X be the number of correct guesses. If there really were no difference in taste between organic milk and regular milk, what would be the probability of $8$ or more correct guesses, to three decimal places?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The marketing department of Low-Brau, a microbrewery that makes what it considers an excellent light beer, asked $7$ volunteers, individually, to taste $10$ types of light beer (one of which was the company&amp;rsquo;s brand and the others being competitor brands) and report which one they liked the best. Out of those $7$ people, $3$ said they liked Low-Brau&amp;rsquo;s light beer the best. The taste test was &amp;ldquo;blind&amp;rdquo; in that the volunteers did not know beforehand which of the $7$ beers they tasted was Low-Brau&amp;rsquo;s. If there is no difference in the taste of the light beers, what is the chance that $3$ out of $7$ people would be able to rate Low-Brau&amp;rsquo;s as the best?&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;descriptive-statistics&#34;&gt;Descriptive Statistics&lt;/h1&gt;
&lt;p&gt;Definition: Descriptive statistics is a collection of techniques for organizing, summarizing, and presenting a sample of data in a convenient, informative way.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If you follow sports, you know this definition:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Completed passes, yards gained by passing, rushing touchdowns&lt;/li&gt;
&lt;li&gt;Turnovers, rebounds, free-throws made&lt;/li&gt;
&lt;li&gt;Batting average, on-base percentage, runs produced&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Business/Economics&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Average time customers spend on hold in a call center&lt;/li&gt;
&lt;li&gt;Proportion of prospects at a car dealership who end up purchasing&lt;/li&gt;
&lt;li&gt;Variability in stock returns for the Dow Jones for the past year&lt;/li&gt;
&lt;li&gt;Average amount of debt students carry after graduation&lt;/li&gt;
&lt;li&gt;Average market value of homes in a neighborhood&lt;/li&gt;
&lt;li&gt;Proportion of members of a focus group who think favorably about a new product&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;heading&#34;&gt;&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Bias&lt;/li&gt;
&lt;li&gt;Variance&lt;/li&gt;
&lt;li&gt;Mean Square Error&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;histograms&#34;&gt;Histograms&lt;/h1&gt;
&lt;p&gt;A histogram is a bar graph that represents the frequency distribution of a dataset. It is a way to visualize a pattern in a numerical data set. It answers the questions about&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Center (where the numbers tend to fall)&lt;/li&gt;
&lt;li&gt;Spread (the degree of variability in the data) and&lt;/li&gt;
&lt;li&gt;Shape (the pattern in which the numbers tend to fall)&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;A histogram can be used to estimate continuous probability distributions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Properties of a histogram:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Horizontal scale measures the data values&lt;/li&gt;
&lt;li&gt;Vertical scale measures the frequencies of the bins&lt;/li&gt;
&lt;li&gt;Consecutive bars touch&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h1 id=&#34;histogram-example-call-durations&#34;&gt;Histogram Example: Call Durations&lt;/h1&gt;
&lt;p&gt;Suppose you work at a call center for a cable company. Your job is to help customers troubleshoot their technology problems. Many people are bad with technology, and so the call center stays busy. Your manager wants to make sure that people aren‚Äôt on hold for too long; otherwise they might bail and go to another provider. Your department keeps track of the hold times for customers. Each week, your manager reviews the data to see if changes need to be made. To do this, she makes a histogram of the duration times, which we will call Y.&lt;/p&gt;
&lt;p&gt;Here we see that most customers get served pretty quickly (the times between 0 and 2 are far more frequent). However, some customers have to wait a lot longer.&lt;/p&gt;
&lt;p&gt;Suppose we want to find the probability that a customer has to wait between 1 and 4 minutes for service. How can we do that?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/Continuous-Distributions_CURRENT0.png&#34; alt=&#34;phone graph&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like we might be able to fit a smooth curve to the histogram. For N = 100 calls, it doesn‚Äôt fit so well, but for n = 10,000 calls, it gets better.&lt;/p&gt;
&lt;p&gt;This smooth curve is an example of a continuous distribution function, and using it, we can find the probabilities that a caller has to wait between 1 and 4 minutes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/Continuous-Distributions_CURRENT1.png&#34; alt=&#34;phone graph&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;img/Continuous-Distributions_CURRENT2.png&#34; alt=&#34;phone graph 2&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;estimating-probabilities-using-a-histogram&#34;&gt;Estimating Probabilities Using a Histogram&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Find Relative Frequencies&lt;/li&gt;
&lt;li&gt;Estimate the Density&lt;/li&gt;
&lt;li&gt;Probabilities Are Estimated as (width of the rectangle) * density&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;img/Continuous-Distributions_CURRENT3.png&#34; alt=&#34;phone graph 3&#34;&gt;&lt;/p&gt;
&lt;p&gt;This is an estimate of the probability without resorting to calculus, which gives exact answers.&lt;/p&gt;
&lt;h1 id=&#34;what-is-a-scatter-plot&#34;&gt;What is a scatter plot?&lt;/h1&gt;
&lt;p&gt;A way to graph paired data sets where the ordered pairs are graphed as points in a coordinate plane. Scatter plots show the relationship between two quantitative variables.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;central-limit-theorem&#34;&gt;Central limit theorem&lt;/h1&gt;
&lt;p&gt;In the central limit theorem:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No matter the distribution* the more samples, closer to normal&lt;/li&gt;
&lt;li&gt;the bigger the samples, the closer to normal&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;measures-of-dispersion&#34;&gt;Measures of dispersion&lt;/h1&gt;
&lt;p&gt;Population variance: The mean of the squares of the deviations.
Population standard deviation: The square root of the population variance.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Find the mean and each deviation.&lt;/li&gt;
&lt;li&gt;Square each deviation.&lt;/li&gt;
&lt;li&gt;Find the sum of the squares.&lt;/li&gt;
&lt;li&gt;Divide by the number of data entries.&lt;/li&gt;
&lt;li&gt;Take the square root of the population variance.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Standard Deviation: $\sigma = \sqrt{\frac{\Sigma(x-\bar{x})^2}{n}}$&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;empirical-ruleor-68-95-997-rule&#34;&gt;Empirical Rule(Or 68-95-99.7 Rule)&lt;/h1&gt;
&lt;p&gt;For data with a (symmetric) bell-shaped distribution, the standard deviation has the following characteristics.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;About 68% of the data lie within one standard deviation of the mean.&lt;/li&gt;
&lt;li&gt;About 95% of the data lie within two standard deviations of the mean.&lt;/li&gt;
&lt;li&gt;About 99.7% of the data lie within three standard deviations of the mean.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h1 id=&#34;quartiles-of-a-data-set&#34;&gt;Quartiles of a Data Set&lt;/h1&gt;
&lt;p&gt;What are quartiles?&lt;/p&gt;
&lt;p&gt;The three quartiles, $Q_1$ ,$Q_2$ ,and $Q_3$ , approximately divide an ordered data set into four equal parts. About one quarter of the data fall on or below the first quartile $Q_1$.About one half of the data fall on or below the second quartile $Q_2$.About three quarters of the data fall on or below the third quartile $Q_3$.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;interquartile-range-iqr&#34;&gt;Interquartile Range (IQR)&lt;/h1&gt;
&lt;p&gt;The interquartile range (IQR) of a data set is a measure of variation that gives the range of the middle 50% of the data. It is the difference between the third and first quartiles.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;box-and-whisker-plot&#34;&gt;Box-and-Whisker Plot&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Find the five-number summary of the data set. (minimum entry, first quartile, median, third quartile, maximum entry)&lt;/li&gt;
&lt;li&gt;Construct a horizontal scale that spans the range of the data.&lt;/li&gt;
&lt;li&gt;Plot the five numbers above the horizontal scale.&lt;/li&gt;
&lt;li&gt;Draw a box above the horizontal scale from
$Q_1$ to $Q_3$ and draw a vertical line in the box at
$Q_2$.&lt;/li&gt;
&lt;li&gt;Draw whiskers from the box to the minimum and maximum entries.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h1 id=&#34;the-shapes-of-frequency-distributions&#34;&gt;The Shapes of Frequency Distributions&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;img/paranormal.png&#34; alt=&#34;Distributions&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;skewed-distributions&#34;&gt;Skewed Distributions&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;img/skewed_dist.jpeg&#34; alt=&#34;Skewed&#34;&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Skew&lt;/th&gt;
&lt;th&gt;Neg/Positive&lt;/th&gt;
&lt;th&gt;Meaning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Left&lt;/td&gt;
&lt;td&gt;Negatively Skewed&lt;/td&gt;
&lt;td&gt;The tail of the graph extends more to the left. More of the data entries are clumped on the right.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Right&lt;/td&gt;
&lt;td&gt;Positively Skewed&lt;/td&gt;
&lt;td&gt;The tail of the graph extends more to the right. More of the data entries are clumped on the left.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h1 id=&#34;heading-1&#34;&gt;&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;img/Continuous_Distributions_and_the_Normal_Distribution_CURRENT4.png&#34; alt=&#34;normal&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;standard-normal-distribution&#34;&gt;Standard normal distribution&lt;/h1&gt;
&lt;p&gt;It is a normal distribution with a mean of 0 and a standard deviation of 1.&lt;/p&gt;
&lt;p&gt;Every normal distribution can be standardized using the following formula:&lt;/p&gt;
&lt;p&gt;$$\alpha = \frac{x-/mu}{\sigma}$$&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;discrete-probability-distributions&#34;&gt;Discrete Probability Distributions&lt;/h1&gt;
&lt;p&gt;Definition: A discrete probability distribution is a listing or specification of the possible values of a random variable and their associated probabilities
Notation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An upper-case letter (like X) will represent the name of the random variable,&lt;/li&gt;
&lt;li&gt;Its lower-case counterpart  (like x) will represent the value of the random variable.&lt;/li&gt;
&lt;li&gt;The probability that the random variable X will equal x is P(X = x)&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Distribution&lt;/th&gt;
&lt;th&gt;Definition&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Normal distribution&lt;/td&gt;
&lt;td&gt;Also known as Gaussian distribution or Bell Curve. it is mostly used in regression analysis. When data is normally distributed then distribution is symmetric and mean = median = mode.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Uniform or Rectangular&lt;/td&gt;
&lt;td&gt;All entries in the distribution have approximately equal frequencies.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Symmetric&lt;/td&gt;
&lt;td&gt;If a vertical line is drawn through the middle of the graph, then the two halves would be mirror images.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Continuous Distributions&lt;/td&gt;
&lt;td&gt;A distribution for a continuous random variable, say, Y, is written as f(y). It is a smooth function with points on the curve indicating ‚Äúrelative likelihoods‚Äù of observations. It must meet two criteria: 1. f(y) ‚â• 0 for all possible values that the random variable can take on. 2. The total area under the curve is exactly 1. Probabilities are calculated as areas under the curve.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;There are many ways of calculating percentiles from data, and no method is truly the standard. You can read more about this issue here: &lt;a href=&#34;http://www.dummies.com/how-to/content/how-to-calculate-percentiles-in-statistics.html&#34;&gt;http://www.dummies.com/how-to/content/how-to-calculate-percentiles-in-statistics.html&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Sampling Distributions &amp; Central Limit Theorem</title>
      <link>/courses/bana3363/3-1-sampling-distributions/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/bana3363/3-1-sampling-distributions/</guid>
      <description>&lt;h2 id=&#34;sampling-distributions&#34;&gt;Sampling Distributions&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s get a definition out of the way. This comes very close to the &amp;ldquo;official&amp;rdquo;¬†Google definition:&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    A &lt;strong&gt;sample&lt;/strong&gt; is a small collection of objects whose properties are thought to correspond closely with those of a large (or even infinite) collection of items or quantities that is the main interest of the researcher.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This large collection of items or quantities about which we would like to make reasonable statements is called a &lt;strong&gt;population&lt;/strong&gt;, or, more accurately, a &lt;strong&gt;statistical process&lt;/strong&gt;, or just &lt;strong&gt;process&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;You will see the word &amp;ldquo;population&amp;rdquo; used more often than &amp;ldquo;process&amp;rdquo; in most of the material you read despite &amp;ldquo;process&amp;rdquo; being a more accurate description of how a statistician views the natural world. The word &amp;ldquo;population&amp;rdquo; evokes images of a single, fixed collection of objects that we sample from. In much of statistics, and especially in forecasting and time series, the population view is simply wrong. In many cases, what a researcher is trying to study is an ever-changing process with many factors that can influence the actual data the researcher can see. For these situations, the process view is better &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;For example, suppose wish to study income of beginning accounting professionals in the U. S. Many factors interact to produce the income population: labor markets, the government, regional events, education, etc.. You take a sample of $300$ professionals and invite them to complete a survey about income. Your goal might be to estimate the &amp;ldquo;population&amp;rdquo; mean beginning accounting salary, $\mu$, using the sample of size $300$. There are two main problems. First, the population you want to make a conclusion about (all beginning accountants) is not the population you drew from (all beginning accountants who don&amp;rsquo;t mind filling out a survey). Furthermore, some respondents no doubt will (unintentionally or intentionally) give you incorrect responses, so the &amp;ldquo;population&amp;rdquo; you are sampling from is again not quite what you want. Taking the process view, though, you can view $\mu$ as the mean of the process that produces the actual accounting salary data you see, and you can use inference to make conclusions about that process. If your study is designed properly, your results will be a reasonable representation of the &amp;ldquo;population&amp;rdquo; you are really interested in, beginning accounting graduates.&lt;/p&gt;
&lt;p&gt;For purposes of doing the work in this class, however, you can use phrases such as &amp;ldquo;the population mean&amp;rdquo; (I probably will, just out of habit) and be fine. Now we need to be more specific about what a sample is.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;random sample&lt;/strong&gt; is a collection of $n$ independent random variables $Y_{1},Y_{2},\ldots,Y_{n}$ that are assumed to have been &amp;ldquo;drawn&amp;rdquo; from the same probability distribution $f(y|\mathbf{\theta ),}$where $\mathbf{\theta }$ is a generic collection of parameters. For example, if we are taking a sample from a normal distribution $\mathbf{\theta ={}\mu,\sigma }$.&lt;/p&gt;
&lt;p&gt;Figure 
&lt;a href=&#34;#sampling&#34;&gt;[sampling]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;sampling&amp;rdquo;} illustrates this idea. Essentially, before the researcher gathers data, he or she imagines that some (usually unknown) probability distribution produces data from which he or she will draw a single sample of size, say, $300$. When the data are actually gathered, the researcher takes the view that he or she is observing one possible realization of those $300$ random variables.&lt;/p&gt;
&lt;p&gt;The key to understanding statistical inference is that &lt;em&gt;different samples produce different values of calculated statistics such as the sample mean and sample standard deviation, but these values behave in a predictable way.&lt;/em&gt; Although we only work with one sample, we use the properties, we use the fact that the samples behave in a predictable way to make the leap from any one sample to the population or process of interest.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;sampling distribution&lt;/strong&gt; is the distribution of a calculated statistic (for example, the sample mean) when we imagine taking repeated samples from the same process and calculating the statistic over and over.&lt;/p&gt;
&lt;p&gt;There is an entire distribution of values because each random sample of size $n$ will have different data in it. So, a statistic is actually a random variable too. Therefore, when we talk about a statistic like the sample mean in general terms, we will use a capital letter, $\overline{Y}$, and when we talk about a specific value of the statistic, like $2.3$, we will use lowercase, as in $\overline{y}=2.3$. A good illustration of this concept is found here &lt;a href=&#34;http://onlinestatbook.com/stat_sim/sampling_dist/&#34;&gt;http://onlinestatbook.com/stat_sim/sampling_dist/&lt;/a&gt;, courtesy of our friends at Rice University.&lt;/p&gt;
&lt;p&gt;We can summarize the distribution like we have before using expected value and variance. It turns out that if we choose the right statistic, we can make some powerful conclusions about the population. Here are some cool facts.&lt;/p&gt;
&lt;p&gt;[[samp_dist]]{#samp_dist label=&amp;quot;samp_dist&amp;rdquo;} For a random sample of size $n$ from a process (or population), the sample mean, $\overline{Y}$, is a random variable with mean $=E(\overline{Y})=% %TCIMACRO{\U{3bc} }% %BeginExpansion \mu %EndExpansion$ (the process/population mean) and variance $V(\overline{Y})=$ $\frac{% \sigma ^{2}}{n}$, the population variance divided by $n$.&lt;/p&gt;
&lt;p&gt;Stand by for the most important fact in statistics.¬†It&amp;rsquo;s so important that it deserves its own section.&lt;/p&gt;
&lt;h2 id=&#34;sampling-distribution-of-the-sample-mean&#34;&gt;Sampling Distribution of the Sample Mean&lt;/h2&gt;
&lt;h3 id=&#34;the-central-limit-theorem&#34;&gt;The Central Limit Theorem&lt;/h3&gt;
&lt;p&gt;The sampling distribution of the sample mean $\overline{Y}$ is approximately a normal distribution with mean $=\mu$ and variance $\frac{\sigma ^{2}}{n}$, &lt;em&gt;no matter what the original distribution looks like&lt;/em&gt;, as the sample size $n$ gets large.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Large&amp;rdquo; here technically refers to letting the sample size go to infinity, but in most practical cases, the Central Limit Theorem (CLT) applies when $n$ is as small as $30$.&lt;/p&gt;
&lt;p&gt;If we assume the sample is drawn from a normal distribution, then the distribution of $\overline{Y}$ is normal regardless of sample size.&lt;/p&gt;
&lt;p&gt;The CLT is the reason we don&amp;rsquo;t really have to worry about what the population distribution looks like in Figure #sampling. It says that the distribution can take &lt;em&gt;any shape&lt;/em&gt; and the distribution of the sample average will still be approximately normal. The CLT is what makes most of statistics work for practical problems.&lt;/p&gt;
&lt;p&gt;We can use the distribution of the sample mean to make conclusions about a sample. For example, we can ask, what is $P(\overline{Y}&amp;gt;4)?$ To answer this question, we can use our old friend the $z$-score. Remember that a $z$-score is, in general,&lt;/p&gt;
&lt;p&gt;$$\frac{\text{observation }-\text{ mean}}{\text{standard deviation}}$$.&lt;/p&gt;
&lt;p&gt;Z-scores for sample averages are found by subtracting the mean and dividing by the standard deviation, which are given in Theorem 
&lt;a href=&#34;#samp_dist&#34;&gt;[samp_dist]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;samp_dist&amp;rdquo;}:&lt;/p&gt;
&lt;p&gt;$$Z=\frac{\overline{Y}-\mu }{\sigma /\sqrt{n}}$$&lt;/p&gt;
&lt;p&gt;Here are some examples to try out.&lt;/p&gt;
&lt;p&gt;The foreman of a bottling plant has observed that the amount of soda in each &amp;ldquo;$32$-ounce&amp;rdquo;¬†bottle is actually a normally distributed random variable, with a mean of $32$ ounces and a standard deviation of $0.3$ ounce. If the foreman is doing quality control by taking samples of $n$ $=4$ bottles from the line and calculating the average fill, would a reading of $32.5$ be unusually high? Answer by finding $P(\overline{Y}&amp;gt;32.5)$. How would your answer change if we took away the assumption that the amount of soda in each bottle comes from a normal distribution?&lt;/p&gt;
&lt;p&gt;You take a random sample of size $50$ from a population with mean $\mu =16$ and standard deviation $\sigma = 8$.&lt;/p&gt;
&lt;p&gt;a). What are the mean and standard deviation of $\overline{Y}$, the sample mean?&lt;/p&gt;
&lt;p&gt;b). Sketch the distribution, indicating the intervals within which $68%$, $% 95%$, and $99.7%$ of the observations are expected to lie.&lt;/p&gt;
&lt;p&gt;c). Find $P(\overline{Y}&amp;gt;18)$&lt;/p&gt;
&lt;p&gt;d). Find $P(17&amp;lt;\overline{Y}&amp;lt;19.5)$&lt;/p&gt;
&lt;p&gt;e). Explain why all of these probabilities are approximate.&lt;/p&gt;
&lt;p&gt;In a random sample of $20$ college students, the average reported working time was $15.4$ hours per week. A researcher claims that the true working time of college students is normally distributed with a mean of $16$ hours and a standard deviation of $2.5$ hours. Let $\overline{Y}$ denote the sample mean.&lt;/p&gt;
&lt;p&gt;a). What is $P(\overline{Y}&amp;lt;15.4)$?&lt;/p&gt;
&lt;p&gt;b). Using your answer to (a), how reasonable do you think the researcher&amp;rsquo;s claim is?&lt;/p&gt;
&lt;p&gt;c). Using the $68-95-99.7$ rule, find a set of sample averages that would be unusual to see (either &amp;ldquo;too large&amp;rdquo; or &amp;ldquo;too small&amp;rdquo;) if the true mean and standard deviation of college student working hours were as the researcher claims.&lt;/p&gt;
&lt;p&gt;In the next section, we will discuss how we can use the sampling distribution of the sample mean to make inferences about $\mu $.&lt;/p&gt;
&lt;h2 id=&#34;interval-inference-about-a-single-population-process-mean&#34;&gt;Interval Inference about a Single Population (Process) Mean&lt;/h2&gt;
&lt;h3 id=&#34;basic-concepts&#34;&gt;Basic Concepts&lt;/h3&gt;
&lt;p&gt;Our ultimate goal is to obtain a reasonable &amp;quot; guess&amp;rdquo;¬†at the true value of a process parameter (in this section, $\mu$) using a sample of data. Before we do that, we have to introduce some terms.&lt;/p&gt;
&lt;p&gt;A**¬†point estimator** for a parameter is a particular function of the random sample that gives a single number that we hope is &amp;quot; close&amp;rdquo;¬†to the true value of the parameter.&lt;/p&gt;
&lt;p&gt;An &lt;strong&gt;interval estimator&lt;/strong&gt; is a range of values, constructed using observed data, within which the parameter is believed to fall.&lt;/p&gt;
&lt;p&gt;This sounds very technical, but it really is not so bad. An example of a point estimator is the sample mean: $\overline{Y}=\frac{\sum Y_{i}}{n}$. Notice how it is a function of the data (the values of $Y_{i})$. In practice, we typically use the point estimator to construct an interval estimator.&lt;/p&gt;
&lt;p&gt;In theory, there are an infinite number of point estimators we could pick to estimate $\mu$, but it turns out that only one, $\overline{Y}$, is &amp;ldquo;the best.&amp;rdquo; The following criteria are used to judge whether a point estimator is &amp;ldquo;good:&amp;rdquo;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unbiasedness&lt;/strong&gt; &amp;ndash; The average of the estimator is equal to the population parameter.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Consistency&lt;/strong&gt; &amp;ndash; The estimator gets closer and closer to the true population parameter as the sample size increases&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relative efficiency&lt;/strong&gt; &amp;ndash;Given two different estimators with the same sample size, we choose the one with the smaller variance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We know from Theorem 
&lt;a href=&#34;#samp_dist&#34;&gt;[samp_dist]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;samp_dist&amp;rdquo;} that the sample mean $% \overline{Y}$ is unbiased because the theorem states that $E(\overline{Y})=% %TCIMACRO{\U{3bc} }% %BeginExpansion \mu %EndExpansion $.&lt;/p&gt;
&lt;p&gt;The other two properties are also true, but we won&amp;rsquo;t prove it; just take my word for it. Also, it can be shown that $\overline{Y}$ is has the smallest variability among all unbiased estimators, so when we use it, we are using &amp;ldquo;the best&amp;rdquo; guess of $\mu $.&lt;/p&gt;
&lt;h3 id=&#34;a-confidence-interval-for-the-mean-when-sigma-is-known&#34;&gt;A Confidence Interval for the Mean When $\sigma$ Is Known&lt;/h3&gt;
&lt;p&gt;By the Central Limit Theorem, we know that the distribution of $\overline{Y}$ is approximately $N(\mu,\sigma /\sqrt{n})$. This implies that we can predict something about the behavior of any given sample average $\overline{x}$. Specifically, we know that a sample average will have about a $68%$ chance of being within one standard deviation of $% \mu$ about a $95%$ chance of being within two standard deviations of $\mu$, and about a $99.7%$ chance of being within three standard deviations of $% \mu $.&lt;/p&gt;
&lt;p&gt;Confidence intervals flip this logic around and use the fact that if $% \overline{Y}$ has a $68%$ chance of being within one standard deviation of $% \mu$, then we can also say that $\mu$ has a $68%$ chance of being within one standard deviation of $\overline{Y}$. This means that&lt;/p&gt;
&lt;p&gt;$$P(\mu -2\sigma /\sqrt{n}&amp;lt;\overline{Y}&amp;lt;\mu +2\sigma /\sqrt{n})$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$P(\overline{Y}-2\sigma /\sqrt{n}&amp;lt;\mu &amp;lt;\overline{Y}+2\sigma /\sqrt{n})$$&lt;/p&gt;
&lt;p&gt;are equal. One statement can be made from the other simply by rearranging what goes in the middle using algebra.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a picture that might make things clearer.&lt;/p&gt;
&lt;p&gt;We don&amp;rsquo;t know what $\mu$ is, but we know that $\overline{Y}$ should fall around it in a predictable way. So we use this idea to &amp;ldquo;guess&amp;rdquo; at the location of $\mu$. If we were to take the range of numbers $% \overline{x}-2\sigma /\sqrt{n}$ to $\overline{x}+2\sigma /\sqrt{n}$ as our interval estimator, about $95%$ of the time, the true value of $\mu$ would fall in the interval; about $5%$ of the time it wouldn&amp;rsquo;t. So we could say we were $95%$ confident in the interval.&lt;/p&gt;
&lt;p&gt;What if we want a different level of confidence than $68,95$, or $99.7%?$¬†Suppose we wanted to be $98%$ confident, for example? Recall from Handout 2 that $P(-z_{\alpha /2}&amp;lt;Z&amp;lt;z_{\alpha /2})=1-\alpha$. If we want $98%$ confidence, we¬†begin by setting $1-\alpha =0.98$, which means $\alpha =0.02$. Then $z_{\alpha /2}=z_{0.02/2}=z_{0.01}=2.33$. Then we substitute $Z=\frac{% \overline{Y}-\mu }{\sigma /\sqrt{n}}$ in the middle and solve for $\mu$:&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} P(-2.33 &amp;amp;&amp;lt;&amp;amp;Z&amp;lt;2.33)=0.98 \ &amp;amp;\Rightarrow &amp;amp;P(-2.33&amp;lt;\frac{\overline{Y}-\mu }{\sigma /\sqrt{n}}&amp;lt;2.33)=0.98 \ &amp;amp;\Rightarrow &amp;amp;P(-2.33\sigma /\sqrt{n}&amp;lt;\overline{Y}-\mu &amp;lt;2.33\sigma /\sqrt{n}% )=0.98 \ &amp;amp;\Rightarrow &amp;amp;P(-2.33\sigma /\sqrt{n}+\overline{Y}&amp;lt;-\mu &amp;lt;2.33\sigma /\sqrt{n}% -\overline{Y})=0.98 \ &amp;amp;&amp;amp;\text{Multiply by }-1\text{ and rearrange to get} \ P(\overline{Y}-2.33\sigma /\sqrt{n} &amp;amp;&amp;lt;&amp;amp;\mu &amp;lt;\overline{Y}+2.33\sigma /\sqrt{n}% )=0.98\end{aligned}$$&lt;/p&gt;
&lt;p&gt;You can apply these steps for any confidence level you want. Therefore, we have the following:&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;A $(1-\alpha )100%$ confidence interval for $\mu$ when $\sigma$ is known (or when $n$ is large) is given by&lt;/p&gt;
&lt;p&gt;$$\overline{x}\pm z_{\alpha /2}\frac{\sigma }{\sqrt{n}}$$&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Calculation is easy with software. Interpretation is key. We view the mean $\mu$ as a &lt;em&gt;fixed&lt;/em&gt; but unknown quantity. Once we gather data and compute the sample mean $\overline{x}$ and the associated confidence interval, the interval either contains $\mu$ or it does not (see Figure 
&lt;a href=&#34;#Figure_c_int&#34;&gt;[Figure_c_int]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;Figure_c_int&amp;rdquo;} above). In the long run, however, if you took repeated samples of size $n$ and computed the interval each time, $(1-\U{3b1} )100%$ of them would contain $\mu$.Thus, the &amp;ldquo;confidence&amp;rdquo;¬†you have is in the &lt;em&gt;method&lt;/em&gt; used to construct an interval, &lt;em&gt;not in the particular interval&lt;/em&gt; you have constructed.&lt;/p&gt;
&lt;p&gt;Rarely do we know $\sigma$, but at times we have fairly large samples, so we can use Definition 
&lt;a href=&#34;#CI_sig_known&#34;&gt;[CI_sig_known]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;CI_sig_known&amp;rdquo;} anyway by just replacing $\sigma$ with the sample standard deviation $s$. If we have a small sample and do not know $\sigma$, we must use some other multiplier other than $z_{\alpha /2}$. For this, we use values from Student&amp;rsquo;s t distribution.&lt;/p&gt;
&lt;h3 id=&#34;a-confidence-interval-for-the-mean-when-sigma-is-unknown&#34;&gt;A Confidence Interval for the Mean When $\sigma$ Is Unknown&lt;/h3&gt;
&lt;p&gt;Student&amp;rsquo;s $t$ distribution with $\nu$ degrees of freedom is the probability distribution of the quantity&lt;/p&gt;
&lt;p&gt;$$t=\frac{\overline{Y}-\mu }{s/\sqrt{n}}$$&lt;/p&gt;
&lt;p&gt;As with the standard normal distribution, it is symmetric, bell-shaped, and centered at $0$. The degrees of freedom $\nu$ is a parameter that governs the width of the distribution. The notation $t_{\nu }$ refers to a random variable from a $t$ distribution with $\nu$ degrees of freedom.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a picture. As you can see in Figure 
&lt;a href=&#34;#studt&#34;&gt;[studt]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;studt&amp;rdquo;}, Student&amp;rsquo;s $t$ distribution looks a lot like the standard normal. In fact, it can be shown that as $\nu$ gets very large, the the $t$ distribution &amp;ldquo;becomes&amp;rdquo; the standard normal distribution. That&amp;rsquo;s why we can use values of $z$ when $n$ is large in a confidence interval.&lt;/p&gt;
&lt;p&gt;Using the $t$ distribution, we can define a confidence interval as follows:&lt;/p&gt;
&lt;p&gt;$(1-\alpha )100%$ confidence interval for $\mu$ when $\sigma$ is unknown (or when $n$ is small) is given by&lt;/p&gt;
&lt;p&gt;$$\overline{x}\pm t_{n-1,\alpha /2}\frac{s}{\sqrt{n}}$$&lt;/p&gt;
&lt;p&gt;with $t_{n-1,\alpha /2}$, being the value of a $t$ distribution with $n-1$ degrees of freedom that has probability $\alpha /2$ to its right.&lt;/p&gt;
&lt;p&gt;Here are some examples.&lt;/p&gt;
&lt;p&gt;A recent survey asked, &amp;ldquo;What is the ideal number of children for a person to have?&amp;rdquo;¬†A sample of $15$ American adults aged $20$ to $30$ reported the following numbers:&lt;/p&gt;
&lt;p&gt;$1$ $2$ $0$ $3$ $4$ $2$ $0$ $1$ $3$ $4$ $2$ $2$ $2$ $3$ $8$.&lt;/p&gt;
&lt;p&gt;Find a $93%$ confidence interval for the mean number of children American adults in this age group believe is ideal.&lt;/p&gt;
&lt;p&gt;In a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8$, respectively. Calculate $95%$ confidence intervals for the amount of time spent studying per week for both accounting and general business majors. Interpret the intervals in context.&lt;/p&gt;
&lt;p&gt;In the next subsection, we address the situation in which you wish examine two population/process means.&lt;/p&gt;
&lt;h2 id=&#34;interval-inference-about-two-populationprocess-means&#34;&gt;Interval Inference about Two Population/Process Means&lt;/h2&gt;
&lt;p&gt;Many times we are not interested only in a single group, but multiple $(\geq 2)$ groups. For example, we might compare the amount of weight lost for two groups to determine the effect of a new drug on adult men $45-60$. Let Group $1$ be men 45-60 who follow a specific diet and let Group $2$ be men $45-60$ who use the same diet but also take a weight loss drug. In the language of experimental design, Group $1$ is a &lt;strong&gt;control group&lt;/strong&gt;, the group that receives no &lt;strong&gt;treatment&lt;/strong&gt; (i.e., a specific manipulation of the members of the group), or no treatment above a reasonable standard. ¬†Group $2$ is an &lt;strong&gt;experimental&lt;/strong&gt; or &lt;strong&gt;treatment group.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;By the fundamental assumption of statistics (that variability is inescapable), any one person in Group $1$ could lose more weight than any one person in Group $2$, even if the drug is effective, simply by random chance. The question is not whether the drug works for everyone, but if it &amp;ldquo;works on average.&amp;rdquo;¬†The average for Group $1$ we can specify as $\mu_{1}$ and the average for Group 2 we can specify as $\mu_{2}$. Then the question is whether $\mu_{1}=\mu_{2}$, or, equivalently, whether $\mu_{1}-\mu_{2}=0$. We can address this question either through a confidence interval or a hypothesis test. We will discuss the first of these here. There are two cases to consider:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You know (or assume) that the two population standard deviations are the same, that is, $\sigma_{1}=\sigma_{2}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You don&amp;rsquo;t know (or don&amp;rsquo;t wish to assume) that the two population standard deviations are the same, that is $\sigma_{1}\neq \sigma_{2}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;case-1-confidence-interval-for-difference-in-means-sigma_1sigma_2&#34;&gt;Case 1: Confidence Interval for Difference in Means $\sigma_{1}=\sigma_{2}$&lt;/h3&gt;
&lt;p&gt;A $(1-\alpha )100%$ confidence interval for $\mu_{1}-\mu_{2}$ is given by $\overline{x}_{1}-\overline{x}_{2}\pm t_{n_{1}+n_{2}-2,\alpha /2}s_{p}\sqrt{% \frac{1}{n_{1}}+\frac{1}{n_{2}}}$, where $s_{p}=\sqrt{\frac{% (n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2}}$ is the &amp;ldquo;pooled&amp;rdquo; standard deviation from the two samples.&lt;/p&gt;
&lt;h3 id=&#34;case-2-confidence-interval-for-difference-in-means-sigma_1neq-sigma_2&#34;&gt;Case 2: Confidence Interval for Difference in Means $\sigma_{1}\neq \sigma_{2}$&lt;/h3&gt;
&lt;p&gt;A $(1-\alpha )100%$ confidence interval for $\mu_{1}-\mu_{2}$ is given by $\overline{x}_{1}-\overline{x}_{2}\pm t_{\nu,\alpha /2}\sqrt{\frac{s_{1}^{2}% }{n_{1}}+\frac{s_{2}^{2}}{n_{2}}}$, where $\nu =\frac{\left( \frac{s_{1}^{2}% }{n_{1}}+\frac{s_{2}^{2}}{n_{2}}\right) ^{2}}{\frac{\left( \frac{s_{1}^{2}}{% n_{1}}\right) ^{2}}{n_{1}-1}+\frac{\left( \frac{s_{2}^{2}}{n_{2}}\right) ^{2}% }{n_{2}-1}}$.&lt;/p&gt;
&lt;p&gt;The Case 2 interval makes one less assumption than Case 1, so it is closer to the truth (in reality, it&amp;rsquo;s unlikely that the two standard deviations will be exactly equal). On the other hand, it is known that the Case 2 interval is only an approximate interval, although the approximation is quite good. As with most things in life, there is a trade-off. In general, however, Case 2 is the safer bet. The degrees of freedom parameter, $\nu$, is easily calculated using software.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s an example:&lt;/p&gt;
&lt;p&gt;In a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8$, respectively. Calculate a $90%$ confidence interval for mean difference in the amount of time spent studying per week between accounting and general business majors. Interpret the interval in context. Assume the population standard deviations are equal.&lt;/p&gt;
&lt;p&gt;In the next section, we&amp;rsquo;ll address another form of inference for means, hypothesis testing.&lt;/p&gt;
&lt;h2 id=&#34;hypothesis-testing-inference-for-a-single-populationprocess-mean&#34;&gt;Hypothesis Testing Inference for a Single Population/Process Mean&lt;/h2&gt;
&lt;h3 id=&#34;basic-concepts-1&#34;&gt;Basic Concepts&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s begin with a definition.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;hypothesis test&lt;/strong&gt; (also called a &lt;strong&gt;significance test&lt;/strong&gt;) is a method of using data to evaluate the evidence about a specified value of a parameter.&lt;/p&gt;
&lt;p&gt;Every statistical test has two mutually exclusive &lt;strong&gt;hypotheses&lt;/strong&gt;, or claims about the value of a parameter.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;null hypothesis&lt;/strong&gt;, denoted $H_{0}$ and pronounced &amp;ldquo;$H$ not&amp;rdquo;¬†or &amp;ldquo;$H$ sub-zero,&amp;rdquo;¬†is typically a statement of &amp;quot; no effect,&amp;rdquo;¬†&amp;ldquo;no difference,&amp;rdquo;¬†&amp;ldquo;no special ability beyond random chance,&amp;rdquo;¬†&amp;quot; equality,&amp;rdquo;¬†etc. This is usually what the researcher wants to disprove, or reject.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;alternative hypothesis&lt;/strong&gt;, denoted $H_{a}$ or $H_{1}$, is the complement (&amp;ldquo;opposite&amp;rdquo;) of the null, and usually what the researcher is trying to establish.&lt;/p&gt;
&lt;p&gt;Hypotheses always come in pairs.&lt;/p&gt;
&lt;p&gt;When performing a hypothesis test, it is possible to be wrong, or, more formally, to commit an error. Here are the two types of errors we&amp;rsquo;ll be concerned with (there is a third type as well but we won&amp;rsquo;t worry about that one).&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;Type I error&lt;/strong&gt; occurs when we reject a true null hypothesis. This is akin to a &amp;ldquo;false positive&amp;rdquo; (claiming there is an effect or difference when there isn&amp;rsquo;t).&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;Type II error&lt;/strong&gt; occurs when we don&amp;rsquo;t reject a false null hypothesis. This is akin to a &amp;ldquo;false negative&amp;rdquo; (claiming there is not an effect or difference when there is).&lt;/p&gt;
&lt;p&gt;For any given test, we don&amp;rsquo;t know if we have committed a Type I error because we don&amp;rsquo;t know what the true value of the parameter is (if we did, why do a test at all?). What we can do is specify that we want to construct our test such that the $P($Type¬†I error$)$ $=\alpha$ is &amp;ldquo;small.&amp;rdquo; That is, if we could imagine doing the test repeatedly for different samples from the same population, in the long run, we would only want to reject $H_{0}$ $% \alpha (100)%$ of the time. A similar criterion exists for a Type II error: we want $P($Type II error) $=\beta$ to be small as well.&lt;/p&gt;
&lt;p&gt;The two types of errors cannot be minimized at the same time; lowering one error probability raises the other. What is done is to find tests with a small $\alpha$, and from that set, find the test that also has a small $% \beta$. For our purposes, we will only consider Type I error. It can be shown that the hypothesis tests presented next also have small $\beta $.&lt;/p&gt;
&lt;p&gt;Hypothesis testing can be thought of as a process with a number of steps. Here they are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Specify the hypothesis&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calculate the &lt;strong&gt;test statistic&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the &lt;strong&gt;p-value&lt;/strong&gt; or &lt;strong&gt;rejection region&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make a conclusion&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We already talked about (1) in Figure 
&lt;a href=&#34;#hyps&#34;&gt;[hyps]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;hyps&amp;rdquo;}. Here are some more definitions:&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;test statistic&lt;/strong&gt; is a specific function of data and the parameter value specified by the null hypothesis.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;p-value&lt;/strong&gt; of a test is the probability, assuming the null is true, that we would observe a value of the test statistic as extreme or more extreme than what we actually got.&lt;/p&gt;
&lt;p&gt;We make a conclusion based on the $p$-value or the rejection region. $P$ -values are more commonly used in practice. The judgement is simple: if the $p$-value $\leq$ $\alpha$ (i.e., our Type I¬†error probability),then we &lt;strong&gt;reject&lt;/strong&gt; the null hypothesis; if the $p$-value $&amp;gt;\alpha$, we fail to reject the null hypothesis. Most software programs report $p$-values automatically.&lt;/p&gt;
&lt;p&gt;Another approach is to specify, &lt;em&gt;before examining the data&lt;/em&gt;, the $% \alpha$ (Type I error probability) that we want and determine a &lt;strong&gt;critical value&lt;/strong&gt;, a value of test statistic distribution &lt;em&gt;under the null hypothesis&lt;/em&gt; such that if the test statistic falls within that region, we reject the null hypothesis. A visual of this idea is shown in Figure 
&lt;a href=&#34;#rr&#34;&gt;[rr]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;rr&amp;rdquo;} for a test of a mean. If the calculated value of the test statistic falls in the shaded region, we would reject the null hypothesis.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    If a $(1-\alpha )100%$ confidence interval &lt;strong&gt;contains&lt;/strong&gt; the value of $% H_{0}$, then a test of $H_{0}:\mu =\mu_{0}$ conducted at the $\alpha$ level will **fail to reject** the null hypothesis.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This fact can be proven but the intuition is simple: if the interval contains the most reasonable values of the parameter (based on the data and the confidence level) then it&amp;rsquo;s sensible that the test should not reject one of the values that lies inside the interval. This logic extends to the situation of testing the difference of two means, described below.&lt;/p&gt;
&lt;h3 id=&#34;a-hypothesis-test-for-a-single-populationprocess-mean&#34;&gt;A Hypothesis Test for a Single Population/Process Mean&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now address a specific test. In every equation that follows, $\mu_{0}$ is the value specified value of $\mu$ that we are testing.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Specify the hypotheses&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} \text{(a) }H_{0} &amp;amp;:&amp;amp;\mu \leq \mu_{0}\text{ or} \ \text{(b) }H_{0} &amp;amp;:&amp;amp;\mu \geq \mu_{0}\text{ or } \ \text{(c) }H_{0} &amp;amp;:&amp;amp;\mu =\mu_{0}\end{aligned}$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The alternative hypothesis is the complement of the null.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Calculate the test statistic&lt;/p&gt;
&lt;p&gt;For a test of a single mean, the test statistic is&lt;/p&gt;
&lt;p&gt;$$t_{0}=\frac{\overline{y}-\mu_{0}}{s/\sqrt{n}}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the &lt;strong&gt;rejection region&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The the rejection region (i.e., the &amp;ldquo;critical values&amp;rdquo;) for a test with the $% \alpha$ level of significance would be computed as follows:&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} \text{(a)}{\text{ }t_{n-1} &amp;amp;:&amp;amp;t_{n-1}&amp;gt;\text{ }t_{\alpha,n-1}} \ \text{(b) }{t_{n-1} &amp;amp;:&amp;amp;t_{n-1}&amp;lt;-t_{\alpha,n-1}} \ \text{(c) }{t_{n-1} &amp;amp;:&amp;amp;|t_{n-1}|&amp;gt;\text{ }t_{\alpha /2,n-1}}\end{aligned}$$&lt;/p&gt;
&lt;p&gt;where $t_{n-1}$is a Student&amp;rsquo;s $t$ random variable with $n_{d}-1$ degrees of freedom. Alternatively, we can calculate the $p$-value as&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\begin{aligned} \text{(a) }p &amp;amp;=&amp;amp;P(t_{n-1}&amp;lt;t_{0}) \ \text{(b) }p &amp;amp;=&amp;amp;P(t_{n-1}&amp;gt;t_{0}) \ \text{(c) }p &amp;amp;=&amp;amp;2\min [P(t_{n-1}&amp;gt;t_{0}),P(t_{n-1}&amp;lt;t_{0})]\end{aligned}$$&lt;/p&gt;
&lt;p&gt;Calculating the $p$-value for a test involving the $t$ distribution requires software.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make a conclusion.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here are some examples.&lt;/p&gt;
&lt;p&gt;A machine produces ball bearings is calibrated to produce diameters of $0.5$ inches. A sample of $10$ ball bearings from the process was taken. The average diameter was $0.493$ inches, with a standard deviation of $0.022$ inches. A quality control inspector wants to determine if the machine is not calibrated properly. Conduct a hypothesis test at the $\alpha =0.05$ level to answer his question.&lt;/p&gt;
&lt;p&gt;A specialty cell phone manufacturer wants to determine how many pictures are stored on phones used by older consumers. A survey was conducted and a random sample of $6$ older consumers reported the following numbers of pictures on their cell phones:$25$ $\ 6$ $22$ $26$ $31$ $18$. Historically, the average number of pictures stored on phones by this consumer group has been $10$. The company wants to know if this has changed. Use a hypothesis test at the $\alpha =0.05$ level to answer this question.&lt;/p&gt;
&lt;h3 id=&#34;a-hypothesis-test-for-two-populationprocess-means&#34;&gt;A Hypothesis Test for Two Population/Process Means&lt;/h3&gt;
&lt;p&gt;Here is the procedure for testing two means.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Specify the hypotheses.&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} \text{(a) }H_{0} &amp;amp;:&amp;amp;\mu_{1}-\mu_{2}\leq d\text{ or} \ \text{(b) }H_{0} &amp;amp;:&amp;amp;\mu_{1}-\mu_{2}\geq d\text{ or } \ \text{(c) }H_{0} &amp;amp;:&amp;amp;\mu_{1}-\mu_{2}\neq d\end{aligned}$$&lt;/p&gt;
&lt;p&gt;where $d$ is some specified difference (usually $0)$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calculate the test statistic.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 1:&lt;/strong&gt; $\sigma_{1}=\sigma_{2}$&lt;/p&gt;
&lt;p&gt;$$t_{0}=\frac{\overline{x}_{1}-\overline{x}_{2}-d}{s_{p}\sqrt{\frac{1}{n_{1}}+% \frac{1}{n_{2}}}}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 2:&lt;/strong&gt; $\sigma_{1}\neq \sigma_{2}$&lt;/p&gt;
&lt;p&gt;$$t_{0}=\frac{\overline{x}_{1}-\overline{x}_{2}-d}{\sqrt{\frac{s_{1}^{2}}{n_{1}% }+\frac{s_{2}^{2}}{n_{2}}}}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the p-value&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 1:&lt;/strong&gt; $\sigma_{1}=\sigma_{2}$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} \text{(a) }p &amp;amp;=&amp;amp;P(t_{n_{1}+n_{2}-2}&amp;lt;t_{0}) \ \text{(b) }p &amp;amp;=&amp;amp;P(t_{n_{1}+n_{2}-2}&amp;gt;t_{0}) \ \text{(c) }p &amp;amp;=&amp;amp;2\min [P(t_{n_{1}+n_{2}-2}&amp;gt;t_{0}),P(t_{n_{1}+n_{2}-2}&amp;lt;t_{0})]\end{aligned}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 2:&lt;/strong&gt; $\sigma_{1}\neq \sigma_{2}$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} \text{(a) }p &amp;amp;=&amp;amp;P(t_{\nu }&amp;lt;t_{0}) \ \text{(b) }p &amp;amp;=&amp;amp;P(t_{\nu }&amp;gt;t_{0}) \ \text{(c) }p &amp;amp;=&amp;amp;2\min [P(t_{\nu }&amp;gt;t_{0}),P(t_{\nu }&amp;lt;t_{0})]\end{aligned}$$&lt;/p&gt;
&lt;p&gt;where $\nu =\frac{\left( \frac{s_{1}^{2}}{n_{1}}+\frac{s_{2}^{2}}{n_{2}}% \right) ^{2}}{\frac{\left( \frac{s_{1}^{2}}{n_{1}}\right) ^{2}}{n_{1}-1}+% \frac{\left( \frac{s_{2}^{2}}{n_{2}}\right) ^{2}}{n_{2}-1}}$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make a conclusion&lt;/p&gt;
&lt;p&gt;If $p\leq \alpha $, reject $H_{0}$. If $p&amp;gt;\alpha $, fail to reject $H_{0}$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here are some examples:&lt;/p&gt;
&lt;p&gt;In a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8$, respectively. Determine at the $% \alpha =0.05$ level if there is a difference in the population mean amount of time spent studying per week between accounting and general business majors. Assume the population standard deviations are equal.&lt;/p&gt;
&lt;p&gt;In a recent survey of $121$ undergraduates, students reported the number of hours per week they spend on school work. Figure 
&lt;a href=&#34;#table1&#34;&gt;[table1]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;table1&amp;rdquo;} below summarizes the data for finance and marketing majors.&lt;/p&gt;
&lt;p&gt;A researcher claims that marketing majors and finance majors do not study the same amount of time per week on average. Test the hypothesis at the $% 10%$ level of significance, assuming the two population standard deviations are not the same.&lt;/p&gt;
&lt;p&gt;The critical region approach can be used for testing two means as well (and for every hypothesis test, in fact). However, because software generally reports $p$-values, and $p$-values are what are used most often in practice, we will not address this approach here.&lt;/p&gt;
&lt;p&gt;This document has been a &amp;ldquo;quick and dirty&amp;rdquo; review of hypothesis testing. The basic principles discussed here will apply to all of the tests we examine in this class. I encourage you to go back to this document each time we discuss a new test so that you can see the general pattern.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;If you want to know more about how statisticians really view the world, here&amp;rsquo;s a shameless plug for Kevin&amp;rsquo;s book: &lt;a href=&#34;http://www.amazon.com/Understanding-Advanced-Statistical-Methods-Chapman/dp/1466512105&#34;&gt;http://www.amazon.com/Understanding-Advanced-Statistical-Methods-Chapman/dp/1466512105&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Sampling Distributions, the Central Limit Theorem, Inference about Means</title>
      <link>/courses/bana3363/3-2-sampling-distributions-clt/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/bana3363/3-2-sampling-distributions-clt/</guid>
      <description>&lt;h2 id=&#34;sampling-distributions&#34;&gt;Sampling Distributions&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s get a definition out of the way. This comes very close to the &amp;ldquo;official&amp;rdquo;¬†Google definition:&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;sample&lt;/strong&gt; is a small collection of objects whose properties are thought to correspond closely with those of a large (or even infinite) collection of items or quantities that is the main interest of the researcher.&lt;/p&gt;
&lt;p&gt;This large collection of items or quantities about which we would like to make reasonable statements is called a &lt;strong&gt;population,&lt;/strong&gt; or, more accurately, a &lt;strong&gt;statistical process,&lt;/strong&gt; or just &lt;strong&gt;process.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You will see the word &amp;ldquo;population&amp;rdquo; used more often than &amp;ldquo;process&amp;rdquo; in most of the material you read despite &amp;ldquo;process&amp;rdquo; being a more accurate description of how a statistician views the natural world. The word &amp;ldquo;population&amp;rdquo; evokes images of a single, fixed collection of objects that we sample from. In much of statistics, and especially in forecasting and time series, the population view is simply wrong. In many cases, what a researcher is trying to study is an ever-changing process with many factors that can influence the actual data the researcher can see. For these situations, the process view is better &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;For example, suppose wish to study income of beginning accounting professionals in the U. S. Many factors interact to produce the income population: labor markets, the government, regional events, education, etc.. You take a sample of $300$ professionals and invite them to complete a survey about income. Your goal might be to estimate the &amp;ldquo;population&amp;rdquo; mean beginning accounting salary, $\mu$,  using the sample of size $300$. There are two main problems. First, the population you want to make a conclusion about (all beginning accountants) is not the population you drew from (all beginning accountants who don&amp;rsquo;t mind filling out a survey). Furthermore, some respondents no doubt will (unintentionally or intentionally) give you incorrect responses, so the &amp;ldquo;population&amp;rdquo; you are sampling from is again not quite what you want. Taking the process view, though, you can view $\mu$ as the mean of the process that produces the actual accounting salary data you see, and you can use inference to make conclusions about that process. If your study is designed properly, your results will be a reasonable representation of the &amp;ldquo;population&amp;rdquo; you are really interested in, beginning accounting graduates.&lt;/p&gt;
&lt;p&gt;For purposes of doing the work in this class, however, you can use phrases such as &amp;ldquo;the population mean&amp;rdquo; (I probably will, just out of habit) and be fine. Now we need to be more specific about what a sample is.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;random sample&lt;/strong&gt; is a collection of $n$ independent random variables $Y_{1},Y_{2},\ldots ,Y_{n}$ that are assumed to have been &amp;ldquo;drawn&amp;rdquo; from the same probability distribution $f(y|\mathbf{\theta ),}$where $\mathbf{\theta }$ is a generic collection of parameters. For example, if we are taking a sample from a normal distribution $\mathbf{\theta ={}\mu ,\sigma }.$&lt;/p&gt;
&lt;p&gt;Figure 
&lt;a href=&#34;#sampling&#34;&gt;[sampling]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;sampling&amp;rdquo;} illustrates this idea. Essentially, before the researcher gathers data, he or she imagines that some (usually unknown) probability distribution produces data from which he or she will draw a single sample of size, say, $300$. When the data are actually gathered, the researcher takes the view that he or she is observing one possible realization of those $300$ random variables.&lt;/p&gt;
&lt;p&gt;The key to understanding statistical inference is that different samples produce different values of calculated statistics such as the sample mean and sample standard deviation, but these values behave in a predictable way. Although we only work with one sample, we use the properties, we use the fact that the samples behave in a predictable way to make the leap from any one sample to the population or process of interest.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;sampling distribution&lt;/strong&gt; is the distribution of a calculated statistic (for example, the sample mean) when we imagine taking repeated samples from the same process and calculating the statistic over and over.&lt;/p&gt;
&lt;p&gt;There is an entire distribution of values because each random sample of size $n$ will have different data in it. So, a statistic is actually a random variable too. Therefore, when we talk about a statistic like the sample mean in general terms, we will use a capital letter, $\overline{Y}$ , and when we talk about a specific value of the statistic, like $2.3$, we will use lowercase, as in $\overline{y}=2.3.$ A good illustration of this concept is found 
&lt;a href=&#34;http://onlinestatbook.com/stat_sim/sampling_dist/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;, courtesy of our friends at Rice University.&lt;/p&gt;
&lt;p&gt;We can summarize the distribution like we have before using expected value and variance. It turns out that if we choose the right statistic, we can make some powerful conclusions about the population. Here are some cool facts.&lt;/p&gt;
&lt;p&gt;[[samp_dist]]{#samp_dist label=&amp;quot;samp_dist&amp;rdquo;} For a random sample of size $n$ from a process (or population), the sample mean, $\overline{Y}$, is a random variable with mean $=E(\overline{Y})=% %TCIMACRO{\U{3bc} }% %BeginExpansion \mu %EndExpansion$ (the process/population mean) and variance $V(\overline{Y})=$ $\frac{% \sigma ^{2}}{n}$, the population variance divided by $n.$&lt;/p&gt;
&lt;p&gt;Stand by for the most important fact in statistics.¬†It&amp;rsquo;s so important that it deserves its own section.&lt;/p&gt;
&lt;h2 id=&#34;sampling-distribution-of-the-sample-mean&#34;&gt;Sampling Distribution of the Sample Mean&lt;/h2&gt;
&lt;p&gt;[[CLT]]{#CLT label=&amp;quot;CLT&amp;rdquo;}(The Central Limit Theorem ) The sampling distribution of the sample mean $\overline{Y}$ is approximately a normal distribution with mean $=\mu$ and variance $\frac{\sigma ^{2}}{n}$, &lt;em&gt;no matter what the original distribution looks like&lt;/em&gt;, as the sample size $n$ gets large.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Large&amp;rdquo; here technically refers to letting the sample size go to infinity, but in most practical cases, the Central Limit Theorem (CLT) applies when $n$ is as small as $30.$&lt;/p&gt;
&lt;p&gt;If we assume the sample is drawn from a normal distribution, then the distribution of $\overline{Y}$ is normal regardless of sample size.&lt;/p&gt;
&lt;p&gt;The CLT is the reason we don&amp;rsquo;t really have to worry about what the population distribution looks like in Figure 
&lt;a href=&#34;#sampling&#34;&gt;[sampling]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;sampling&amp;rdquo;}. It says that the distribution can take &lt;em&gt;any shape&lt;/em&gt; and the distribution of the sample average will still be approximately normal. The CLT is what makes most of statistics work for practical problems.&lt;/p&gt;
&lt;p&gt;We can use the distribution of the sample mean to make conclusions about a sample. For example, we can ask, what is $P(\overline{Y}&amp;gt;4)?$ To answer this question, we can use our old friend the $z$-score. Remember that a $z$-score is, in general, $$\frac{\text{observation }-\text{ mean}}{\text{standard deviation}}.$$&lt;/p&gt;
&lt;p&gt;Z-scores for sample averages are found by subtracting the mean and dividing by the standard deviation, which are given in Theorem 
&lt;a href=&#34;#samp_dist&#34;&gt;[samp_dist]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;samp_dist&amp;rdquo;}:&lt;/p&gt;
&lt;p&gt;$$Z=\frac{\overline{Y}-\mu }{\sigma /\sqrt{n}}$$&lt;/p&gt;
&lt;p&gt;Here are some examples to try out.&lt;/p&gt;
&lt;p&gt;The foreman of a bottling plant has observed that the amount of soda in each &amp;ldquo;$32$-ounce&amp;rdquo;¬†bottle is actually a normally distributed random variable, with a mean of $32$ ounces and a standard deviation of $0.3$ ounce. If the foreman is doing quality control by taking samples of $n$ $=4$ bottles from the line and calculating the average fill, would a reading of $32.5$ be unusually high? Answer by finding $P(\overline{Y}&amp;gt;32.5).$ How would your answer change if we took away the assumption that the amount of soda in each bottle comes from a normal distribution?&lt;/p&gt;
&lt;p&gt;You take a random sample of size $50$ from a population with mean $\mu =16$ and standard deviation $\sigma =$ $8$.&lt;/p&gt;
&lt;p&gt;a). What are the mean and standard deviation of $\overline{Y}$, the sample mean?&lt;/p&gt;
&lt;p&gt;b). Sketch the distribution, indicating the intervals within which $68 %$, $95 %$, and $99.7 %$ of the observations are expected to lie.&lt;/p&gt;
&lt;p&gt;c). Find $P(\overline{Y}&amp;gt;18)$&lt;/p&gt;
&lt;p&gt;d). Find $P(17&amp;lt;\overline{Y}&amp;lt;19.5)$&lt;/p&gt;
&lt;p&gt;e). Explain why all of these probabilities are approximate.&lt;/p&gt;
&lt;p&gt;In a random sample of $20$ college students, the average reported working time was $15.4$ hours per week. A researcher claims that the true working time of college students is normally distributed with a mean of $16$ hours and a standard deviation of $2.5$ hours. Let $\overline{Y}$ denote the sample mean.&lt;/p&gt;
&lt;p&gt;a). What is $P(\overline{Y}&amp;lt;15.4)$?&lt;/p&gt;
&lt;p&gt;b). Using your answer to (a), how reasonable do you think the researcher&amp;rsquo;s claim is?&lt;/p&gt;
&lt;p&gt;c). Using the $68-95-99.7$ rule, find a set of sample averages that would be unusual to see (either &amp;ldquo;too large&amp;rdquo; or &amp;ldquo;too small&amp;rdquo;) if the true mean and standard deviation of college student working hours were as the researcher claims.&lt;/p&gt;
&lt;p&gt;In the next section, we will discuss how we can use the sampling distribution of the sample mean to make inferences about $\mu$.&lt;/p&gt;
&lt;h1 id=&#34;interval-inference-about-a-single-population-process-mean&#34;&gt;Interval Inference about a Single Population (Process) Mean&lt;/h1&gt;
&lt;h2 id=&#34;basic-concepts&#34;&gt;Basic Concepts&lt;/h2&gt;
&lt;p&gt;Our ultimate goal is to obtain a reasonable &amp;quot; guess&amp;rdquo;¬†at the true value of a process parameter (in this section, $\mu$) using a sample of data. Before we do that, we have to introduce some terms.&lt;/p&gt;
&lt;p&gt;A**¬†point estimator** for a parameter is a particular function of the random sample that gives a single number that we hope is &amp;quot; close&amp;rdquo;¬†to the true value of the parameter.&lt;/p&gt;
&lt;p&gt;An &lt;strong&gt;interval estimator&lt;/strong&gt; is a range of values, constructed using observed data, within which the parameter is believed to fall.&lt;/p&gt;
&lt;p&gt;This sounds very technical, but it really is not so bad. An example of a point estimator is the sample mean: $\overline{Y}=\frac{\sum Y_{i}}{n}.$ Notice how it is a function of the data (the values of $Y_{i})$. In practice, we typically use the point estimator to construct an interval estimator.&lt;/p&gt;
&lt;p&gt;In theory, there are an infinite number of point estimators we could pick to estimate $\mu$, but it turns out that only one, $\overline{Y}$, is &amp;ldquo;the best.&amp;rdquo; The following criteria are used to judge whether a point estimator is &amp;ldquo;good:&amp;rdquo;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unbiasedness&lt;/strong&gt; &amp;ndash; The average of the estimator is equal to the population parameter.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Consistency&lt;/strong&gt; &amp;ndash; The estimator gets closer and closer to the true population parameter as the sample size increases&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relative efficiency&lt;/strong&gt; &amp;ndash; Given two different estimators with the same sample size, we choose the one with the smaller variance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We know from Theorem 
&lt;a href=&#34;#samp_dist&#34;&gt;[samp_dist]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;samp_dist&amp;rdquo;} that the sample mean $\overline{Y}$ is unbiased because the theorem states that $E(\overline{Y})=% %TCIMACRO{\U{3bc} }% %BeginExpansion \mu %EndExpansion$.  The other two properties are also true, but we won&amp;rsquo;t prove it; just take my word for it. Also, it can be shown that $\overline{Y}$ is has the smallest variability among all unbiased estimators, so when we use it, we are using &amp;ldquo;the best&amp;rdquo; guess of $\mu$.&lt;/p&gt;
&lt;h2 id=&#34;a-confidence-interval-for-the-mean-when-sigma-is-known&#34;&gt;A Confidence Interval for the Mean When $\sigma$ Is Known&lt;/h2&gt;
&lt;p&gt;By the Central Limit Theorem, Theorem 
&lt;a href=&#34;#CLT&#34;&gt;[CLT]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;CLT&amp;rdquo;}, we know that the distribution of $\overline{Y}$ is approximately $N(\mu ,\sigma /\sqrt{n})$. This implies that we can predict something about the behavior of any given sample average $\overline{x}.$ Specifically, we know that a sample average will have about a $68 %$ chance of being within one standard deviation of $\mu$ about a $95 %$ chance of being within two standard deviations of $\mu$ , and about a $99.7 %$ chance of being within three standard deviations of $\mu$.&lt;/p&gt;
&lt;p&gt;Confidence intervals flip this logic around and use the fact that if $\overline{Y}$ has a $68 %$ chance of being within one standard deviation of $\mu$, then we can also say that $\mu$ has a $68 %$ chance of being within one standard deviation of $\overline{Y}.$ This means that&lt;/p&gt;
&lt;p&gt;$$P(\mu -2\sigma /\sqrt{n}&amp;lt;\overline{Y}&amp;lt;\mu +2\sigma /\sqrt{n})$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$P(\overline{Y}-2\sigma /\sqrt{n}&amp;lt;\mu &amp;lt;\overline{Y}+2\sigma /\sqrt{n})$$&lt;/p&gt;
&lt;p&gt;are equal. One statement can be made from the other simply by rearranging what goes in the middle using algebra.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a picture that might make things clearer.&lt;/p&gt;
&lt;p&gt;We don&amp;rsquo;t know what $\mu$ is, but we know that $\overline{Y}$ should fall around it in a predictable way. So we use this idea to &amp;ldquo;guess&amp;rdquo; at the location of $\mu$.  If we were to take the range of numbers $\overline{x}-2\sigma /\sqrt{n}$ to $\overline{x}+2\sigma /\sqrt{n}$ as our interval estimator, about $95 %$ of the time, the true value of $\mu$ would fall in the interval; about $5 %$ of the time it wouldn&amp;rsquo;t. So we could say we were $95 %$ confident in the interval.&lt;/p&gt;
&lt;p&gt;What if we want a different level of confidence than $68,95$, or $99.7 %?$¬†Suppose we wanted to be $98 %$ confident, for example? Recall from Handout 2 that $P(-z_{\alpha /2}&amp;lt;Z&amp;lt;z_{\alpha /2})=1-\alpha$.  If we want $98 %$ confidence, we¬†begin by setting $1-\alpha =0.98$, which means $\alpha =0.02.$ Then $z_{\alpha /2}=z_{0.02/2}=z_{0.01}=2.33.$ Then we substitute $Z=\frac{% \overline{Y}-\mu }{\sigma /\sqrt{n}}$ in the middle and solve for $\mu :$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} P(-2.33 &amp;amp;&amp;lt;&amp;amp;Z&amp;lt;2.33)=0.98 \ &amp;amp;\Rightarrow &amp;amp;P(-2.33&amp;lt;\frac{\overline{Y}-\mu }{\sigma /\sqrt{n}}&amp;lt;2.33)=0.98 \ &amp;amp;\Rightarrow &amp;amp;P(-2.33\sigma /\sqrt{n}&amp;lt;\overline{Y}-\mu &amp;lt;2.33\sigma /\sqrt{n}% )=0.98 \ &amp;amp;\Rightarrow &amp;amp;P(-2.33\sigma /\sqrt{n}+\overline{Y}&amp;lt;-\mu &amp;lt;2.33\sigma /\sqrt{n}% -\overline{Y})=0.98 \ &amp;amp;&amp;amp;\text{Multiply by }-1\text{ and rearrange to get} \ P(\overline{Y}-2.33\sigma /\sqrt{n} &amp;amp;&amp;lt;&amp;amp;\mu &amp;lt;\overline{Y}+2.33\sigma /\sqrt{n}% )=0.98$&lt;/p&gt;
&lt;p&gt;You can apply these steps for any confidence level you want. Therefore, we have the following:&lt;/p&gt;
&lt;p&gt;[[CI_sig_known]]{#CI_sig_known label=&amp;quot;CI_sig_known&amp;rdquo;}A $(1-\alpha )100 %$ confidence interval for $\mu$ when $\sigma$ is known (or when $n$ is large) is given by&lt;/p&gt;
&lt;p&gt;$$\overline{x}\pm z_{\alpha /2}\frac{\sigma }{\sqrt{n}}$$&lt;/p&gt;
&lt;p&gt;Calculation is easy with software. Interpretation is key. We view the mean $%TCIMACRO{\U{3bc} }% %BeginExpansion \mu %EndExpansion$ as a &lt;em&gt;fixed&lt;/em&gt; but unknown quantity. Once we gather data and compute the sample mean $\overline{x}$ and the associated confidence interval, the interval either contains $\mu$ or it does not (see Figure 
&lt;a href=&#34;#Figure_c_int&#34;&gt;[Figure_c_int]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;Figure_c_int&amp;rdquo;} above). In the long run, however, if you took repeated samples of size $n$ and computed the interval each time, $(1-\U{3b1} )100 %$ of them would contain $\mu$ . Thus, the &amp;ldquo;confidence&amp;rdquo;¬†you have is in the &lt;em&gt;method&lt;/em&gt; used to construct an interval, &lt;em&gt;not in the particular interval&lt;/em&gt; you have constructed.&lt;/p&gt;
&lt;p&gt;Rarely do we know $\sigma$, but at times we have fairly large samples, so we can use Definition 
&lt;a href=&#34;#CI_sig_known&#34;&gt;[CI_sig_known]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;CI_sig_known&amp;rdquo;} anyway by just replacing $\sigma$ with the sample standard deviation $s.$ If we have a small sample and do not know $\sigma$, we must use some other multiplier other than $z_{\alpha /2}.$ For this, we use values from Student&amp;rsquo;s t distribution.&lt;/p&gt;
&lt;h2 id=&#34;a-confidence-interval-for-the-mean-when-sigma-is-unknown&#34;&gt;A Confidence Interval for the Mean When $\sigma$ Is Unknown&lt;/h2&gt;
&lt;p&gt;Student&amp;rsquo;s $t$ distribution with $\nu$ degrees of freedom is the probability distribution of the quantity&lt;/p&gt;
&lt;p&gt;$$t=\frac{\overline{Y}-\mu }{s/\sqrt{n}}$$&lt;/p&gt;
&lt;p&gt;As with the standard normal distribution, it is symmetric, bell-shaped, and centered at $0.$ The degrees of freedom $\nu$ is a parameter that governs the width of the distribution. The notation $t_{\nu }$ refers to a random variable from a $t$ distribution with $\nu$ degrees of freedom.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a picture. As you can see in Figure 
&lt;a href=&#34;#studt&#34;&gt;[studt]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;studt&amp;rdquo;}, Student&amp;rsquo;s $t$ distribution looks a lot like the standard normal. In fact, it can be shown that as $\nu$ gets very large, the the $t$ distribution &amp;ldquo;becomes&amp;rdquo; the standard normal distribution. That&amp;rsquo;s why we can use values of $z$ when $n$ is large in a confidence interval.&lt;/p&gt;
&lt;p&gt;Using the $t$ distribution, we can define a confidence interval as follows:&lt;/p&gt;
&lt;p&gt;$(1-\alpha )100 %$ confidence interval for $\mu$ when $\sigma$ is unknown (or when $n$ is small) is given by&lt;/p&gt;
&lt;p&gt;$$\overline{x}\pm t_{n-1,\alpha /2}\frac{s}{\sqrt{n}}$$&lt;/p&gt;
&lt;p&gt;with $t_{n-1,\alpha /2}$, being the value of a $t$ distribution with $n-1$ degrees of freedom that has probability $\alpha /2$ to its right.&lt;/p&gt;
&lt;p&gt;Here are some examples.&lt;/p&gt;
&lt;p&gt;A recent survey asked, &amp;ldquo;What is the ideal number of children for a person to have?&amp;rdquo;¬†A sample of $15$ American adults aged $20$ to $30$ reported the following numbers: $1$ $\ \ 2$ $\ \ 0$ $\ \ 3$ $\ \ 4$ $\ \ 2$ $\ \ 0$ $\ 1$ $\ \ 3$ $\ \ 4$ $\ \ 2$ $\ \ 2$ $\ \ 2$ $\ \ 3$ $\ \ 8.$ Find a $93 %$ confidence interval for the mean number of children American adults in this age group believe is ideal.&lt;/p&gt;
&lt;p&gt;In a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8$, respectively. Calculate $95 %$ confidence intervals for the amount of time spent studying per week for both accounting and general business majors. Interpret the intervals in context.&lt;/p&gt;
&lt;p&gt;In the next subsection, we address the situation in which you wish examine two population/process means.&lt;/p&gt;
&lt;h1 id=&#34;interval-inference-about-two-populationprocess-means&#34;&gt;Interval Inference about Two Population/Process Means&lt;/h1&gt;
&lt;p&gt;Many times we are not interested only in a single group, but multiple $(\geq 2)$ groups. For example, we might compare the amount of weight lost for two groups to determine the effect of a new drug on adult men $45-60$. Let Group $1$ be men 45-60 who follow a specific diet and let Group $2$ be men $45-60$ who use the same diet but also take a weight loss drug. In the language of experimental design, Group $1$ is a &lt;strong&gt;control group&lt;/strong&gt;, the group that receives no &lt;strong&gt;treatment&lt;/strong&gt; (i.e., a specific manipulation of the members of the group), or no treatment above a reasonable standard. ¬†Group $2$ is an &lt;strong&gt;experimental&lt;/strong&gt; or &lt;strong&gt;treatment&lt;/strong&gt; &lt;strong&gt;group.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;By the fundamental assumption of statistics (that variability is inescapable), any one person in Group $1$ could lose more weight than any one person in Group $2$, even if the drug is effective, simply by random chance. The question is not whether the drug works for everyone, but if it &amp;ldquo;works on average.&amp;rdquo;¬†The average for Group $1$ we can specify as $\mu _{1}$ and the average for Group 2 we can specify as $\mu _{2}.$ Then the question is whether $\mu _{1}=\mu _{2}$, or, equivalently, whether $\mu _{1}-\mu _{2}=0.$ We can address this question either through a confidence interval or a hypothesis test. We will discuss the first of these here. There are two cases to consider:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You know (or assume) that the two population standard deviations are the same, that is, $\sigma _{1}=\sigma _{2}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You don&amp;rsquo;t know (or don&amp;rsquo;t wish to assume) that the two population standard deviations are the same, that is $\sigma _{1}\neq \sigma _{2}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;case-1-confidence-interval-for-difference-in-means-sigma-_1sigma-_2&#34;&gt;Case 1: Confidence Interval for Difference in Means $\sigma _{1}=\sigma _{2}$&lt;/h2&gt;
&lt;p&gt;A $(1-\alpha )100 %$ confidence interval for $\mu &lt;em&gt;{1}-\mu &lt;em&gt;{2}$ is given by $\overline{x}&lt;/em&gt;{1}-\overline{x}&lt;/em&gt;{2}\pm t_{n_{1}+n_{2}-2,\alpha /2}s_{p}\sqrt{% \frac{1}{n_{1}}+\frac{1}{n_{2}}}$, where $s_{p}=\sqrt{\frac{% (n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2}}$ is the &amp;ldquo;pooled&amp;rdquo; standard deviation from the two samples.&lt;/p&gt;
&lt;h2 id=&#34;case-2-confidence-interval-for-difference-in-means-sigma-_1neq-sigma-_2&#34;&gt;Case 2: Confidence Interval for Difference in Means $\sigma _{1}\neq \sigma _{2}$&lt;/h2&gt;
&lt;p&gt;A $(1-\alpha )100 %$ confidence interval for $\mu &lt;em&gt;{1}-\mu &lt;em&gt;{2}$ is given by $\overline{x}&lt;/em&gt;{1}-\overline{x}&lt;/em&gt;{2}\pm t_{\nu ,\alpha /2}\sqrt{\frac{s_{1}^{2}% }{n_{1}}+\frac{s_{2}^{2}}{n_{2}}}$, where $\nu =\frac{\left( \frac{s_{1}^{2}% }{n_{1}}+\frac{s_{2}^{2}}{n_{2}}\right) ^{2}}{\frac{\left( \frac{s_{1}^{2}}{% n_{1}}\right) ^{2}}{n_{1}-1}+\frac{\left( \frac{s_{2}^{2}}{n_{2}}\right) ^{2}% }{n_{2}-1}}$ .&lt;/p&gt;
&lt;p&gt;The Case 2 interval makes one less assumption than Case 1, so it is closer to the truth (in reality, it&amp;rsquo;s unlikely that the two standard deviations will be exactly equal). On the other hand, it is known that the Case 2 interval is only an approximate interval, although the approximation is quite good. As with most things in life, there is a trade-off. In general, however, Case 2 is the safer bet. The degrees of freedom parameter, $\nu$,  is easily calculated using software.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s an example:&lt;/p&gt;
&lt;p&gt;In a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8$, respectively. Calculate a $90 %$ confidence interval for mean difference in the amount of time spent studying per week between accounting and general business majors. Interpret the interval in context. Assume the population standard deviations are equal.&lt;/p&gt;
&lt;p&gt;In the next section, we&amp;rsquo;ll address another form of inference for means, hypothesis testing.&lt;/p&gt;
&lt;h1 id=&#34;hypothesis-testing-inference-for-a-single-populationprocess-mean&#34;&gt;Hypothesis Testing Inference for a Single Population/Process Mean&lt;/h1&gt;
&lt;h2 id=&#34;basic-concepts-1&#34;&gt;Basic Concepts&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s begin with a definition.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;hypothesis test&lt;/strong&gt; (also called a &lt;strong&gt;significance test&lt;/strong&gt; ) is a method of using data to evaluate the evidence about a specified value of a parameter.&lt;/p&gt;
&lt;p&gt;Every statistical test has two mutually exclusive &lt;strong&gt;hypotheses&lt;/strong&gt;, or claims about the value of a parameter.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;null hypothesis&lt;/strong&gt;, denoted $H_{0}$ and pronounced &amp;ldquo;$H$ not&amp;rdquo;¬†or &amp;ldquo;$H$ sub-zero,&amp;rdquo;¬†is typically a statement of &amp;quot; no effect,&amp;rdquo;¬†&amp;ldquo;no difference,&amp;rdquo;¬†&amp;ldquo;no special ability beyond random chance,&amp;rdquo;¬†&amp;quot; equality,&amp;rdquo;¬†etc. This is usually what the researcher wants to disprove, or reject.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;alternative hypothesis&lt;/strong&gt;, denoted $H_{a}$ or $H_{1}$, is the complement (&amp;ldquo;opposite&amp;rdquo;) of the null, and usually what the researcher is trying to establish.&lt;/p&gt;
&lt;p&gt;Hypotheses always come in pairs, and the general forms of the hypothesis pairs are given in Figure 
&lt;a href=&#34;#hyps&#34;&gt;[hyps]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;hyps&amp;rdquo;}.&lt;/p&gt;
&lt;p&gt;When performing a hypothesis test, it is possible to be wrong, or, more formally, to commit an error. Here are the two types of errors we&amp;rsquo;ll be concerned with (there is a third type as well but we won&amp;rsquo;t worry about that one).&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;Type I error&lt;/strong&gt; occurs when we reject a true null hypothesis. This is akin to a &amp;ldquo;false positive&amp;rdquo; (claiming there is an effect or difference when there isn&amp;rsquo;t).&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;Type II error&lt;/strong&gt; occurs when we don&amp;rsquo;t reject a false null hypothesis. This is akin to a &amp;ldquo;false negative&amp;rdquo; (claiming there is not an effect or difference when there is).&lt;/p&gt;
&lt;p&gt;For any given test, we don&amp;rsquo;t know if we have committed a Type I error because we don&amp;rsquo;t know what the true value of the parameter is (if we did, why do a test at all?). What we can do is specify that we want to construct our test such that the $P($Type¬†I error$)$ $=\alpha$ is &amp;ldquo;small.&amp;rdquo; That is, if we could imagine doing the test repeatedly for different samples from the same population, in the long run, we would only want to reject $H_{0}$ $\alpha (100) %$ of the time. A similar criterion exists for a Type II error: we want $P($Type II error) $=\beta$ to be small as well.&lt;/p&gt;
&lt;p&gt;The two types of errors cannot be minimized at the same time; lowering one error probability raises the other. What is done is to find tests with a small $\alpha$, and from that set, find the test that also has a small $\beta$.  For our purposes, we will only consider Type I error. It can be shown that the hypothesis tests presented next also have small $\beta$.&lt;/p&gt;
&lt;p&gt;Hypothesis testing can be thought of as a process with a number of steps. Here they are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Specify the hypothesis&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calculate the &lt;strong&gt;test statistic&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the &lt;strong&gt;p-value&lt;/strong&gt; or &lt;strong&gt;rejection region&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make a conclusion&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We already talked about (1) in Figure 
&lt;a href=&#34;#hyps&#34;&gt;[hyps]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;hyps&amp;rdquo;}. Here are some more definitions:&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;test statistic&lt;/strong&gt; is a specific function of data and the parameter value specified by the null hypothesis.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;p-value&lt;/strong&gt; of a test is the probability, assuming the null is true, that we would observe a value of the test statistic as extreme or more extreme than what we actually got.&lt;/p&gt;
&lt;p&gt;We make a conclusion based on the $p$-value or the rejection region. $P$ -values are more commonly used in practice. The judgement is simple: if the $p$-value $\leq$ $\alpha$ (i.e., our Type I¬†error probability)$$,then we &lt;strong&gt;reject&lt;/strong&gt; the null hypothesis; if the $p$-value $&amp;gt;\alpha$,  we fail to reject the null hypothesis. Most software programs report $p$-values automatically.&lt;/p&gt;
&lt;p&gt;Another approach is to specify, &lt;em&gt;before examining the data&lt;/em&gt;, the $\alpha$ (Type I error probability) that we want and determine a &lt;strong&gt;critical value&lt;/strong&gt;, a value of test statistic distribution &lt;em&gt;under the null hypothesis&lt;/em&gt; such that if the test statistic falls within that region, we reject the null hypothesis. A visual of this idea is shown in Figure 
&lt;a href=&#34;#rr&#34;&gt;[rr]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;rr&amp;rdquo;} for a test of a mean. If the calculated value of the test statistic falls in the shaded region, we would reject the null hypothesis.&lt;/p&gt;
&lt;p&gt;Here is an important fact.&lt;/p&gt;
&lt;p&gt;If a $(1-\alpha )100 %$ confidence interval &lt;strong&gt;contains&lt;/strong&gt; the value of $H_{0}$, then a test of $H_{0}:\mu =\mu _{0}$ conducted at the $\alpha$ level will **fail to reject** the null hypothesis.&lt;/p&gt;
&lt;p&gt;This fact can be proven but the intuition is simple: if the interval contains the most reasonable values of the parameter (based on the data and the confidence level) then it&amp;rsquo;s sensible that the test should not reject one of the values that lies inside the interval. This logic extends to the situation of testing the difference of two means, described below.&lt;/p&gt;
&lt;h2 id=&#34;a-hypothesis-test-for-a-single-populationprocess-mean&#34;&gt;A Hypothesis Test for a Single Population/Process Mean&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s now address a specific test. In every equation that follows, $\mu _{0}$ is the value specified value of $\mu$ that we are testing.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Specify the hypotheses&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$H_{0}:\mu \leq \mu _{0}$&lt;/li&gt;
&lt;li&gt;$H_{0}:\mu \geq \mu _{0}$&lt;/li&gt;
&lt;li&gt;$H_{0}:\mu =\mu _{0}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The alternative hypothesis is the complement of the null.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Calculate the test statistic&lt;/p&gt;
&lt;p&gt;For a test of a single mean, the test statistic is&lt;/p&gt;
&lt;p&gt;$$t_{0}=\frac{\overline{y}-\mu _{0}}{s/\sqrt{n}}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the &lt;strong&gt;rejection region&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The the rejection region (i.e., the &amp;ldquo;critical values&amp;rdquo;) for a test with the $\alpha$ level of significance would be computed as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$t_{n-1}:t_{n-1}&amp;gt;\text{ }t_{\alpha ,n-1}$&lt;/li&gt;
&lt;li&gt;$t_{n-1}:t_{n-1}&amp;lt;-t_{\alpha ,n-1}$&lt;/li&gt;
&lt;li&gt;$t_{n-1}:|t_{n-1}|&amp;gt;\text{ }t_{\alpha /2,n-1}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where $t_{n-1}$is a Student&amp;rsquo;s $t$ random variable with $n_{d}-1$ degrees of freedom. Alternatively, we can calculate the $p$-value as&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;$p = P(t_{n-1}&amp;lt;t_{0})$&lt;/li&gt;
&lt;li&gt;$p = P(t_{n-1}&amp;gt;t_{0})$&lt;/li&gt;
&lt;li&gt;$p = 2\min [P(t_{n-1}&amp;gt;t_{0}),P(t_{n-1}&amp;lt;t_{0})]$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Calculating the $p$-value for a test involving the $t$ distribution requires software.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make a conclusion.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here are some examples.&lt;/p&gt;
&lt;p&gt;A machine produces ball bearings is calibrated to produce diameters of $0.5$ inches. A sample of $10$ ball bearings from the process was taken. The average diameter was $0.493$ inches, with a standard deviation of $0.022$ inches. A quality control inspector wants to determine if the machine is not calibrated properly. Conduct a hypothesis test at the $\alpha =0.05$ level to answer his question.&lt;/p&gt;
&lt;p&gt;A specialty cell phone manufacturer wants to determine how many pictures are stored on phones used by older consumers. A survey was conducted and a random sample of $6$ older consumers reported the following numbers of pictures on their cell phones:$25$ $\ 6$ $\ \ 22$ $\ \ 26$ $\ \ 31$ $\ \ 18.$ Historically, the average number of pictures stored on phones by this consumer group has been $10.$ The company wants to know if this has changed. Use a hypothesis test at the $\alpha =0.05$ level to answer this question.&lt;/p&gt;
&lt;h2 id=&#34;a-hypothesis-test-for-two-populationprocess-means&#34;&gt;A Hypothesis Test for Two Population/Process Means&lt;/h2&gt;
&lt;p&gt;Here is the procedure for testing two means.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Specify the hypotheses.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$H_{0}:\mu _{1}-\mu _{2}\leq d$&lt;/li&gt;
&lt;li&gt;$H_{0}:\mu _{1}-\mu _{2}\geq d$&lt;/li&gt;
&lt;li&gt;$H_{0}:\mu _{1}-\mu _{2}\neq d$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where $d$ is some specified difference (usually 0).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calculate the test statistic.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 1:&lt;/strong&gt; $\sigma _{1}=\sigma _{2}$&lt;/p&gt;
&lt;p&gt;$$t_{0}=\frac{\overline{x}_{1}-\overline{x}_{2}-d}{s_{p}\sqrt{\frac{1}{n_{1}}+% \frac{1}{n_{2}}}}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 2:&lt;/strong&gt; $\sigma _{1}\neq \sigma _{2}$&lt;/p&gt;
&lt;p&gt;$$t_{0}=\frac{\overline{x}_{1}-\overline{x}_{2}-d}{\sqrt{\frac{s_{1}^{2}}{n_{1}% }+\frac{s_{2}^{2}}{n_{2}}}}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the p-value&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 1:&lt;/strong&gt; $\sigma _{1}=\sigma _{2}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p = P(t_{n_{1}+n_{2}-2}&amp;lt;t_{0})$&lt;/li&gt;
&lt;li&gt;$p = P(t_{n_{1}+n_{2}-2}&amp;gt;t_{0})$&lt;/li&gt;
&lt;li&gt;$p = 2\min [P(t_{n_{1}+n_{2}-2}&amp;gt;t_{0}),P(t_{n_{1}+n_{2}-2}&amp;lt;t_{0})]$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Case 2:&lt;/strong&gt; $\sigma _{1}\neq \sigma _{2}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p = P(t_{\nu }&amp;lt;t_{0})$&lt;/li&gt;
&lt;li&gt;$p = P(t_{\nu }&amp;gt;t_{0})$&lt;/li&gt;
&lt;li&gt;$p = 2\min [P(t_{\nu }&amp;gt;t_{0}),P(t_{\nu }&amp;lt;t_{0})]$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where $\nu =\frac{\left( \frac{s_{1}^{2}}{n_{1}}+\frac{s_{2}^{2}}{n_{2}}% \right) ^{2}}{\frac{\left( \frac{s_{1}^{2}}{n_{1}}\right) ^{2}}{n_{1}-1}+% \frac{\left( \frac{s_{2}^{2}}{n_{2}}\right) ^{2}}{n_{2}-1}}.$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make a conclusion&lt;/p&gt;
&lt;p&gt;If $p\leq \alpha$,  reject $H_{0}.$ If $p&amp;gt;\alpha$,  fail to reject $H_{0}.$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here are some examples:&lt;/p&gt;
&lt;p&gt;In a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8$, respectively. Determine at the $\alpha =0.05$ level if there is a difference in the population mean amount of time spent studying per week between accounting and general business majors. Assume the population standard deviations are equal.&lt;/p&gt;
&lt;p&gt;In a recent survey of $121$ undergraduates, students reported the number of hours per week they spend on school work. Figure 
&lt;a href=&#34;#table1&#34;&gt;[table1]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;table1&amp;rdquo;} below summarizes the data for finance and marketing majors.&lt;/p&gt;
&lt;p&gt;A researcher claims that marketing majors and finance majors do not study the same amount of time per week on average. Test the hypothesis at the $10 %$ level of significance, assuming the two population standard deviations are not the same.&lt;/p&gt;
&lt;p&gt;The critical region approach can be used for testing two means as well (and for every hypothesis test, in fact). However, because software generally reports $p$-values, and $p$-values are what are used most often in practice, we will not address this approach here.&lt;/p&gt;
&lt;p&gt;This document has been a &amp;ldquo;quick and dirty&amp;rdquo; review of hypothesis testing. The basic principles discussed here will apply to all of the tests we examine in this class. I encourage you to go back to this document each time we discuss a new test so that you can see the general pattern.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;If you want to know more about how statisticians really view the world, here&amp;rsquo;s a shameless plug for Kevin&amp;rsquo;s book: &lt;a href=&#34;http://www.amazon.com/Understanding-Advanced-Statistical-Methods-Chapman/dp/1466512105&#34;&gt;http://www.amazon.com/Understanding-Advanced-Statistical-Methods-Chapman/dp/1466512105&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Hypothesis Testing for a Proportion</title>
      <link>/courses/bana3363/6-hypothesis-testing-for-a-proportion/</link>
      <pubDate>Tue, 28 Jul 2020 15:19:27 +0000</pubDate>
      <guid>/courses/bana3363/6-hypothesis-testing-for-a-proportion/</guid>
      <description>&lt;h2 id=&#34;hypothesis-test-for-a-proportion-large-sample&#34;&gt;Hypothesis Test for a Proportion (Large Sample)&lt;/h2&gt;
&lt;p&gt;Just as the case with means, we might be interested in coming up with a hypothesis test for the population proportion. There are two approaches we could take. The most common is to use the approximate normal sampling distribution of the sample proportion for large sample sizes. This method is the one presented in most introductory books. Alternatively, we could work with the &lt;em&gt;exact&lt;/em&gt; distribution of the sample proportion. However, this approach does not lend itself well to hand calculation, and the benefits of an exact test diminish as the sample size becomes larger&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;The general four-step procedure still applies. What changes are the specifics.¬†Let&amp;rsquo;s see how.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Specify the hypothesis:&lt;/p&gt;
&lt;p&gt;Now instead of $\mu$ we work with the population proportion, $p:$&lt;/p&gt;
&lt;p&gt;a) $H_{0}: p\leq p_{0}\text{ vs. }H_{1}:p&amp;gt;p_{0}$
b) $H_{0}: p\geq p_{0}\text{ vs. }H_{1}:p&amp;lt;p_{0}$
c) $H_{0}: p=p_{0}\text{ vs. }H_{1}:p\neq p_{0}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calculate the &lt;strong&gt;test statistic&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The test statistic will always be a standard normal $z$ statistic since we assume $n$ is large:&lt;/p&gt;
&lt;p&gt;$$z=\frac{\widehat{p}-p_0}{\sqrt{p_0(1-p_{0})/n}}$$&lt;/p&gt;
&lt;p&gt;When the null hypothesis is true, the test statistic has the standard normal distribution. Notice that we use $p_{0}$ in the     denominator rather than $\widehat{p}$. This is because we always assume $H_{0}$ is true when we calculate the test statistic. If $H_{0}$ is true, $p_{0}$ is the     true population proportion by definition.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the &lt;strong&gt;p-value&lt;/strong&gt; or &lt;strong&gt;rejection region&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$pval = P(Z&amp;gt;z_{0})$&lt;/li&gt;
&lt;li&gt;$pval = P(Z&amp;lt;z_{0})$&lt;/li&gt;
&lt;li&gt;$pval = 2\min [P(Z&amp;gt;z_{0}),P(Z&amp;lt;z_{0})]$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The the rejection region (i.e., the &amp;ldquo;critical values&amp;rdquo;) for a test     with the $\alpha$ level of significance would be computed as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$z: z&amp;gt;\text{ }z_{\alpha }$&lt;/li&gt;
&lt;li&gt;$z: z&amp;lt;-z_{\alpha }$&lt;/li&gt;
&lt;li&gt;$z: |z|&amp;gt;\text{ }z_{\alpha /2}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As an     example, the rejection regions for a test at the $\alpha =0.05$     level are depicted in Figures     
&lt;a href=&#34;#left_tail&#34;&gt;[left_tail]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo;     reference=&amp;quot;left_tail&amp;rdquo;},     
&lt;a href=&#34;#right_tail&#34;&gt;[right_tail]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo;     reference=&amp;quot;right_tail&amp;rdquo;}, and     
&lt;a href=&#34;#two_tail&#34;&gt;[two_tail]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo;     reference=&amp;quot;two_tail&amp;rdquo;} . ¬†¬†&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Make a conclusion&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the $p$-value $\leq \alpha$, we reject $H_{0}$. If the $p$-value $&amp;gt;\alpha$, we fail to reject $H_{0}$. Equivalently, if $z$ lies within the rejection region, we reject $H_{0}$. If $z$ lies outside the rejection region, we fail to reject $H_{0}$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s go back to the issue of marijuana legalization. The latest General Social Survey, which was conducted in $2012$, gives the following breakdown on the question of whether marijuana should be made legal or not.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Response (2012; all ages)&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Count of Response (2012; all ages)&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;LEGAL&lt;/td&gt;
&lt;td&gt;$586$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NOT LEGAL&lt;/td&gt;
&lt;td&gt;$648$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;TOTAL&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;$1234$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;If we take LEGAL as the event of interest, then we can calculate the sample proportion as&lt;/p&gt;
&lt;p&gt;$$\widehat{p}=\frac{586}{1234}=0.475$$&lt;/p&gt;
&lt;p&gt;Suppose a research firm wishes to see if the true proportion of individuals who favor legalization is less than $0.50$ using an $\alpha =0.01$ level of significance. Then we have the following hypotheses:&lt;/p&gt;
&lt;p&gt;$$H_{0}:p\geq 0.50\text{  vs.  }H_{1}:p&amp;lt;0.50$$&lt;/p&gt;
&lt;p&gt;The test statistic is calculated as&lt;/p&gt;
&lt;p&gt;$$z=\frac{0.475-0.50}{\sqrt{0.50(1-0.50)/1234}}=-1.76$$&lt;/p&gt;
&lt;p&gt;The p-value can be calculated as&lt;/p&gt;
&lt;p&gt;$$P(Z &amp;amp;&amp;lt;&amp;amp;-1.76) = 0.0392$$&lt;/p&gt;
&lt;p&gt;Alternatively, the rejection region is defined as all values of $z$ that are greater than $z_{0.01}=2.33,$ as shown in Figure 
&lt;a href=&#34;#rr1&#34;&gt;[rr1]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;rr1&amp;rdquo;}:&lt;/p&gt;
&lt;p&gt;Using either method, we would fail-to-reject $H_{0}$ and state that we do not have enough evidence to conclude that the proportion of Americans favoring legalization exceeds $0.50$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s work some more examples of this.&lt;/p&gt;
&lt;p&gt;The number of individuals for and against legalization of marijuana in the $% 30$ and older age group is given in the table below. Conduct an $\alpha =0.05$ test of the hypothesis that the proportion of adults over $30$ who favor legalization is less than $0.50$.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Response (2012; 30 and above)&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Count of Response (2012; 30 and above)&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;LEGAL&lt;/td&gt;
&lt;td&gt;$476$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NOT LEGAL&lt;/td&gt;
&lt;td&gt;$555$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TOTAL&lt;/td&gt;
&lt;td&gt;$1031$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In a recent survey of $260$ undergraduates $83$ indicated that they had at least one tattoo. A cultural researcher wants to test whether the true proportion of undergraduates who have tattoos is different from $38%,$ the figure given in a recent report by the Pew Research Center&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. Conduct a hypothesis test at the $\alpha =0.10$ level of significance.&lt;/p&gt;
&lt;p&gt;The Centers for Disease Control and Prevention (CDC) claims &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; that $18%$ of all crashes involve some sort of &amp;ldquo;distracted driving,&amp;rdquo; which is defined as driving while engaged in another activity (such as texting, eating, and using GPS devices) that steals the driver&amp;rsquo;s attention from the road. An insurance investigator takes a random sample of $450$ crash records from one particular state and determines that $90$ indicated the primary cause of the accident was a distracted driver. Does the investigator have enough evidence at the $0.01$ level of significance to conclude that the proportion of distracted drivers in this state differs from the CDC claim?&lt;/p&gt;
&lt;p&gt;An attorney representing a group of people who took Drug X and experienced a particularly severe side effect is interested in determining whether the true proportion of the population who might take the drug and who would experience the side effect is greater than $0.02,$ the proportion reported by the drug company. After obtaining side effect reports for a random sample of $400$ patients who took the drug, she finds $11$ reported cases of the side effect. Test the hypothesis at the $\alpha =0.05$ level&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;A good overview of the various methods of forming confidence     intervals for proportions can be found on Wikipedia:     &lt;a href=&#34;http://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval&#34;&gt;http://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://www.pewsocialtrends.org/files/2010/10/millennials-confident-connected-open-to-change.pdf&#34;&gt;http://www.pewsocialtrends.org/files/2010/10/millennials-confident-connected-open-to-change.pdf&lt;/a&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://www.cdc.gov/Motorvehiclesafety/Distracted_Driving/&#34;&gt;http://www.cdc.gov/Motorvehiclesafety/Distracted_Driving/&lt;/a&gt; &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Interval for Differences in Means--Independent Samples</title>
      <link>/courses/bana3363/7-interval-for-differences-in-means-independent-samples/</link>
      <pubDate>Tue, 28 Jul 2020 15:19:27 +0000</pubDate>
      <guid>/courses/bana3363/7-interval-for-differences-in-means-independent-samples/</guid>
      <description>&lt;h2 id=&#34;confidence-intervals-for-the-difference-in-two-means-independent-samples&#34;&gt;Confidence Intervals for the Difference in Two Means (Independent Samples)&lt;/h2&gt;
&lt;p&gt;Many times we are not interested only in a single group, but in multiple $% (\geq 2)$ groups. For example, we might compare the amount of weight lost on a new drug for men $45-60$. Let Group $1$ be men $45-60$ who follow a specific healthful diet and let Group $2$ be men $45-60$ who follow the same diet but also take a weight loss drug. In the language of experimental design, Group $1$ is a &lt;strong&gt;control group&lt;/strong&gt;, the group that receives no &lt;strong&gt;treatment&lt;/strong&gt; (i.e., a specific manipulation of the members of the group), or no treatment above a reasonable standard. Group $2$ is an &lt;strong&gt;experimental&lt;/strong&gt; or &lt;strong&gt;treatment&lt;/strong&gt; &lt;strong&gt;group.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The general notation for inference about two population/process means is as follows.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Group 1&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Group 2&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sample of size&lt;/td&gt;
&lt;td&gt;$n_{1}:$ $X_{11},X_{12},\ldots ,X_{1n_{1}}$ , all either $0$ or $1$&lt;/td&gt;
&lt;td&gt;$n_{2}:$ $X_{21},X_{22},\ldots ,X_{2n_{2}},$ all either $0$ or $1$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mean:&lt;/td&gt;
&lt;td&gt;$p_{1}$&lt;/td&gt;
&lt;td&gt;$p_{2}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sample Proportion&lt;/td&gt;
&lt;td&gt;$\widehat{p_{1}}=\frac{\sum_{j}x_{1j}}{n_{1}}$ (i.e., number of successes in Group 1)&lt;/td&gt;
&lt;td&gt;$\widehat{p_{2}}=\frac{\sum_{j}x_{2j}}{n_{2}}$ (i.e., number of successes in Group 2)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Estimated Standard Deviation&lt;/td&gt;
&lt;td&gt;$\sqrt{\frac{\widehat{p_{1}}(1-\widehat{p_{1}{n_1}}$&lt;/td&gt;
&lt;td&gt;$\sqrt{\frac{\widehat{p_{2}}(1-\widehat{p_{2}}{n_{2}}}$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;By the fundamental assumption of statistics (i.e., that variability is inescapable), any one person in Group $1$ could lose more weight than any one person in Group $2$, even if the drug is effective, simply by random chance. See Figure 
&lt;a href=&#34;#drug_diet&#34;&gt;[drug_diet]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;drug_diet&amp;rdquo;}. The question, then, is not whether the drug works for everyone, but if it &amp;ldquo;works on average.&amp;rdquo;¬†The average for Group $1$ we can specify as $\mu_{1}$ and the average for Group 2 we can specify as $\mu_{2}.$ Then the question is whether $\mu_{1}=\mu_{2},$ or, equivalently, whether $\mu_{1}-\mu_{2}=0.$ We can address this question either through a confidence interval or a hypothesis test. We will discuss the interval here.&lt;/p&gt;
&lt;p&gt;There are two cases to consider:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You know (or assume) that the two population standard deviations are     the same, that is, $\sigma_{1}=\sigma_{2}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You don&amp;rsquo;t know (or don&amp;rsquo;t wish to assume) that the two population     standard deviations are the same, that is     $\sigma_{1}\neq \sigma_{2}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;case-1-confidence-interval-for-difference-in-means-sigma_1sigma_2&#34;&gt;Case 1: Confidence Interval for Difference in Means $\sigma_{1}=\sigma_{2}$&lt;/h2&gt;
&lt;p&gt;A $(1-\alpha )100%$ confidence interval for $\mu_{1}-\mu_{2}$ is given by $\overline{Y}_{1}-\overline{Y}_{2}\pm t_{n_{1}+n_{2}-2,\alpha /2}s_{p}\sqrt{% \frac{1}{n_{1}}+\frac{1}{n_{2}}}$, where $s_{p}=\sqrt{\frac{% (n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2}}$ is the &amp;ldquo;pooled&amp;rdquo; standard deviation from the two samples.&lt;/p&gt;
&lt;h2 id=&#34;case-2-confidence-interval-for-difference-in-means-sigma_1neq-sigma_2&#34;&gt;Case 2: Confidence Interval for Difference in Means $\sigma_{1}\neq \sigma_{2}$&lt;/h2&gt;
&lt;p&gt;A $(1-\alpha )100%$ confidence interval for $\mu_{1}-\mu_{2}$ is given by $\overline{Y}_{1}-\overline{Y}_{2}\pm t_{\nu ,\alpha /2}\sqrt{\frac{s_{1}^{2}% }{n_{1}}+\frac{s_{2}^{2}}{n_{2}}},$ where $\nu =\frac{\left( \frac{s_{1}^{2}% }{n_{1}}+\frac{s_{2}^{2}}{n_{2}}\right) ^{2}}{\frac{\left( \frac{s_{1}^{2}}{% n_{1}}\right) ^{2}}{n_{1}-1}+\frac{\left( \frac{s_{2}^{2}}{n_{2}}\right) ^{2}% }{n_{2}-1}}$ .&lt;/p&gt;
&lt;p&gt;The Case $2$ interval makes one fewer assumption than Case $1$, so it is closer to the truth because in reality, it is unlikely that the two standard deviations will be exactly equal. On the other hand, it is known that the Case $2$ interval is only an approximate interval, although the approximation is usually quite good. ¬†It is also more work to calculate the appropriate degrees of freedom because of the rather unpleasant looking formula for $\nu .$ As with most things in life, there is a trade off. In general, Case $2$ is the safer bet. The degrees of freedom parameter, $\nu ,$ is always calculated for you using software. In large samples $\nu$ will be large as well, so $t_{\nu ,\alpha /2}$ will almost be the same as $z_{\alpha /2},$ the critical value from the standard normal distribution.&lt;/p&gt;
&lt;p&gt;Here are some examples.&lt;/p&gt;
&lt;p&gt;A real estate agent in Ames, Iowa, has access to a large data set with $80$ variables related to the sale of nearly $3,000$ houses. The variables include total square footage, number of bathrooms, the age of each house at the time of sale, and whether the house had a number of amenities such as a fireplace or a pool&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. The agent is interested in what effect, if any, having at least one fireplace in the house has on the mean selling price. Using a Pivot Table, the agent has the following data. Calculate a $95%$ confidence interval for the difference in the mean selling price for homes with and without at least one fireplace. Interpret the interval in context. Assume the population standard deviations are not equal.&lt;/p&gt;
&lt;p&gt;In a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8,$ respectively. Calculate a $90%$ confidence interval for the mean difference in the amount of time spent studying per week between accounting and general business majors. Interpret the interval in context. Assume the population standard deviations are equal.&lt;/p&gt;
&lt;p&gt;Is there a difference in the average number of hours worked per week between those with a bachelor&amp;rsquo;s degree and those with a high school diploma? A sample of $354$ people with bachelor&amp;rsquo;s degrees and $976$ people with high school diplomas was taken. The average number of hours worked for those with bachelor&amp;rsquo;s degrees was $42.6$ with a standard deviation of $15.5$, and the average number of hours worked for those with high school diplomas was $39.5$ with a standard deviation of $15.1$.&lt;/p&gt;
&lt;p&gt;a). Construct a $99%$ confidence interval for the true mean difference in work hours, assuming the two population standard deviations are the same.&lt;/p&gt;
&lt;p&gt;The most recent General Social Survey asked American adults over $18$ years of age how many total weeks they worked in the previous year. The data is summarized in the table shown below, which was created using Excel&amp;rsquo;s pivot table feature&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. Find a $95%$ confidence interval for the true mean difference in the number of weeks spent at work for persons possessing a bachelor&amp;rsquo;s degree versus a high school diploma, assuming the population standard deviations are the same.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The data are available here:     &lt;a href=&#34;http://www.amstat.org/publications/jse/jse_data_archive.htm&#34;&gt;http://www.amstat.org/publications/jse/jse_data_archive.htm&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;See     &lt;a href=&#34;http://office.microsoft.com/en-us/excel-help/pivottable-reports-101-HA001034632.aspx&#34;&gt;http://office.microsoft.com/en-us/excel-help/pivottable-reports-101-HA001034632.aspx&lt;/a&gt;     for more information about pivot tables. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Hypothesis Test for a Difference in Proportion (Independent Samples)</title>
      <link>/courses/bana3363/11-test-for-differences-in-proportions-independent-samples/</link>
      <pubDate>Tue, 28 Jul 2020 15:19:27 +0000</pubDate>
      <guid>/courses/bana3363/11-test-for-differences-in-proportions-independent-samples/</guid>
      <description>&lt;h2 id=&#34;hypothesis-test-for-a-difference-in-proportion-independent-samples&#34;&gt;Hypothesis Test for a Difference in Proportion (Independent Samples)&lt;/h2&gt;
&lt;p&gt;Just as with means, we might be interested in testing whether two population proportions are equal. The general four-step procedure still applies. What changes are the specifics.¬†Let&amp;rsquo;s see how.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Specify the hypothesis:&lt;/p&gt;
&lt;p&gt;Now instead of $\mu &lt;em&gt;{1}-\mu &lt;em&gt;{2}$, we work with the population proportions, $p&lt;/em&gt;{1}-p&lt;/em&gt;{2}:$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$H_{0}: p_{1}-p_{2}\leq 0\text{ vs. }H_{1}: p_{1}-p_{2}&amp;gt;0$&lt;/li&gt;
&lt;li&gt;$H_{0}: p_{1}-p_{2}\geq 0\text{ vs. }H_{1}: p_{1}-p_{2}&amp;lt;0$&lt;/li&gt;
&lt;li&gt;$H_{0}: p_{1}-p_{2}=0\text{ vs. }H_{1}: p_{1}-p_{2}\neq 0$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that now we explicitly say that $d=0$. It doesn&amp;rsquo;t have to be, but if we consider nonzero differences, we must change the test statistic. The additional burden of describing two cases ($d=0$ versus $d$ being nonzero) is not warranted for our purposes, since testing the zero difference is far more common.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calculate the &lt;strong&gt;test statistic&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The test statistic will always be a $z$ statistic since we assume     $n$ is large:&lt;/p&gt;
&lt;p&gt;$$z=\frac{\widehat{p_{1}}-\widehat{p_{2}}}{\sqrt{\widehat{p}_{p}(1-p_{p})\left( \frac{1}{n_{1}}+\frac{1}{n_{2}}\right) }}$$&lt;/p&gt;
&lt;p&gt;where $\widehat{p}&lt;em&gt;{p}$ is the &amp;ldquo;pooled&amp;rdquo; proportion of successes, i.e., $\widehat{p}&lt;/em&gt;{p}=\frac{\sum_{j}x_{1j}+\sum_{j}x_{2j}}{n_{1}+n_{2}}=\frac{\text{Total Number of &amp;ldquo;Successes&amp;rdquo;}}{\text{Total Sample Size}}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Determine the rejection region or compute the $p$-value.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The the rejection region for a test with the $\alpha$ level of significance would be determined using the standard normal distribution using the following critical values. Note that these are the same critical values that we used for the test of a single proportion.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$z: z&amp;gt;\text{ }z_{\alpha }&lt;/li&gt;
&lt;li&gt;$z: z&amp;lt;-z_{\alpha }&lt;/li&gt;
&lt;li&gt;$z: |z|&amp;gt;\text{ }z_{\alpha /2}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Alternatively, we can compute the $p$-value as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$pval =P(Z&amp;gt;z)&lt;/li&gt;
&lt;li&gt;$pval =P(Z&amp;lt;z)$&lt;/li&gt;
&lt;li&gt;$pval =2\min [P(Z&amp;gt;z),P(Z&amp;lt;z)]$&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Make a conclusion&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the $p$-value $\leq \alpha ,$ we reject $H_{0}.$ If the $p$-value $% &amp;gt;\alpha ,$ we fail to reject $H_{0}.$ Equivalently, if $z$ lies within the rejection region, we reject $H_{0}$. If $z$ lies outside the rejection region, we fail to reject $H_{0}.$&lt;/p&gt;
&lt;p&gt;Time for some examples.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s go back to the issue of marijuana legalization from a previous handout Test the hypothesis that the proportion of people in the $18-30$ group who favor legalization is different from the proportion favoring legalization in the $31$ and over group. Use $\alpha =0.10.$&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Response (2012; 18-30)&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Count (18-30)&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Count (31 &amp;amp; over)&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;LEGAL&lt;/td&gt;
&lt;td&gt;$131$&lt;/td&gt;
&lt;td&gt;$455$&lt;/td&gt;
&lt;td&gt;$586$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NOT LEGAL&lt;/td&gt;
&lt;td&gt;$104$&lt;/td&gt;
&lt;td&gt;$541$&lt;/td&gt;
&lt;td&gt;$645$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TOTAL&lt;/td&gt;
&lt;td&gt;$235$&lt;/td&gt;
&lt;td&gt;$996$&lt;/td&gt;
&lt;td&gt;$1231$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;A website designer wants to examine the effectiveness of two different layouts (A and B) for a small retail client in terms of the proportion of individuals who find and click on a special coupon within the first five minutes of browsing the retailer&amp;rsquo;s site. Presumably, the more people who find and click the coupon, the more effective the layout is. For two weeks, whenever a customer accessed the retailer&amp;rsquo;s site, one of the two layouts (A or B) was randomly chosen to load on the customer&amp;rsquo;s screen. Customers were tracked by their IP addresses, numbers that uniquely identify devices over the internet. Of the customers who browsed the site for at least five minutes, the proportion who clicked the coupon using each layout was recorded. The data are below. At the ¬†$\alpha =0.01$ level of significance, can the website designer conclude that the layouts differ in effectiveness?&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Layout&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Found Coupon&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Did Not Find Coupon&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;A&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;$675$&lt;/td&gt;
&lt;td&gt;$658$&lt;/td&gt;
&lt;td&gt;$1333$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;B&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;$690$&lt;/td&gt;
&lt;td&gt;$477$&lt;/td&gt;
&lt;td&gt;$1167$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;$1365$&lt;/td&gt;
&lt;td&gt;$1135$&lt;/td&gt;
&lt;td&gt;$2500$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In a recent study, a drug manufacturer wanted to determine if a new antidepressant (Drug X) could also reduce pain that often comes with diabetes, called diabetic neuropathy. The data for the study are shown below. At the $\alpha =0.05$ ¬†level of significance, can the company conclude that the proportion of people who experience relief from Drug X is larger than for the placebo?.&lt;/p&gt;
&lt;p&gt;|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- |&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; |&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; |&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; |
|&lt;strong&gt;Dosage&lt;/strong&gt;    |&lt;strong&gt;Some/Significant Relief&lt;/strong&gt;   |&lt;strong&gt;No Relief&lt;/strong&gt;   |&lt;strong&gt;Total&lt;/strong&gt;   |
|&lt;strong&gt;Drug X&lt;/strong&gt;    |$159$                         |$122$           |$281$   |
|&lt;strong&gt;Placebo&lt;/strong&gt;   |$114$                         |$197$           |$311$    |
|Total | $273$                         |$319$           |$592$  |&lt;/p&gt;
&lt;p&gt;A researcher is interested to see if there is a difference between married and unmarried people when it comes to certain social behavior, in particular, the frequency of sexual activity. The data from the study are shown below. at the $\alpha =0.01$ level of significance, can the researcher conclude a difference exists between the two groups?&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Group&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;$\geq$&lt;strong&gt;Once/Week&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;$\mathbf{&amp;lt;}$ &lt;strong&gt;Once/Week&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Married&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;$608$&lt;/td&gt;
&lt;td&gt;$28$&lt;/td&gt;
&lt;td&gt;$636$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Unmarried&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;$553$&lt;/td&gt;
&lt;td&gt;$7$&lt;/td&gt;
&lt;td&gt;$560$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Total&lt;/td&gt;
&lt;td&gt;$1161$&lt;/td&gt;
&lt;td&gt;$35$&lt;/td&gt;
&lt;td&gt;$1196$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;example-astrology&#34;&gt;Example: Astrology&lt;/h3&gt;
&lt;p&gt;Astrology is a system of belief that, broadly speaking, claims that astronomical events somehow influence human behavior. One aspect of astrology is belief that the zodiac sign that one is born under (e.g., Pisces, Libra, Leo, Scorpio, etc.) somehow influences a person&amp;rsquo;s behavior. Is belief in astrology related significantly to education level? The $2012$ General Social Survey asked adults $18$ and over whether they believed at all in the zodiac. The results are presented below for individuals with an associate&amp;rsquo;s degree or higher (&amp;ldquo;College&amp;rdquo;) and those with a high school diploma or less (&amp;ldquo;HS or Below&amp;rdquo;). Can we conclude at the $\alpha =0.10$ level of significance that the proportion of individuals with a college degree who believe in the zodiac is smaller than that of individuals with a high school diploma or below?&lt;/p&gt;
&lt;p&gt;|&lt;strong&gt;Dosage&lt;/strong&gt;        |&lt;strong&gt;Believe&lt;/strong&gt;   |&lt;strong&gt;Do Not Believe&lt;/strong&gt;   |&lt;strong&gt;Total&lt;/strong&gt;  |
|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; |&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- |&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; &amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;  |
|&lt;strong&gt;College&lt;/strong&gt;       |$162$         |$198$                |$360$   |
|&lt;strong&gt;HS or Below&lt;/strong&gt;   |$330$         |$306$                |$636$       |          &lt;br&gt;
|&lt;strong&gt;Total&lt;/strong&gt; |$492$         |$504$                |$996$   |&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intervals for Differences in Proportions--Independent Samples</title>
      <link>/courses/bana3363/8-intervals-for-differences-in-proportions-independent-samples/</link>
      <pubDate>Tue, 28 Jul 2020 15:19:27 +0000</pubDate>
      <guid>/courses/bana3363/8-intervals-for-differences-in-proportions-independent-samples/</guid>
      <description>&lt;h2 id=&#34;confidence-interval-for-difference-in-proportion&#34;&gt;Confidence Interval for Difference in Proportion&lt;/h2&gt;
&lt;p&gt;We might be interested in the difference in the proportions of two groups who meet a certain criterion. For example, we might want to determine what type of website banner ad (one that flashes or one that is steady) leads to a higher proportion of people clicking on it. We might want to determine which of two packaging designs leads to a higher proportion of people who indicate they would buy the product. An excellent example of inference for proportions comes from prescription drug testing. Researchers might be interested to see if a new cancer drug has a reduced chance of causing nausea in patients compared to an existing drug. Opinion polls on issues such as marijuana legalization can be analyzed using techniques for differences in proportions.&lt;/p&gt;
&lt;p&gt;The general notation for inference about two population/process proportions is similar to that of two population/process means. The big difference is that the data for both groups can only take on the values of 0 or 1, with 1 indicating a &amp;ldquo;success&amp;rdquo; (either a click on an ad, an indication to purchase, or a person indicating that they did not experience nausea, for example).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Group 1&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Group 2&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sample of size&lt;/td&gt;
&lt;td&gt;$n_{1}:$ $X_{11},X_{12},\ldots ,X_{1n_{1}}$ , all either $0$ or $1$&lt;/td&gt;
&lt;td&gt;$n_{2}:$ $X_{21},X_{22},\ldots ,X_{2n_{2}},$ all either $0$ or $1$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mean:&lt;/td&gt;
&lt;td&gt;$p_{1}$&lt;/td&gt;
&lt;td&gt;$p_{2}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sample Proportion&lt;/td&gt;
&lt;td&gt;$\widehat{p_{1}}=\frac{\sum_{j}x_{1j}}{n_{1}}$ (i.e., number of successes in Group 1)&lt;/td&gt;
&lt;td&gt;$\widehat{p_{2}}=\frac{\sum_{j}x_{2j}}{n_{2}}$ (i.e., number of successes in Group 2)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Estimated Standard Deviation&lt;/td&gt;
&lt;td&gt;$\sqrt{\frac{\widehat{p_{1}}(1-\widehat{p_{1}{n_1}}$&lt;/td&gt;
&lt;td&gt;$\sqrt{\frac{\widehat{p_{2}}(1-\widehat{p_{2}}{n_{2}}}$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can now introduce the confidence interval for the difference in two proportions.&lt;/p&gt;
&lt;p&gt;A $100(1-\alpha )%$ confidence interval for the difference in ¬†two population proportions $p_{1}-p_{2}$, if the two sample sizes $n_{1}$ and $n_{2}$ are sufficiently large, is given by&lt;/p&gt;
&lt;p&gt;$$\widehat{p_{1}}-\widehat{p_{2}}\pm z_{\alpha /2}\sqrt{\frac{\widehat{p_{1}}% (1-\widehat{p_{1}})}{n_{1}}+\frac{\widehat{p_{2}}(1-\widehat{p_{2}})}{n_{2}}}$$&lt;/p&gt;
&lt;p&gt;The interval for the difference in proportions is interpreted in a similar manner as with the interval for the difference in two means. Because we are comparing two population proportions, we must phrase our interpretation in terms of being confident that the difference in proportions lies between the lower and upper bounds. Note that the interval will always fall between $-1$ and $1.$ Negative values will arise when the proportion in Group $2$ is higher than the proportion in Group $1.$ In a single sample interval for a proportion, we cannot have negative values, but when we discuss &lt;em&gt;differences in proportions,&lt;/em&gt; we certainly can.&lt;/p&gt;
&lt;p&gt;Another issue regarding interpretation should be mentioned here. Because proportions can be interpreted as percentages, it would be tempting to say, if your interval were [0.03,0.05] for example, that the difference between Group 1 and Group 2 on an issue is between 3% and 5%. This is wrong because the difference in two percents is NOT the percentage difference.&lt;/p&gt;
&lt;p&gt;For example, if the interval above represented the difference in the percentage of people in two voting populations who favored a new tax initiative, we &lt;em&gt;cannot&lt;/em&gt; say that between 3% and 5% more of Group 1 favors the initiative than Group 2. If, in a population of 12,000, 1,440 (12%) of people in Group 2 really do favor the initiative, then saying 3% more people in Group 1 favor the initiative implies that 1,483 people in that group should be in favor. But if the population size of Group 1 is, let&amp;rsquo;s say, 34,000 (populations can be different sizes) and 5,100 $\frac{5,100}{34,000}=15 %)$ were in favor, a 3% increase would suggest that 5,253 people in Group 1 would be in favor, not 1,483. Thus, we must always speak of &lt;em&gt;percentage-point differences&lt;/em&gt; when we interpret these intervals.&lt;/p&gt;
&lt;p&gt;The number of individuals for and against legalization of marijuana in the 18-30 and 31 and over age groups are given in the table below. Find a  95% confidence interval for the true difference in the proportion of people favoring marijuana legalization between the two groups.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Group&lt;/th&gt;
&lt;th&gt;Legal&lt;/th&gt;
&lt;th&gt;Not Legal&lt;/th&gt;
&lt;th&gt;Total&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;18-30 (Group 1&lt;/td&gt;
&lt;td&gt;131&lt;/td&gt;
&lt;td&gt;104&lt;/td&gt;
&lt;td&gt;235&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;31 &amp;amp; over (Group 2&lt;/td&gt;
&lt;td&gt;455&lt;/td&gt;
&lt;td&gt;541&lt;/td&gt;
&lt;td&gt;996&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TOTAL&lt;/td&gt;
&lt;td&gt;586&lt;/td&gt;
&lt;td&gt;645&lt;/td&gt;
&lt;td&gt;1231&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;A website designer wants to examine the effectiveness of two different layouts (A and B) for a small retail client in terms of the proportion of individuals who find and click on a special coupon within the first five minutes of browsing the retailer&amp;rsquo;s site. Presumably, the more people who find and click the coupon, the more effective the layout is. For two weeks, whenever a customer accessed the retailer&amp;rsquo;s site, one of the two layouts (A or B) was randomly chosen to load on the customer&amp;rsquo;s screen. Customers were tracked by their IP addresses, numbers that uniquely identify devices over the internet. Of the customers who browsed the site for at least five minutes, the proportion who clicked the coupon using each layout was recorded. The data are below. Construct a 95% confidence interval for the difference in the two proportions and interpret the interval in context.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Layout**&lt;/th&gt;
&lt;th&gt;Found Coupon**&lt;/th&gt;
&lt;th&gt;Did Not Find Coupon**&lt;/th&gt;
&lt;th&gt;Total**&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;A&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;675&lt;/td&gt;
&lt;td&gt;658&lt;/td&gt;
&lt;td&gt;1333&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;B&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;690&lt;/td&gt;
&lt;td&gt;477&lt;/td&gt;
&lt;td&gt;1167&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1365&lt;/td&gt;
&lt;td&gt;1135&lt;/td&gt;
&lt;td&gt;2500&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In a recent study, a drug manufacturer wanted to determine if a new antidepressant (Drug X) could also reduce pain that often comes with diabetes, called diabetic neuropathy. The data for the study are shown below. Calculate a 92% confidence interval for the difference in the proportion of people who experience relief from Drug X and the placebo.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Dosage&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Some/Significant Relief&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;No Relief&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Drug X&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;159&lt;/td&gt;
&lt;td&gt;122&lt;/td&gt;
&lt;td&gt;281&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Placebo&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;114&lt;/td&gt;
&lt;td&gt;197&lt;/td&gt;
&lt;td&gt;311&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Total&lt;/td&gt;
&lt;td&gt;273&lt;/td&gt;
&lt;td&gt;319&lt;/td&gt;
&lt;td&gt;592&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;A researcher is interested to see if there is a difference between married and unmarried people when it comes to certain social behavior, in particular, the frequency of sexual activity. The data from the study are shown below. Calculate a 90% confidence interval for the difference in the proportion of married and unmarried persons who have sex at least once per week.&lt;/p&gt;
&lt;p&gt;|Group      |$\geq$**Once/Week  |&amp;lt; **Once/Week | **Total  |
|&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; |&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; |&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- &amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; |
| **Married   | 608                | 28                       |  636   |
| **Unmarried  |553                | 7                        |  560  |
| Total           |1161                |35                       |  1196  |&lt;/p&gt;
&lt;p&gt;Astrology is a system of belief that, broadly speaking, claims that astronomical events somehow influence human behavior. One aspect of astrology is belief that the zodiac sign that one is born under (e.g., Pisces, Libra, Leo, Scorpio, etc.) somehow influences a person&amp;rsquo;s behavior. Is belief in astrology related significantly to education level? The 2012 General Social Survey asked adults 18 and over whether they believed at all in the zodiac. The results are presented below for individuals with an associate&amp;rsquo;s degree or higher (&amp;ldquo;College&amp;rdquo;) and those with a high school diploma or less (&amp;ldquo;HS or Below&amp;rdquo;). Calculate a 98% confidence interval for the difference in proportions between the two groups and interpret the interval in context.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Dosage&lt;/th&gt;
&lt;th&gt;Believe&lt;/th&gt;
&lt;th&gt;Do Not Believe&lt;/th&gt;
&lt;th&gt;Total&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;College&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;162&lt;/td&gt;
&lt;td&gt;198&lt;/td&gt;
&lt;td&gt;360&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;HS or Below&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;330&lt;/td&gt;
&lt;td&gt;306&lt;/td&gt;
&lt;td&gt;636&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;492&lt;/td&gt;
&lt;td&gt;504&lt;/td&gt;
&lt;td&gt;996&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>The Normal Distribution</title>
      <link>/courses/bana3363/2-the-normal-distribution/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/bana3363/2-the-normal-distribution/</guid>
      <description>&lt;h2 id=&#34;basic-concepts&#34;&gt;Basic Concepts&lt;/h2&gt;
&lt;p&gt;Because the values and associated probabilities for a continuous random variable cannot be listed, we use a &lt;strong&gt;probability density function (pdf)&lt;/strong&gt; to describe the &amp;ldquo;relative likelihoods&amp;rdquo; of various values of the function. For a pdf to be valid, 1). it must lie completely above the horizontal axis, and 2). the total area under the curve must equal $1$ exactly.&lt;/p&gt;
&lt;p&gt;We can plug in values to the pdf to graph the distribution of a continuous random variable and we can use calculus to find the mean and variance of the random variable. For any continuous random variable $X$, the probability that $X$ lies between two values, say $3$ and $8$, is denoted $P(3\leq X\leq 8)$ and would be found by taking $\int_{3}^{8}f(x)dx$, where $f(x)$ is the probability density function. In simpler terms, probabilities are found by finding the area under the curve (the integral) between two specified values and $\int$ is the integral sign from calculus.&lt;/p&gt;
&lt;p&gt;In this class, we won&amp;rsquo;t worry about finding the mean and variance of a continuous distribution, nor will we find probabilities directly. For the most part, we will restrict our attention to a few distributions for which we can use software or tables to find probabilities.&lt;/p&gt;
&lt;p&gt;For continuous distributions, the probability that the random variable $X$ equals any specific value $x$ is always $0,$ so $P(X\leq x)=P(X&amp;lt;x).$ The same goes if you reverse the inequality.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;normal distribution&lt;/strong&gt; is a continuous probability distribution that is completely defined by its mean, $\mu ,$ and standard deviation, $% \sigma .$ If a random variable $Y$ has a normal distribution with a specified mean $\mu$ and standard deviation $\sigma$, we write this as $Y% \symbol{126}N(\mu ,\sigma )$, where the $\symbol{126}$ symbol stands for &amp;ldquo;is distributed as.&amp;rdquo; Also, $Y$ has the following pdf:&lt;/p&gt;
&lt;p&gt;$$f(y|\mu ,\sigma )=\frac{1}{\sqrt{2\pi }\sigma }e^{\frac{-(y-\mu )^{2}}{% 2\sigma ^{2}}},\text{ }-\infty &amp;lt;\mu &amp;lt;\infty ;\text{ }\sigma &amp;gt;0 \label{norm_pdf}$$&lt;/p&gt;
&lt;p&gt;The possible values of $y$ range from $-\infty$ to $\infty$, but most of &amp;ldquo;the action&amp;rdquo; of the curve takes place within a narrow range. The graph of a normal distribution is, in general, &lt;strong&gt;symmetric&lt;/strong&gt; around $\mu$ and &lt;strong&gt;bell shaped&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Figure 
&lt;a href=&#34;#normcrve&#34;&gt;[normcrve]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;normcrve&amp;rdquo;} examples of normal distributions. In general, the effect of changing $\mu$ is to move the graph to the left or right on the horizontal axis, while the effect of changing $\sigma$ is to make the graph narrower (decreasing $\sigma$) or wider (increasing $\sigma$).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;(&lt;strong&gt;$68-95-99.7$&lt;/strong&gt;¬†Rule)&lt;/strong&gt;. If $Y$ has a normal distribution with mean $\mu$ and standard deviation $\sigma$, then the following are true:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;About $68%$ of the normal curve lies within $1$ standard deviation of the mean. That is, $P(\mu -\sigma \leq Y\leq \mu +\sigma )\approx 0.68$&lt;/li&gt;
&lt;li&gt;About $95%$ of the normal curve lies within $2$ standard deviations of the mean. That is, $P(\mu -2\sigma \leq Y\leq \mu +2\sigma )\approx 0.95$&lt;/li&gt;
&lt;li&gt;About $99.7%$ of the normal curve lies within $3$ standard deviation of the mean. That is, $P(\mu -3\sigma \leq Y\leq \mu +3\sigma )\approx 0.997$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A special type of normal distribution where $\mu =0$ and $\sigma =1$ is called &lt;strong&gt;the standard normal distribution&lt;/strong&gt; or &amp;ldquo;$Z$ distribution,&amp;rdquo; after the letter that we use to denote a random variable with a standard normal distribution.&lt;/p&gt;
&lt;p&gt;This is the distribution that you had a table for in BANA 2372.&lt;/p&gt;
&lt;p&gt;The standard normal distribution is so important because of the following fact.&lt;/p&gt;
&lt;p&gt;($Z$**-Score Theorem**). If $Y$ has a normal distribution with mean $\mu$ and standard deviation $\sigma$, then&lt;/p&gt;
&lt;p&gt;$$Z=\frac{Y-\mu }{\sigma }$$&lt;/p&gt;
&lt;p&gt;has a standard normal distribution&lt;/p&gt;
&lt;p&gt;For any specific value of $y$ (note the lowercase letter), we can calculate the $z$ (again, not the lowercase) score as&lt;/p&gt;
&lt;p&gt;$$z=\frac{y-\mu }{\sigma }$$&lt;/p&gt;
&lt;p&gt;So, you can &amp;ldquo;transform&amp;rdquo; any normal random variable to a standard normal simply by subtracting the mean and dividing by the standard deviation.&lt;/p&gt;
&lt;p&gt;Another important implication of equation $(\ref{z_score})$ above is that you can think of a $Z$¬†score as the number of standard deviations that an observation is away from its mean**.** This idea comes into play when we look at finding confidence intervals.&lt;/p&gt;
&lt;p&gt;The number of hits per day to a small company&amp;rsquo;s website have a normal distribution with mean $\mu =90,000$ and standard deviation $\sigma = 5,000.$ Sketch this distribution and indicate the intervals within with about $68%$, $95%$, and $99.7%$ of the hits are expected to lie.&lt;/p&gt;
&lt;h2 id=&#34;finding-normal-probabilities&#34;&gt;Finding Normal Probabilities&lt;/h2&gt;
&lt;p&gt;Essentially, there are three types of questions we can ask about a normal random variable, say, $Y$. For each of these we can &amp;ldquo;turn a question about $% Y$ into a question about $Z&amp;quot;$ using the Z-Score Theorem. The idea is to transform each side of the inequality into a $z$-score using the basic algebra concept of &amp;ldquo;whatever you do to one side you do to the other,&amp;rdquo; as shown below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;What is the probability that $Y$ is less than or equal to some     specific number $y?$     Answer:$\ P(Y\leq y)=P\left( \frac{Y-\mu }{\sigma }\leq      \frac{y-\mu }{\sigma }\right) =P(Z\leq z)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What is the probability that $Y$ is greater than or equal to some     value $y?$ Answer:     $P(Y\geq y)=1-P(Y&amp;lt;y)=1-P\left( \frac{Y-\mu }{\sigma }&amp;lt;%     \frac{y-\mu }{\sigma }\right) =1-P(Z&amp;lt;z)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What is the probability that $Y$ is between $y_{1}$ and $y_{2}?$     Answer: $P(y_{1}\leq Y\leq y_{2})=P(Y\leq y_{2})-P(Y\leq y_{1})$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$=P\left( \frac{Y-\mu }{\sigma }\leq \frac{y_{2}-\mu }{\sigma }\right)     -P\left( \frac{Y-\mu }{\sigma }\leq \frac{y_{1}-\mu }{\sigma }\right)     =P(Z\leq z_{2})-P(Z\leq z_{1})$&lt;/p&gt;
&lt;p&gt;You can see how three works by drawing a picture.&lt;/p&gt;
&lt;p&gt;The number of hits per day to a small company&amp;rsquo;s website have a normal distribution with mean $\mu = 90,000$ and standard deviation $\sigma = 5,000.$&lt;/p&gt;
&lt;p&gt;(a). Find the z-score associated with $84,000$ hits.&lt;/p&gt;
&lt;p&gt;b). Find the probability that the website will have fewer than $84,000$ hits on any given day.&lt;/p&gt;
&lt;p&gt;c). Find the probability that the website will have more than $84,000$ hits on any given day.&lt;/p&gt;
&lt;p&gt;The number of hits per day to a small company&amp;rsquo;s website have a normal distribution with mean $\mu =90,000$ and standard deviation $\sigma =$ $% 5,000.$ The company believes that customers will be dissatisfied with the site&amp;rsquo;s performance if daily hits get above $99,000$, since that might make the site less responsive. What is the probability that customers will be dissatisfied on any given day due to this issue?&lt;/p&gt;
&lt;p&gt;The number of hits per day to a small company&amp;rsquo;s website have a normal distribution with mean $\mu =90,000$ and standard deviation $\sigma =$ $% 5,000.$ The IT¬†department has concluded that if the site receives between $% 89,000$ and $97,000$ hits per day, service should be considered &amp;ldquo;acceptable&amp;rdquo; for that day. What is the probability that on any given day, service is considered acceptable?&lt;/p&gt;
&lt;p&gt;c). Adult systolic blood pressure (the top number) has a normal distribution with a mean of $120$ and a standard deviation of $20$. What percentage of adults have blood pressures greater than $140?$&lt;/p&gt;
&lt;h2 id=&#34;finding-a-normal-interval-with-a-specified-probability&#34;&gt;Finding a Normal Interval with a Specified Probability&lt;/h2&gt;
&lt;p&gt;Suppose now that I¬†want to find two values, call them $z_{L}$ and $z_{U}$, on the standard normal curve so that $P(z_{L}\leq Z\leq z_{U})=1-\alpha .$ To make things easier, we&amp;rsquo;ll take advantage of the symmetry of the normal distribution and say that $% -z_{L}=z_{U},$ that is, the two numbers are just negatives of one another. The area we want is the area between the negative and positive value of $% z_{U}.$&lt;/p&gt;
&lt;p&gt;For example, suppose we want to find two values $z_{L}$ and $z_{U}$ so that $% P(z_{L}\leq Z\leq z_{U})=0.90.$ So $1-\alpha =0.90,$ and therefore $\alpha =0.10.$ Look at Figure 
&lt;a href=&#34;#shade_norm&#34;&gt;[shade_norm]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;shade_norm&amp;rdquo;}. If we want the area *between* $z_{L}$ and $z_{U}$ to be $0.90$, that means that the area *outside* of this interval must be $0.10$. Furthermore, since we specify that $% -z_{L}=z_{U}$, then the symmetry of the distribution implies that $% P(Z&amp;gt;z_{U})=P(Z&amp;lt;z_{L})$, i.e., the area to the right of $z_{U}$ is the same as the area to the left of $z_{L}.$ That combined area must be $0.10,$ so that must mean $P(Z&amp;gt;z_{U})=P(Z&amp;lt;z_{L})=0.05$ (i.e., we have $0.05$ on either end, since $0.05+0.05=0.10).$ Therefore, we can find these two values of $z$ simply by finding $z_{U}=z_{0.05}$ and then taking the negative of this amount to find $z_{L}.$&lt;/p&gt;
&lt;p&gt;To answer this question, we can use the fact that we can solve the $z$-score formula for $y$ to &amp;ldquo;recover&amp;rdquo; an original observation if we know a specific value of $z:$&lt;/p&gt;
&lt;p&gt;$$z=\frac{y-\mu }{\sigma }\Leftrightarrow y=\sigma z+\mu$$&lt;/p&gt;
&lt;p&gt;For example, if $z=2.5$, $\mu =3$, and $\sigma =10$, then&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} y &amp;amp;=&amp;amp;\sigma z+\mu \ &amp;amp;=&amp;amp;(10)\ast 2.5+3=28\end{aligned}$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s practice this concept first.&lt;/p&gt;
&lt;p&gt;Find the original observation $(y)$ from the following $z$-scores, listed along with the mean $\mu$ and standard deviation $\sigma$.&lt;/p&gt;
&lt;p&gt;a). $z=1.23,\mu =12,\sigma =3$&lt;/p&gt;
&lt;p&gt;b). $z=-0.65,\mu =30,\sigma =11$&lt;/p&gt;
&lt;p&gt;c). $z=2.33,\mu =9,\sigma =15$&lt;/p&gt;
&lt;p&gt;The value $z_{\alpha }$ is the value of the standard normal curve that has area (i.e., probability) $\alpha$ **to its right**. That is, $% P(Z&amp;gt;z_{\alpha })=\alpha$ (and therefore, $P(Z\leq z_{\alpha })=1-\alpha )$.&lt;/p&gt;
&lt;p&gt;For example, $z_{0.30\text{ }}$is the value of $z$ such that $% P(Z&amp;gt;z_{0.30})=0.30.$ This is just notation that we will have to get used to. These values of $z$ can be found two ways. One way is to go inside the $z$ table and find $(1-\alpha )$, and then read outward to the associated row and column. An easier way is to use the **quantile function** that is found in statistics software. For example, in Excel, the function is &lt;code&gt; NORMINV&lt;/code&gt;. You supply $(1-\alpha )$ and it gives you $z_{\alpha }.$ We will focus on using tables to find these values.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s make sure we have this down.&lt;/p&gt;
&lt;p&gt;Find the values of $z_{\alpha }$ indicated below.&lt;/p&gt;
&lt;p&gt;a). $z_{0.10}$&lt;/p&gt;
&lt;p&gt;b). $z_{0.025}$&lt;/p&gt;
&lt;p&gt;c). $z_{0.98}$&lt;/p&gt;
&lt;p&gt;d). $z_{0.80}$&lt;/p&gt;
&lt;p&gt;e). $z_{0.1271}$&lt;/p&gt;
&lt;p&gt;f). $z_{0.9838}$&lt;/p&gt;
&lt;p&gt;In this case of $P(z_{L}\leq Z\leq z_{U})=0.90$, $z_{0.05}$ can be found (using software or a table) to be $1.645,$ so $z_{L}=-1.645,$ and we get the important result that $P(-1.645\leq Z\leq 1.645)=0.90$.&lt;/p&gt;
&lt;p&gt;We can state the general result:&lt;/p&gt;
&lt;p&gt;(math fact) $P(-z_{\alpha /2}\leq Z\leq z_{\alpha /2})=1-\alpha .$&lt;/p&gt;
&lt;p&gt;Once we find the value of $z$ we can &amp;ldquo;recover&amp;rdquo; the original observations by setting $y_{L}=-z_{\alpha /2}\sigma +\mu$ and $y_{L}=z_{\alpha /2}\sigma +\mu$, and therefore state that $Y$ falls within $z_{\alpha /2}$ standard deviations of the mean with probability $1-\alpha .$ Here are some practice exercises.&lt;/p&gt;
&lt;p&gt;Within how many standard deviations from the mean does $88%$ of any normal curve lie?&lt;/p&gt;
&lt;p&gt;The diameter of a hypodermic needle is measured in &amp;ldquo;gauge.&amp;rdquo; Different gauges are used depending on the type of substance that needs to be injected. Becton Company manufactures insulin needles which must have an inside diameter of $0.016$ inches. Their current manufacturing process has a standard deviation of $0.0001$ (this is high-precision business)$.$ Within what interval will $97%$ of all the needles produced by the company fall?&lt;/p&gt;
&lt;p&gt;The weights of babies at birth have a normal distribution with a mean of $119.6$ ounces and a standard deviation of $18.2$ ounces. Birth weights that are too large or too small might indicate a serious problem. What interval will contain $95%$ of birth weights? Find the answer using the $68-95-99.7$ rule and the approach just discussed. Compare your answers.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Types of Statistical Analysis</title>
      <link>/courses/bana3363/2-types-of-statistical-analysis/</link>
      <pubDate>Tue, 05 May 2020 00:00:00 +0100</pubDate>
      <guid>/courses/bana3363/2-types-of-statistical-analysis/</guid>
      <description>&lt;h2 id=&#34;descriptive-type&#34;&gt;Descriptive type&lt;/h2&gt;
&lt;p&gt;As the name suggests, the descriptive statistic is used to describe the data. It describes the basic features of the data and shows or summarizes it in a rational way.&lt;/p&gt;
&lt;p&gt;Limitation: Descriptive statistics do not allow making conclusions. You cannot get conclusions or make generalizations that extend beyond the observed data.&lt;/p&gt;
&lt;h2 id=&#34;inferential-type&#34;&gt;Inferential type&lt;/h2&gt;
&lt;p&gt;Inferential statistics is a result of more complicated mathematical estimations and allows us to infer trends about a larger population based on observed data.&lt;/p&gt;
&lt;p&gt;This type of statistical analysis is used to study the relationships between variables within a sample and you can make conclusions, generalization, or predictions about a bigger population.&lt;/p&gt;
&lt;h2 id=&#34;predictive-analytics&#34;&gt;Predictive Analytics&lt;/h2&gt;
&lt;p&gt;Predictive analytics uses techniques such as statistical algorithms and machine learning to define the likelihood of future results, behavior, and trends based on both new and historical data.&lt;/p&gt;
&lt;p&gt;It is important to note that no statistical method can predict the future with 100% certainty. Businesses use these statistics to answer the question &amp;ldquo;What might happen?&amp;rdquo;&lt;/p&gt;
&lt;h2 id=&#34;prescriptive-analytics&#34;&gt;Prescriptive Analytics&lt;/h2&gt;
&lt;p&gt;Prescriptive analytics is a study which examines data to answer the question &amp;ldquo;What should be done?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Prescriptive analysis aims to find the optimal recommendations for a decision-making process. It is all about providing advice.&lt;/p&gt;
&lt;h2 id=&#34;causal-analysis&#34;&gt;Causal analysis&lt;/h2&gt;
&lt;p&gt;Causal inference is used to hen you would like to understand and identify the reasons why things are as they are. This type of analysis answers the question &amp;quot;
Why did this happen?&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The business world is full of events that lead to failure. Causal inference seeks to identify the reasons why things occurred. It is better to find causes and to treat them instead of treating symptoms.&lt;/p&gt;
&lt;h2 id=&#34;exploratory-data-analysis-eda&#34;&gt;Exploratory data analysis (EDA)&lt;/h2&gt;
&lt;p&gt;EDA is an analysis approach that focuses on identifying general patterns in the data n to find previously unknown relationships.&lt;/p&gt;
&lt;p&gt;EDA alone should not be used for generalizing or predicting. EDA is used for taking a bird&amp;rsquo;s eye view of the data and trying to make some sense of it. Commonly, it is the first step in data analysis preformed before other formal statistical techniques.&lt;/p&gt;
&lt;h2 id=&#34;mechanistic-analysis&#34;&gt;Mechanistic analysis&lt;/h2&gt;
&lt;p&gt;The mechanistic analysis is about understanding the exact changes in given variables that lead to changes in other variables. However, mechanistic does not considered external influences.&lt;/p&gt;
&lt;p&gt;The assumption is that a give system is affected by interaction of its own components. It is useful on those systems for which there are very clear definitions.&lt;/p&gt;
&lt;p&gt;Source: &lt;a href=&#34;http://www.intellspot.com/types-statistical-analysis/&#34;&gt;http://www.intellspot.com/types-statistical-analysis/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Confidence Interval for the Mean</title>
      <link>/courses/bana3363/5-ci-for-mean/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/bana3363/5-ci-for-mean/</guid>
      <description>&lt;h2 id=&#34;concepts-of-inference&#34;&gt;Concepts of Inference&lt;/h2&gt;
&lt;p&gt;Our ultimate goal is to obtain a reasonable &amp;ldquo;guess&amp;rdquo;¬†at the true value of a process parameter (in this section, the population/process mean $\mu$) using a sample of data. Before we do that, we have to introduce some terms.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;point estimator&lt;/strong&gt; for a parameter is a particular function of the random sample that gives a single number that we hope is &amp;quot; close&amp;rdquo;¬†to the true value of the parameter.&lt;/p&gt;
&lt;p&gt;An &lt;strong&gt;interval estimator&lt;/strong&gt; is a range of values, constructed using observed data, within which the parameter is believed to fall.&lt;/p&gt;
&lt;p&gt;This sounds very technical, but it really is not so bad. An example of a point estimator is the sample mean: $\overline{X}=\frac{\sum X_{i}}{n}$. Notice how it is a function of the data (the values of $X_{i})$. In practice, we typically use the point estimator to construct an interval estimator.&lt;/p&gt;
&lt;p&gt;In theory, there are an infinite number of point estimators we could pick to estimate $\mu$, but it turns out that one, $\overline{X}$, is &amp;ldquo;the best.&amp;rdquo; The following general criteria are used to judge whether a point estimator is &amp;ldquo;good:&amp;rdquo;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Unbiasedness&lt;/strong&gt;&amp;ndash;The average of the estimator is equal to the population parameter. This means that the estimator &amp;ldquo;gets it right on average.&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Consistency&lt;/strong&gt;&amp;ndash; The estimator gets closer and closer to the true population parameter as the sample size increases. This is reflected in the sampling distribution getting increasingly narrow around the true value of the parameter.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Relative efficiency&lt;/strong&gt;&amp;ndash; Given two different estimators with the same sample size, we choose the one with the smaller variance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;How do all of these apply to $\overline{X}?$ We know from Handout 4 that the sample mean $\overline{X}$ is unbiased because the theorem in that handout states that $E(\overline{X})=% %TCIMACRO{\U{3bc} }% %BeginExpansion \mu %EndExpansion$. You can see from the variance of $\overline{X}$ , that is, $\frac{\sigma ^{2}}{n}$, that as $n$ gets larger, the variance gets smaller ($\sigma ^{2}$ does not change with $n)$. If $n$ is large enough, the variability essentially goes to $0$ and $\overline{X}$ &amp;ldquo;collapses&amp;rdquo; onto $\mu$. Lastly, it can be shown (not here) that $\overline{X}$ is has the smallest variability among all unbiased estimators, so when we use it, we are using &amp;ldquo;the best&amp;rdquo; guess of $\mu$.&lt;/p&gt;
&lt;h2 id=&#34;a-confidence-interval-for-the-mean-when-sigma-is-known-or-n-is-large&#34;&gt;A Confidence Interval for the Mean When $\sigma$ Is Known or $n$ is Large&lt;/h2&gt;
&lt;p&gt;By the Central Limit Theorem, we know that the distribution of $\overline{X}$ is approximately $N\left( 0,\frac{\sigma }{\sqrt{n}}\right)$. This implies that we can say something about any given sample average $\overline{x}$. Specifically, we know that a sample average will have about a $68%$ chance of being within one standard deviation of $\mu$ about a $95%$ chance of being within two standard deviations of $\mu$, and about a $99.7%$ chance of being within three standard deviations of $\mu$. Remember that &amp;ldquo;standard deviation&amp;rdquo; here refers to the standard deviation of $\overline{X}$, which is $\frac{\sigma }{\sqrt{n}}$, so the intervals would be $\mu \pm \frac{\sigma  }{\sqrt{n}},\mu \pm 2\frac{\sigma }{\sqrt{n}}$, and $\mu \pm 3\frac{\sigma }{% \sqrt{n}}$ respectively.&lt;/p&gt;
&lt;p&gt;Confidence intervals flip this logic around and use the fact that if  $\overline{X}$ has a $68%$ chance of being within one standard deviation of  $\mu$, then we can also say that $\mu$ has a $68%$ chance of being within one standard deviation of $\overline{X}$. This means that&lt;/p&gt;
&lt;p&gt;$$P(\mu -2\sigma /\sqrt{n}\leq \overline{X}\leq \mu +2\sigma /\sqrt{n})$$&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;$$P(\overline{X}-2\sigma /\sqrt{n}\leq \mu \leq \overline{X}+2\sigma /\sqrt{n})$$&lt;/p&gt;
&lt;p&gt;are equal. One statement can be made from the other simply by rearranging what goes in the middle using algebra.&lt;/p&gt;
&lt;p&gt;Figure 
&lt;a href=&#34;#ci_pic&#34;&gt;[ci_pic]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;ci_pic&amp;rdquo;} might make things clearer.&lt;/p&gt;
&lt;p&gt;We don&amp;rsquo;t know what $\mu$ is, but we know that the sample averages $\overline{x}$ should fall around it in a certain pattern. So we use this idea to &amp;ldquo;guess&amp;rdquo; at the location of $\mu$. If we were to take the range of numbers $\overline{x}-2\sigma /\sqrt{n}$ to $\overline{x}+2\sigma /\sqrt{n}$ as our interval estimator, about $95%$ of the time, if we took repeated samples from the population and calculated an interval for each one, the true value of $\mu$ would fall in the interval; about $5%$ of the time it wouldn&amp;rsquo;t. So we could say we were &amp;ldquo;$95%$ confident&amp;rdquo; in the interval.&lt;/p&gt;
&lt;p&gt;What if we want a different level of confidence than $68,95$, or $99.7%?$¬†Suppose we wanted to be $98%$ confident, for example? Recall from Handout 2 that $P(-z_{\alpha /2}\leq Z\leq z_{\alpha /2})=1-\alpha$. If we want $98%$ confidence, we¬†begin by setting $1-\alpha =0.98$, which means $\alpha =0.02$. Then $z_{\alpha /2}=z_{0.02/2}=z_{0.01}=2.33$. Then we substitute  $Z=\frac{\overline{X}-\mu }{\sigma /\sqrt{n}}$ in the middle and solve for  $\mu$:&lt;/p&gt;
&lt;p&gt;$$P(-2.33 &amp;amp;\leq &amp;amp;Z\leq 2.33)=0.98 \ &amp;amp;\Rightarrow &amp;amp;P(-2.33\leq \frac{\overline{X}-\mu }{\sigma /\sqrt{n}}\leq 2.33)=0.98$$
$$P(-2.33\sigma /\sqrt{n}\leq \overline{X}-\mu \leq 2.33\sigma /% \sqrt{n})=0.98$$
$$P(-2.33\sigma /\sqrt{n}-\overline{X}\leq -\mu \leq 2.33\sigma /% \sqrt{n}-\overline{X})=0.98 \ &amp;amp;&amp;amp;\text{Multiply through by}-1\text{ and rearrange to get} \ P(\overline{X}-2.33\sigma /\sqrt{n} &amp;amp;\leq &amp;amp;\mu \leq \overline{X}+2.33\sigma /% \sqrt{n})=0.98$$&lt;/p&gt;
&lt;p&gt;Then, to construct the interval, substitute in the observed sample average $\overline{x}$ to get the $98%$ confidence interval to be $\overline{x}$ $\pm 2.33\sigma /\sqrt{n}$.&lt;/p&gt;
&lt;p&gt;You can apply these steps for any confidence level you want. Therefore, we have the following:&lt;/p&gt;
&lt;p&gt;[[CI_sig_known]]{#CI_sig_known label=&amp;quot;CI_sig_known&amp;rdquo;}A $(1-\alpha )100%$ confidence interval for $\mu$ when $\sigma$ is known (or when $n$ is large) is given by&lt;/p&gt;
&lt;p&gt;$$\overline{x}\pm z_{\alpha /2}\frac{\sigma }{\sqrt{n}}$$&lt;/p&gt;
&lt;p&gt;Calculation is easy; interpretation is key. We view the mean  $%TCIMACRO{\U{3bc} }% %BeginExpansion \mu %EndExpansion$ as a &lt;em&gt;fixed&lt;/em&gt; but unknown quantity. Once we gather data and compute the sample mean $\overline{x}$ and the associated confidence interval, the interval either contains $\mu$ or it does not (see Figure 
&lt;a href=&#34;#ci_pic&#34;&gt;[ci_pic]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;ci_pic&amp;rdquo;}). In the long run, however, if you took repeated samples of size $n$ and computed the interval each time, $(1-\alpha )100%$ of them would contain $\mu$. Thus, the &amp;ldquo;confidence&amp;rdquo;¬†you have is in the &lt;em&gt;method&lt;/em&gt; used to construct an interval, &lt;em&gt;not in the particular interval&lt;/em&gt; you have constructed.&lt;/p&gt;
&lt;p&gt;Rarely do we know $\sigma$, but at times we have fairly large samples, so we can use Definition 
&lt;a href=&#34;#CI_sig_known&#34;&gt;[CI_sig_known]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;CI_sig_known&amp;rdquo;} anyway by just replacing $\sigma$ with the sample standard deviation $s$. If we have a small sample and do not know $\sigma$, we must use some other multiplier besides $z_{\alpha /2}$. For this, we use values from Student&amp;rsquo;s $t$ distribution.&lt;/p&gt;
&lt;h1 id=&#34;a-confidence-interval-for-the-mean-when-sigma-is-unknown-or-n-is-small&#34;&gt;A Confidence Interval for the Mean When $\sigma$ Is Unknown or $n$ is Small&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Student&amp;rsquo;s&lt;/strong&gt; $t$**¬†distribution with **$\n-1$**¬†degrees of freedom is the probability distribution of the quantity&lt;/p&gt;
&lt;p&gt;$$t=\frac{\overline{X}-\mu }{s/\sqrt{n}}$$&lt;/p&gt;
&lt;p&gt;As with the standard normal distribution, it is symmetric, bell-shaped, and centered at $0$. The degrees of freedom $\nu$ is a parameter that governs the width of the distribution. The notation $t_{\nu }$ refers to a random variable from a $t$ distribution with $\nu$ degrees of freedom.&lt;/p&gt;
&lt;p&gt;As you can see in Figure 
&lt;a href=&#34;#studt&#34;&gt;[studt]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;studt&amp;rdquo;}, Student&amp;rsquo;s $t$ distribution looks a lot like the standard normal. In fact, it can be shown that as $\nu$ gets very large, the the $t$ distribution &amp;ldquo;becomes&amp;rdquo; the standard normal distribution. That&amp;rsquo;s why we can use values of $z$ when $n$ is large in a confidence interval.&lt;/p&gt;
&lt;p&gt;Using the $t$ distribution, we can define a confidence interval as follows:&lt;/p&gt;
&lt;p&gt;A $(1-\alpha )100%$ confidence interval for $\mu$ when $\sigma$ is unknown (or when $n$ is small) is given by&lt;/p&gt;
&lt;p&gt;$$\overline{x}\pm t_{n-1,\alpha /2}\frac{s}{\sqrt{n}}$$&lt;/p&gt;
&lt;p&gt;with $t_{n-1,\alpha /2}$, being the value of a $t$ distribution with $n-1$ degrees of freedom that has probability $\alpha /2$ to its right.&lt;/p&gt;
&lt;p&gt;Unlike with the $z$ interval in Section 
&lt;a href=&#34;#z_int&#34;&gt;[z_int]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;z_int&amp;rdquo;}, we cannot use a table to find any confidence level that we want. We would, in theory, have to have a table for every possible value of $\nu$, which would lead to an infinite number of tables. Instead, we use one table and specify certain values of  $\alpha /2$. The table we will use looks like Figure 
&lt;a href=&#34;#t_table&#34;&gt;[t_table]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;t_table&amp;rdquo;}.&lt;/p&gt;
&lt;p&gt;The logic of the $t$ interval proceeds just like the $z$ interval, except that we work with the quantity $P(-t_{\nu ,\alpha /2}\leq t\leq t_{\nu ,\alpha /2})=1-\alpha$. So, if we want to make a $1-\alpha$ confidence interval, we need to find $t_{\alpha /2}$ in the table across the columns. Then we need to find the degrees of freedom down the rows. The intersection of the row and column gives the value of $t_{\nu },_{\alpha /2}$. Here are some &amp;ldquo;self-check&amp;rdquo; exercises.&lt;/p&gt;
&lt;p&gt;Find the values of $t_{\nu ,\alpha /2}$ for the following confidence interval scenarios.&lt;/p&gt;
&lt;p&gt;a). $n=10$, $95%$ confidence [Answer: 2.262]&lt;/p&gt;
&lt;p&gt;b). $n=8;$ $90%$ confidence [Answer: 1.895]&lt;/p&gt;
&lt;p&gt;c). $n=51$, $98%$ confidence [Answer: 2.403]&lt;/p&gt;
&lt;h1 id=&#34;examples&#34;&gt;Examples&lt;/h1&gt;
&lt;p&gt;Here are some examples.&lt;/p&gt;
&lt;p&gt;A recent survey asked, &amp;ldquo;What is the ideal number of children for a person to have?&amp;rdquo;¬†A sample of $15$ American adults aged $20$ to $30$ reported the following numbers: $1$ $2$ $0$ $3$ $4$ $2$ $0$ $1$ $3$ $4$ $2$ $2$ $2$ $3$ $8$.&lt;/p&gt;
&lt;p&gt;a). Find a $93%$ confidence interval for the mean number of children American adults in this age group believe is ideal. Assume the population standard deviation is known to be $2.1$.&lt;/p&gt;
&lt;p&gt;b). Find a $90%$ confidence interval for the mean number of children American adults in this age group believe is ideal under the more realistic assumption that you do not know $\sigma$. Hint: You need to first calculate $s$, the sample standard deviation (remember that formula?)&lt;/p&gt;
&lt;p&gt;A recent survey of $121$ undergraduates asked the question, &amp;ldquo;How much money do you have available to you right now, in cash and on a debit card?&amp;rdquo;¬†The sample mean was $104.74. Suppose the standard deviation in the population is known to be $$12$. Compute a $99%$ confidence interval for the true mean amount of money that undergraduates have on hand. Interpret the interval in context.&lt;/p&gt;
&lt;p&gt;In a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8$, respectively. Calculate $95%$ confidence intervals for the amount of time spent studying per week for both accounting and general business majors. Interpret the intervals in context.&lt;/p&gt;
&lt;p&gt;Is there a difference in the average number of hours worked per week between those with a bachelor&amp;rsquo;s degree and those with a high school diploma? A sample of $354$ people with bachelor&amp;rsquo;s degrees and $976$ people with high school diplomas was taken. The average number of hours worked for those with bachelor&amp;rsquo;s degrees was $42.6$ with a standard deviation of $15.5$, and the average number of hours worked for those with high school diplomas was $39.5$ with a standard deviation of $15.1$. Calculate $95%$ confidence intervals for the mean amount of hours worked per week for the two groups.&lt;/p&gt;
&lt;h2 id=&#34;additional-examples-of-confidence-intervals-for-a-mean&#34;&gt;Additional Examples of Confidence Intervals for a Mean&lt;/h2&gt;
&lt;p&gt;An information systems analyst is interested in estimating the mean number of hours per week that executives spend reading and answering email. In a small survey, a random sample of $56$ executives reported their best guess at how much time they spent per week using email. The average reported time was $5.1$ hours with a standard deviation of $7.6$ hours. Calculate a $99%$ confidence interval for the true mean amount of time executives spend reading and answering email.&lt;/p&gt;
&lt;p&gt;One of the many variables in the General Social Survey is &amp;ldquo;hompop,&amp;rdquo; the total number of people living in a household. In a random sample of $500$ homes, the average number of people living in a home was found to be $2.45$ with a standard deviation of $1.46.$ Find a $98%$ confidence interval for the true mean number of people living in a household.&lt;/p&gt;
&lt;p&gt;The Texas Tribune lists the salaries of persons employed with public agencies in Texas, including school districts, local governments, teaching hospitals, and universities&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. A random sample of $76$ individuals with the title of &amp;ldquo;Nurse&amp;rdquo; resulted in an average annual salary of $$50,376$ and a standard deviation of $$7,825.$ Find a $95%$ confidence interval for the true mean salary of nurses employed by public agencies in Texas. Explain why the mean might not be the best measure of the &amp;ldquo;typical&amp;rdquo; salary of a nurse.&lt;/p&gt;
&lt;p&gt;Employment is an important issue to many university students. Many people choose to go to college with the aim of finding a steady job. The most recent General Social Survey asked American adults over $18$ years of age how many weeks they worked in the previous year. The data is summarized in the table shown below, which was created using Excel&amp;rsquo;s pivot table feature &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. Find a $95%$ confidence interval for the true mean number of weeks spent at work for persons possessing a bachelor&amp;rsquo;s degree.&lt;/p&gt;
&lt;h1 id=&#34;confidence-interval-for-a-proportion&#34;&gt;Confidence Interval for a Proportion&lt;/h1&gt;
&lt;p&gt;Just as the case with means, we might be interested in coming up with a range of reasonable values for the true population proportion, given a sample of data. What we will see is that, like the confidence interval for the mean, the interval for a proportion will follow the general formula&lt;/p&gt;
&lt;p&gt;$$\lbrack estimate]\pm k\times \lbrack standard\text{ \ \ }deviation\text{ \ \  }of\text{ \ \ }estimate]$$&lt;/p&gt;
&lt;p&gt;With the interval for the mean, $k$ was either a value of the standard normal distribution, $z_{\alpha /2},$ if the true population standard deviation was known (or the sample size $n$ was large), or it was a value of from Student&amp;rsquo;s $t$ distribution, $t_{\alpha /2},_{n-1}$, if the true population standard deviation was not known. With the interval for a population proportion, we won&amp;rsquo;t have to worry about two cases; we will always use $z_{\alpha /2}$ because we will always assume $n$ is large.&lt;/p&gt;
&lt;p&gt;As an example, let&amp;rsquo;s go back to the issue of marijuana legalization. Suppose you were interested in estimating the proportion of Americans who favored full legalization. The General Social Survey&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; is an extensive demographic, behavioral, and attitudinal survey of Americans that has been administered since $1972$. The survey &amp;ldquo;takes the pulse of America&amp;rdquo; according to the National Opinion Research Center (NORC), the organization responsible for the administration of the survey at the University of Chicago. For over forty years, it has tracked the opinions of Americans on hundreds of issues, from marriage to gender roles in society, from views on the ideal number of children couples should have, to opinions on life after death, to belief in the zodiac. The data is completely free to download and analyze.&lt;/p&gt;
&lt;p&gt;The table below summarizes the results of a question asking individuals to state whether they believed marijuana should be legal or illegal.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Response&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Count of Response&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;LEGAL&lt;/td&gt;
&lt;td&gt;$586$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NOT LEGAL&lt;/td&gt;
&lt;td&gt;$648$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;TOTAL&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;$1234$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;If the proportion of people who agree with legalization the event of interest, we can define the Bernoulli random variable $X$ as taking the value $1$ if the response is &amp;ldquo;LEGAL&amp;rdquo; and $0$ otherwise. Then the sample proportion is just the average of the $0/1$ data. We can calculate the sample proportion easily as&lt;/p&gt;
&lt;p&gt;$$\widehat{p}=\frac{586}{1234}=0.47$$&lt;/p&gt;
&lt;p&gt;To justify the confidence interval formula, we recall that the sample proportion $\widehat{P}$ is approximately normally distributed with a mean of $p$ and a standard deviation of $\sqrt{p(1-p)/n}$. Using the relationship between a standard normal random variable and any normal distribution, we can therefore write&lt;/p&gt;
&lt;p&gt;$$P(-z_{\alpha /2}\leq \frac{\widehat{P}-p}{\sqrt{p(1-p)/n}}\leq z_{\alpha /2})=1-\alpha$$&lt;/p&gt;
&lt;p&gt;By rearranging this inequality, we can get that&lt;/p&gt;
&lt;p&gt;$$P(\widehat{P}-z_{\alpha /2}\sqrt{p(1-p)/n}\leq p\leq \widehat{P}+z_{\alpha /2}\sqrt{p(1-p)/n})=1-\alpha$$&lt;/p&gt;
&lt;p&gt;which is a correct but useless interval since it involves $p$, the parameter we are trying to estimate. To work around this problem, we make the natural choice to substitute the calculated value of $\widehat{p}$ for $% p$ in the formula. This gives us a confidence interval we can actually work with.&lt;/p&gt;
&lt;p&gt;A $(1-\alpha )100$% confidence interval for the true population proportion, $p$, is given by&lt;/p&gt;
&lt;p&gt;$$\widehat{p}\pm z_{\alpha /2}\sqrt{\widehat{p}(1-\widehat{p})/n}$$&lt;/p&gt;
&lt;p&gt;As with the confidence interval for a population mean, computing the interval isn&amp;rsquo;t the important part. Interpretation is key. Fortunately, with any confidence interval, our interpretation of the word &amp;ldquo;confidence&amp;rdquo; is in terms of the method. When we are, for example, $95$% confident in a given interval, we are saying that, if we were to repeatedly take samples of size $% n$ from the population, calculate the proportion, and compute the interval, then in the long run, $95$% of those intervals would contain the true value of $p$. &lt;em&gt;We cannot say that the probability is&lt;/em&gt; $95$%*¬†that any particular interval contain $p$, since we view* $p$*¬†as a fixed value. Either the specific interval we calculated contains* $p$*¬†or it does not.* We can only say that this particular interval was computed using a method that &amp;ldquo;gets it right&amp;rdquo;¬†$95$% of the time.&lt;/p&gt;
&lt;p&gt;For the marijuana example, a $95$% confidence interval is then&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} &amp;amp;&amp;amp;0.47\pm 1.96\sqrt{0.47(1-0.47)/1234} \ &amp;amp;=&amp;amp;0.44\text{ to }0.50\end{aligned}$$&lt;/p&gt;
&lt;p&gt;Thus, we are $95$% confident that the true proportion of individuals who believe that marijuana should be made legal is between $0.44$ and $0.50$. This result may seem surprising given that marijuana seems to be rising in popularity if we look at television shows, movies, and the news. The reason for the low numbers here is that the data is aggregated, or grouped, which means that younger people¬†(who might tend to be more accepting of marijuana) are put in together with older people who are likely to be less tolerant. In the first example, we will restrict our age limit to $18-30$ to see if we get different results.&lt;/p&gt;
&lt;p&gt;The number of individuals for and against legalization of marijuana in the $% 18-30$ age group is given in Figure 
&lt;a href=&#34;#grass_table&#34;&gt;[grass_table]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;grass_table&amp;rdquo;}. Calculate a $99$% confidence interval for the true proportion of individuals $18-30$ supporting legalization. Interpret the interval in context.&lt;/p&gt;
&lt;p&gt;The GSS asked the question, &amp;ldquo;Are taxes on high income people too high?&amp;rdquo; The results are shown below in Figure 
&lt;a href=&#34;#rich_tax&#34;&gt;[rich_tax]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;rich_tax&amp;rdquo;}.&lt;/p&gt;
&lt;p&gt;Calculate a $90$% confidence interval for the true proportion of individuals who feel that taxes are &amp;ldquo;about right&amp;rdquo; for high-income people. Interpret the interval in context.&lt;/p&gt;
&lt;p&gt;A social science researcher is studying Americans&amp;rsquo; beliefs about premarital sex. Using the GSS figure below,¬†Figure 
&lt;a href=&#34;#sex_table&#34;&gt;[sex_table]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;sex_table&amp;rdquo;}, calculate a $95$% confidence interval for the true proportion of Americans who believe that premarital sex is &amp;ldquo;not wrong at all&amp;rdquo; or &amp;ldquo;wrong only sometimes.&amp;rdquo;&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;See &lt;a href=&#34;http://www3.norc.org/gss+website/&#34;&gt;http://www3.norc.org/gss+website/&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;See     &lt;a href=&#34;http://office.microsoft.com/en-us/excel-help/pivottable-reports-101-HA001034632.aspx&#34;&gt;http://office.microsoft.com/en-us/excel-help/pivottable-reports-101-HA001034632.aspx&lt;/a&gt;     for more information about pivot tables. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Hypothesis Testing for a Population Mean</title>
      <link>/courses/bana3363/5-1-hypothesis-testing-basics/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>/courses/bana3363/5-1-hypothesis-testing-basics/</guid>
      <description>&lt;h1 id=&#34;hypothesis-testing-inference-for-a-single-populationprocess-mean&#34;&gt;Hypothesis Testing Inference for a Single Population/Process Mean&lt;/h1&gt;
&lt;h2 id=&#34;basic-concepts&#34;&gt;Basic Concepts&lt;/h2&gt;
&lt;p&gt;In the previous two handouts, we discussed confidence intervals for a population mean and a population proportion. Confidence intervals give us a range of reasonable values for the true parameter, based on the data at hand. That is, confidence intervals &amp;ldquo;fix the data&amp;rdquo; and create an interval of possible values of the parameter are consistent with what has been observed. &lt;strong&gt;Hypothesis testing&lt;/strong&gt; addresses the inference problem from another angle by fixing &lt;em&gt;the parameter&lt;/em&gt; at a specified value and determining if the observed data is consistent with that chosen value.&lt;/p&gt;
&lt;p&gt;To discuss hypothesis testing, we need some terms. Let&amp;rsquo;s begin with some definitions:&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;hypothesis test&lt;/strong&gt; (also called a &lt;strong&gt;significance test&lt;/strong&gt; ) is a method of using data to evaluate the evidence about a specified value of a parameter.&lt;/p&gt;
&lt;p&gt;Every statistical test has two mutually exclusive &lt;strong&gt;hypotheses&lt;/strong&gt;, or claims about the value of a parameter.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;null hypothesis&lt;/strong&gt;, denoted $H_{0}$ and pronounced &amp;ldquo;$H$ not&amp;rdquo;¬†or &amp;ldquo;$H$ sub-zero,&amp;rdquo;¬†is typically a statement of &amp;ldquo;no effect,&amp;rdquo;¬†&amp;ldquo;no difference,&amp;rdquo;¬†&amp;ldquo;no special ability beyond random chance,&amp;rdquo;¬†&amp;ldquo;equality,&amp;rdquo;¬†etc. This is usually what the researcher wants to disprove or **reject**.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;alternative hypothesis&lt;/strong&gt;, denoted $H_{1}$, is the complement (&amp;ldquo;opposite&amp;rdquo;) of the null, and usually what the researcher is trying to establish.&lt;/p&gt;
&lt;p&gt;Hypotheses always come in pairs, and the general forms of the hypothesis pairs are given in Figure 
&lt;a href=&#34;#hyps&#34;&gt;[hyps]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;hyps&amp;rdquo;}.&lt;/p&gt;
&lt;p&gt;When performing a hypothesis test, it is possible to be wrong, or, more formally, to commit an error. Here are the two types of errors we&amp;rsquo;ll be concerned with (there is a third type as well but we won&amp;rsquo;t worry about that one).&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;Type I error&lt;/strong&gt; occurs when we reject a true null hypothesis. This is akin to a &amp;ldquo;false positive&amp;rdquo; (claiming there is an effect or difference in the population when there isn&amp;rsquo;t).&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;Type II error&lt;/strong&gt; occurs when we don&amp;rsquo;t reject a false null hypothesis. This is akin to a &amp;ldquo;false negative&amp;rdquo; (claiming there is not an effect or difference in the population when there is).&lt;/p&gt;
&lt;p&gt;These errors are depicted in Figure ¬†below.&lt;/p&gt;
&lt;p&gt;For any given test, we don&amp;rsquo;t know if we have committed a Type I error because we don&amp;rsquo;t know what the true value of the parameter is (if we did, we wouldn&amp;rsquo;t be doing any testing!). What we can do is specify that we want to construct our test such that the $P($Type¬†I error$)$ $=\alpha$ is &amp;ldquo;small.&amp;rdquo; That is, if we could imagine doing the test repeatedly for different samples from the same population, in the long run, we would only want an error $\alpha (100)%$ of the time. A similar criterion exists for a Type II error: we want $P($Type II error) $=\beta$ to be small as well.&lt;/p&gt;
&lt;p&gt;The two types of errors cannot be minimized at the same time; lowering one error probability raises the other. What is done in practice is to find tests with a small $\alpha$, and from that set, find the test that also has a small $\beta$. For our purposes, we will only consider Type I error. It can be shown that the hypothesis tests presented next also have &amp;ldquo;small&amp;rdquo; $% \beta$.&lt;/p&gt;
&lt;p&gt;Hypothesis testing can be thought of as a process with a number of steps. Here they are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Define the hypotheses&lt;/li&gt;
&lt;li&gt;Calculate the &lt;strong&gt;test statistic&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Calculate the &lt;strong&gt;p-value&lt;/strong&gt; or determine the &lt;strong&gt;rejection region&lt;/strong&gt; based on (1)&lt;/li&gt;
&lt;li&gt;Make a conclusion&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We already talked about $(1)$ in Figure 
&lt;a href=&#34;#hyps&#34;&gt;[hyps]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;hyps&amp;rdquo;}. Here are some more definitions:&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;test statistic&lt;/strong&gt; is a specific function of data and the parameter value specified by the null hypothesis.&lt;/p&gt;
&lt;p&gt;The test statistic is chosen so that it has a known probability distribution (e. g. the standard normal or Student&amp;rsquo;s t) when the null hypothesis is true.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;p-value&lt;/strong&gt; of a test is the probability, &lt;em&gt;assuming the null is true&lt;/em&gt;, that we would observe a value of the test statistic as extreme or more extreme than what we actually got.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;rejection region&lt;/strong&gt; is the set of all values of the test statistic distribution for which we would reject $H_{0}$.&lt;/p&gt;
&lt;p&gt;We make a conclusion based on the $p$-value or the rejection region. In either case, we decide &lt;em&gt;before we analyze the data&lt;/em&gt; what the Type I error rate $\alpha$ should be. Typical choices are $0.05,0.01,$ or $0.10$ (in order of popularity in research). $P$-values are more commonly used in practice because they are calculated easily using software. Using $p$ -values, the judgement is simple:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if the $p$-value $\leq \alpha$, then we &lt;strong&gt;reject&lt;/strong&gt; the null hypothesis&lt;/li&gt;
&lt;li&gt;if the $p$-value $&amp;gt;\alpha$, we &lt;strong&gt;fail to reject&lt;/strong&gt; the null hypothesis&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notice that we do not say &amp;ldquo;accept the null hypothesis.&amp;rdquo; To &amp;ldquo;fail to reject&amp;rdquo;¬†means to say that there is not enough evidence to reject the null hypothesis. It is a weak conclusion, similar to the &amp;ldquo;not guilty&amp;rdquo;¬†verdict in the U. S. court system. When someone is found &amp;ldquo;not guilty,&amp;rdquo; it does not mean that the person is innocent; rather such a verdict claims lack of evidence to support a conviction. &lt;em&gt;We can never conclude that the null hypothesis is true using data&lt;/em&gt;. There is an &lt;em&gt;entire set&lt;/em&gt; of values of the parameter that you could plug into the null hypothesis that would lead to a &amp;ldquo;fail to reject&amp;rdquo; decision. This set is, in fact, the $(1-\alpha )100%$ confidence interval for the parameter (see Theorem 
&lt;a href=&#34;#ci_tst_theorem&#34;&gt;[ci_tst_theorem]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;ci_tst_theorem&amp;rdquo;} below).&lt;/p&gt;
&lt;p&gt;Alternatively, we could, using our specified value of $\alpha$, the form of the hypothesis, and the distribution of the test statistic under the null hypothesis to decide on a &lt;strong&gt;critical value&lt;/strong&gt; that defines the boundary of the &lt;strong&gt;rejection region&lt;/strong&gt; on the distribution of the test statistic. This region is a collection of values of the test statistic that have a low probability, specified as $\alpha$, of being observed if the null hypothesis is true. The logic is that if our calculated test statistic falls in the rejection region, either a very unlikely event occurred or the null hypothesis is false. Practically, &lt;em&gt;if our test statistic falls in the rejection region, we reject the null hypothesis.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A visual of this idea is shown in Figure 
&lt;a href=&#34;#rr&#34;&gt;[rr]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;rr&amp;rdquo;} for a test of a population mean with a sample size of $15$. In this example, let&amp;rsquo;s assume that under the null hypothesis, the test statistic $t_{0}$ has Student&amp;rsquo;s $t$ distribution with $n-1=14$ degrees of freedom. If the alternative hypothesis $H_{1}$ is that the true population mean $\mu &amp;gt;\mu _{0}$, for some specified $\mu _{0},$ then &amp;ldquo;large&amp;rdquo; values of $t_{0}$ would support the alternative and make us doubt the null hypothesis that $\mu \leq \mu _{0}$. Our rejection region is defined as all values of the Student&amp;rsquo;s $t$ distribution that are larger than the critical value. To determine the critical value, we would first have to set $\alpha$ to some small value, say $0.05.$ In this example, the critical value is $1.761,$ since $P(t_{n-1}&amp;gt;1.761)=0.05.$ If the calculated value of the test statistic falls in the shaded region, we would reject the null hypothesis; otherwise we would fail to reject.&lt;/p&gt;
&lt;p&gt;You might wonder why we&amp;rsquo;re using $\alpha$ for both confidence intervals and hypothesis tests. There is a good reason! Here is an important fact connecting hypothesis testing and confidence intervals:&lt;/p&gt;
&lt;p&gt;[[ci_tst_theorem]]{#ci_tst_theorem label=&amp;quot;ci_tst_theorem&amp;rdquo;}If a $(1-\alpha )100%$ confidence interval &lt;strong&gt;contains&lt;/strong&gt; the parameter value specified by $H_{0},$ then a test of $H_{0}$ conducted at the $\alpha$ level will **fail to reject** the null hypothesis.&lt;/p&gt;
&lt;p&gt;This fact can be proven but the intuition is simple: if the interval contains the most reasonable values of the parameter (based on the data and the confidence level) then it&amp;rsquo;s sensible that the test should not reject one of the values that lies inside the interval. This logic extends to all hypothesis tests by construction.&lt;/p&gt;
&lt;h2 id=&#34;a-hypothesis-test-for-a-single-populationprocess-mean&#34;&gt;A Hypothesis Test for a Single Population/Process Mean&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s now address a specific test. In every equation that follows, $\mu _{0}$ is a specified value of $\mu$, such as $6.2$, that we are testing.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Specify the hypotheses: Choose one of the three hypotheses depending upon what you are trying to show.&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} \text{(a) }H_{0} &amp;amp;:&amp;amp;\mu \leq \mu _{0}\text{ vs. }H_{1}:\mu &amp;gt;\mu _{0} \ \text{(b) }H_{0} &amp;amp;:&amp;amp;\mu \geq \mu _{0}\text{ vs. }H_{1}:\mu &amp;lt;\mu _{0}\text{ } \ \text{(c) }H_{0} &amp;amp;:&amp;amp;\mu =\mu _{0}\text{ vs. }H_{1}:\mu \neq \mu _{0}\end{aligned}$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As you can see, the alternative hypothesis is the complement or &amp;ldquo;opposite&amp;rdquo; of the null.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Calculate the test statistic:&lt;/p&gt;
&lt;p&gt;For a test of a single mean, it can be shown that the best test statistic is&lt;/p&gt;
&lt;p&gt;$$t=\frac{\overline{y}-\mu _{0}}{s/\sqrt{n}}$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Notice that we plug in the null hypothesis value of $\mu _{0}$ into the test statistic. We do this because it turns out that if we can reject for this specific value, we could also reject for any value less than (in the case of hypothesis (a)) or any value greater than (in the case of hypothesis set (b)) $\mu _{0}.$&lt;/p&gt;
&lt;p&gt;When the null hypothesis is true, $t$ has the Student&amp;rsquo;s $t$ distribution with $n-1$ degrees of freedom. Therefore, we know that $P(-t_{\alpha /2,n-1}&amp;lt;t,&amp;lt;t_{\alpha /2,n-1})=1-\alpha$. This means that there is a total area of $\alpha$ either greater than $t_{\alpha /2,n-1}$ or less than $-t_{\alpha /2,n-1}.$ We use this fact in step 3 to determine the rejection region.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Calculate the $p$-value (or determine the rejection region)&lt;/p&gt;
&lt;p&gt;In practice, software almost always reports $p$-values by default. Without software, you cannot calculate $p$-values for the Student&amp;rsquo;s $t$ distribution. Still, the concept of a $p$-value is important, so it is useful to see how they could be calculated in theory. The form of the $p$ -value depends on the form of the alternative hypothesis. The following table gives the $p$-values associated with the hypotheses in (1).
$$\begin{aligned} \text{(a) }p-value &amp;amp;=&amp;amp;P(t_{n-1}&amp;gt;t) \ \text{(b) }p-value &amp;amp;=&amp;amp;P(t_{n-1}&amp;lt;t) \ \text{(c) }p-value &amp;amp;=&amp;amp;2\min [P(t_{n-1}&amp;gt;t),P(t_{n-1}&amp;lt;t)]\end{aligned}$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The rejection regions also depend on the form of the alternative hypothesis. The general appearances of these rejection regions are given in the following figures.&lt;/p&gt;
&lt;p&gt;Referring to step 1 above, the rejection region for hypothesis set (a) shown in Figure 
&lt;a href=&#34;#right_tail&#34;&gt;[right_tail]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;right_tail&amp;rdquo;} contains all values of $t$ that are &amp;ldquo;too large,&amp;rdquo; expected to be seen with probability $\alpha$ or less. A test with rejection region in the right tail is called a &lt;strong&gt;right-tailed test.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The rejection region for hypothesis set (b) shown in Figure 
&lt;a href=&#34;#left_tail&#34;&gt;[left_tail]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;left_tail&amp;rdquo;} contains all values of $t$ that are &amp;ldquo;too small,&amp;rdquo; expected to be seen with probability $\alpha$ or less. A test with rejection region in the left tail is called a &lt;strong&gt;left-tailed test.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The rejection region for hypothesis set (c) contains all the values of $t$ that are either &amp;ldquo;too small&amp;rdquo; &lt;em&gt;or&lt;/em&gt; &amp;ldquo;too large.&amp;rdquo; Since the $t$ distribution is symmetric around 0 under the null hypotheses, by convention we split $\alpha$ into two equal parts, putting $\frac{\alpha }{2}$ area in each tail. Because there is a rejection region on either side of the distribution, this type of test is known as a &lt;strong&gt;two-tailed test.&lt;/strong&gt; See Figure 
&lt;a href=&#34;#two_tail&#34;&gt;[two_tail]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;two_tail&amp;rdquo;} below.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make a conclusion.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the $p$-value $\leq \alpha$, we reject $H_{0}.$ If the $p$-value $% &amp;gt;\alpha$, we fail to reject $H_{0}.$ Equivalently, if $t$ lies within the rejection region, we reject $H_{0}$. If $t$ lies outside the rejection region, we fail to reject $H_{0}.$&lt;/p&gt;
&lt;p&gt;Here are some examples.&lt;/p&gt;
&lt;p&gt;A machine produces ball bearings is calibrated to produce diameters of $0.5$ inches on average. A sample of $10$ ball bearings from the process was taken. The average diameter was $0.493$ inches, with a standard deviation of $0.022$ inches. A quality control inspector wants to determine if the machine is calibrated properly. Conduct a hypothesis test at the $\alpha =0.05$ level to answer his question.&lt;/p&gt;
&lt;p&gt;A specialty cell phone manufacturer wants to determine how many pictures are stored on phones used by older consumers. A survey was conducted and a random sample of six older consumers reported the following numbers of pictures on their cell phones: $13$ $\ 6$ $\ \ 9$ $\ \ 10$ $\ \ 16$ $\ 18.$ Historically, the average number of pictures stored on phones by this consumer group has been $10.$ The company wants to know if this number has increased. Use a hypothesis test at the $\alpha =0.10$ level to answer this question.&lt;/p&gt;
&lt;p&gt;A recent article on pregnancy&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; discussed a study performed in California on the effects of maternal age on birth defects. The study found that the best age for a woman to have a first child, in terms of the lowest rate of birth defects was $26.$ In the $% 2012$ General Social Survey (GSS), a sample of $846$ women were asked the question, &amp;ldquo;How old were you when you had your first child?&amp;rdquo; The average age was $25.1$ years, with a standard deviation of $5.2$ years. Test the hypothesis that the mean age of GSS respondents is less than $26$ at the $% \alpha =0.05$ level.&lt;/p&gt;
&lt;p&gt;A researcher is interested in the amount of time young men $18-30$ spend on the internet per week. In the $2012$ GSS, $121$ men aged $18-30$ reported an average time on the internet of $15.6$ hours per week, with a standard deviation of $18.8$ hours. Test the hypothesis that the average time spent on the internet for young men is greater than $14$ hours, at the $\alpha =0.01$ level. Then, construct a $99%$ confidence interval for the true average time spent on the internet.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://www.huffingtonpost.com/robin-marantz-henig/whats-the-best-age-to-hav_b_2206136.html&#34;&gt;http://www.huffingtonpost.com/robin-marantz-henig/whats-the-best-age-to-hav_b_2206136.html&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Interval Inference about Two Population/Process Means</title>
      <link>/courses/bana3363/6-1-interval-for-differences-in-means-independent-samples/</link>
      <pubDate>Tue, 28 Jul 2020 15:19:27 +0000</pubDate>
      <guid>/courses/bana3363/6-1-interval-for-differences-in-means-independent-samples/</guid>
      <description>&lt;p&gt;Many times we are not interested only in a single group, but in multiple $% (\geq 2)$ groups. For example, we might compare the amount of weight lost on a new drug for men $45-60$. Let Group $1$ be men $45-60$ who follow a specific healthful diet and let Group $2$ be men $45-60$ who follow the same diet but also take a weight loss drug. In the language of experimental design, Group $1$ is a &lt;strong&gt;control group&lt;/strong&gt;, the group that receives no &lt;strong&gt;treatment&lt;/strong&gt; (i.e., a specific manipulation of the members of the group), or no treatment above a reasonable standard. Group $2$ is an &lt;strong&gt;experimental&lt;/strong&gt; or &lt;strong&gt;treatment group.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The general notation for inference about two population/process means is as follows.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Group 1&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Group 2&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Random sample of size $n_{1}:$ $X_{11},X_{12},\ldots ,X_{1n_{1}}$&lt;/td&gt;
&lt;td&gt;$n_{2}:$ $X_{21},X_{22},\ldots ,X_{2n_{2}}$&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mean&lt;/td&gt;
&lt;td&gt;$\mu_{1}$&lt;/td&gt;
&lt;td&gt;$\mu_{2}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Standard Deviation: $\sigma_{1}$&lt;/td&gt;
&lt;td&gt;$\sigma_{2}$&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sample Mean&lt;/td&gt;
&lt;td&gt;$\overline{x}&lt;em&gt;{1}=\frac{\sum&lt;/em&gt;{j}x_{1j}}{n_{1}}$&lt;/td&gt;
&lt;td&gt;$%\overline{x}&lt;em&gt;{2}=\frac{\sum&lt;/em&gt;{j}x_{2j}}{n_{2}}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sample Standard Deviation&lt;/td&gt;
&lt;td&gt;$s_{1}=\sqrt{\frac{\sum_{j}x_{1j}^{2}-n_{1}%$&lt;/td&gt;
&lt;td&gt;$s_{2}=\sqrt{%   \overline{x}_{1}^{2}}{n_{1}-1}}\frac{\sum_{j}x_{2j}^{2}-n_{2}\overline{x}_{2}^{2}}{n_{2}-1}}$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;By the fundamental assumption of statistics (i.e., that variability is inescapable), any one person in Group $1$ could lose more weight than any one person in Group $2$, even if the drug is effective, simply by random chance. See Figure 
&lt;a href=&#34;#drug_diet&#34;&gt;[drug_diet]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;drug_diet&amp;rdquo;}. The question, then, is not whether the drug works for everyone, but if it &amp;ldquo;works on average.&amp;rdquo;¬†The average for Group $1$ we can specify as $\mu_{1}$ and the average for Group 2 we can specify as $\mu_{2}.$ Then the question is whether $\mu_{1}=\mu_{2},$ or, equivalently, whether $\mu_{1}-\mu_{2}=0.$ We can address this question either through a confidence interval or a hypothesis test. We will discuss the interval here.&lt;/p&gt;
&lt;p&gt;There are two cases to consider:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;You know (or assume) that the two population standard deviations are     the same, that is, $\sigma_{1}=\sigma_{2}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You don&amp;rsquo;t know (or don&amp;rsquo;t wish to assume) that the two population     standard deviations are the same, that is     $\sigma_{1}\neq \sigma_{2}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;case-1-confidence-interval-for-difference-in-means-sigma_1sigma_2&#34;&gt;Case 1: Confidence Interval for Difference in Means $\sigma_{1}=\sigma_{2}$&lt;/h2&gt;
&lt;p&gt;A $(1-\alpha )100%$ confidence interval for $\mu_{1}-\mu_{2}$ is given by $\overline{x}_{1}-\overline{x}_{2}\pm t_{n_{1}+n_{2}-2,\alpha /2}s_{p}\sqrt{% \frac{1}{n_{1}}+\frac{1}{n_{2}}}$, where $s_{p}=\sqrt{\frac{% (n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2}}$ is the &amp;quot;pooled&amp;quot; standard deviation from the two samples.&lt;/p&gt;
&lt;h2 id=&#34;case-2-confidence-interval-for-difference-in-means-sigma_1neq-sigma_2&#34;&gt;Case 2: Confidence Interval for Difference in Means $\sigma_{1}\neq \sigma_{2}$&lt;/h2&gt;
&lt;p&gt;A $(1-\alpha )100%$ confidence interval for $\mu_{1}-\mu_{2}$ is given by $\overline{x}_{1}-\overline{x}_{2}\pm t_{\nu ,\alpha /2}\sqrt{\frac{s_{1}^{2}% }{n_{1}}+\frac{s_{2}^{2}}{n_{2}}},$ where $\nu =\frac{\left( \frac{s_{1}^{2}% }{n_{1}}+\frac{s_{2}^{2}}{n_{2}}\right) ^{2}}{\frac{\left( \frac{s_{1}^{2}}{% n_{1}}\right) ^{2}}{n_{1}-1}+\frac{\left( \frac{s_{2}^{2}}{n_{2}}\right) ^{2}% }{n_{2}-1}}$ .&lt;/p&gt;
&lt;p&gt;The Case $2$ interval makes one less assumption than Case $1$, so it is closer to the truth (in reality, it&amp;rsquo;s unlikely that the two standard deviations will be exactly equal). On the other hand, it is known that the Case $2$ interval is only an approximate interval, although the approximation is usually quite good. As with most things in life, there is a trade-off. In general, however, Case $2$ is the safer bet. The degrees of freedom parameter, $\nu ,$ is always calculated for you using software. In large samples $\nu$ will be large as well, so $t_{\nu ,\alpha /2}$ will almost be the same as $z_{\alpha /2},$ the critical value from the standard normal distribution.&lt;/p&gt;
&lt;p&gt;Here are some examples.&lt;/p&gt;
&lt;p&gt;In a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8,$ respectively. Calculate a $90%$ confidence interval for the mean difference in the amount of time spent studying per week between accounting and general business majors. Interpret the interval in context. Assume the population standard deviations are equal.&lt;/p&gt;
&lt;p&gt;A restaurant is investigating a new device that allows restaurant users to swipe their own credit cards at the table rather than wait for the server to pick up the check. In an experiment to determine which of two methods (the new device or the usual procedure) leads to larger tips, a random sample of $10$ receipts from the usual method and $11$ receipts from the new device were gathered. The mean value of the tip for customers using the device was $12.62$ with a standard deviation of $1.72$, and the mean and standard deviation for the usual method were $14.20$ and $1.61$. Construct a $90%$ confidence for the true mean difference in work hours, assuming the two population standard deviations are equal.&lt;/p&gt;
&lt;p&gt;Is there a difference in the average number of hours worked per week between those with a bachelor&amp;rsquo;s degree and those with a high school diploma? A sample of $354$ people with bachelor&amp;rsquo;s degrees and $976$ people with high school diplomas was taken. The average number of hours worked for those with bachelor&amp;rsquo;s degrees was $42.6$ with a standard deviation of $15.5$, and the average number of hours worked for those with high school diplomas was $39.5$ with a standard deviation of $15.1$.&lt;/p&gt;
&lt;p&gt;a). Construct a $99%$ confidence interval for the true mean difference in work hours, assuming the two population standard deviations are the same.&lt;/p&gt;
&lt;p&gt;b). Explain why assuming the standard deviations were not the same would not have any practical effect on your calculation of the interval in this case.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Confidence Interval for Matched Pairs</title>
      <link>/courses/bana3363/9-confidence-interval-for-matched-pairs/</link>
      <pubDate>Tue, 28 Jul 2020 15:19:27 +0000</pubDate>
      <guid>/courses/bana3363/9-confidence-interval-for-matched-pairs/</guid>
      <description>&lt;h1 id=&#34;confidence-interval-for-the-difference-in-means-matched-pairs&#34;&gt;Confidence Interval for the Difference in Means (Matched Pairs)&lt;/h1&gt;
&lt;p&gt;The confidence interval that we examined in the last handout applies when we are working with two &lt;em&gt;independent&lt;/em&gt; samples. Often, however, it makes sense to examine the effect of a treatment on samples that are in some sense related. These samples are called &lt;strong&gt;matched pairs.&lt;/strong&gt; One way a matched pair can form is if each unit that is sampled (a person or object) is studied before and after a treatment is applied. The two sets of data&amp;ndash;measurements before and after treatment&amp;ndash;will not be independent because the same entity is being measured both times. Another way to form matched pairs is to make groups that are related genetically (e.g. twins), by proximity to one another (e.g., neighbors), by time of measurement (e.g., months of the year), by relationship to one another (e.g., husband and wife), or in some other fashion (e.g., socioeconomic status, GPA).&lt;/p&gt;
&lt;p&gt;When we have a matched pairs design, the analysis is actually easier than for the two independent samples case. What we do, essentially, is make a new variable that represents the &lt;em&gt;differences&lt;/em&gt; between the groups on the measure of interest, and then perform one-sample techniques for $\mu_{d},$ which is our notation for the population mean difference. We need to calculate $\overline{y}_{d},$ the sample mean difference, and $s_{d},$ the standard deviation of the differences. Both of these are calculated using the number of pairs, $n_{d}.$ The formula for the confidence interval is given now.&lt;/p&gt;
&lt;p&gt;A $(1-\alpha )100%$ confidence interval for the mean difference between two related groups, denoted $\mu_{d},$ is&lt;/p&gt;
&lt;p&gt;$$\overline{y_{d}}\pm t_{\alpha /2,n_{d}-1}\frac{s_{d}}{\sqrt{n_{d}}}$$&lt;/p&gt;
&lt;p&gt;Here are some examples.&lt;/p&gt;
&lt;p&gt;A sociologist wants to determine if children, on average, are more educated than their parents. Using the General Social Survey, she takes a sample of $12$ respondents and compares the number of years of formal education the respondent reported (&amp;ldquo;EDUC&amp;rdquo;) with the average of the years the respondent&amp;rsquo;s mother and father spent in formal schooling¬†(&amp;ldquo;P_EDUC&amp;rdquo;). The data are shown below. Calculate a $95%$ confidence interval for the true difference in the mean number of years of education between children and parents.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;EDUC&lt;/th&gt;
&lt;th&gt;P_EDUC&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;19&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In a recent study, the effect that cell phone use had on reaction time while driving was studied. A sample of $20$ drivers was selected and each one was asked to drive through an obstacle course. Each person&amp;rsquo;s average reaction time (in seconds) to stimuli along the course was recorded both during a drive with no cell phone use and during a drive where the subject was engaged in a casual conversation with a friend. The data are summarized in the table below. Compute a $99%$ confidence interval for the true average difference in reaction time between using a cell phone and not using a cell phone. Use the fact that $\overline{y_{d}}=0.067$ and $s_{d}=0.029.$&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Driver&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Cell&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;No_Cell&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Difference&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0.85&lt;/td&gt;
&lt;td&gt;0.78&lt;/td&gt;
&lt;td&gt;0.07&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0.91&lt;/td&gt;
&lt;td&gt;0.85&lt;/td&gt;
&lt;td&gt;0.06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0.84&lt;/td&gt;
&lt;td&gt;0.74&lt;/td&gt;
&lt;td&gt;0.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0.81&lt;/td&gt;
&lt;td&gt;0.75&lt;/td&gt;
&lt;td&gt;0.06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;0.73&lt;/td&gt;
&lt;td&gt;0.07&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;0.85&lt;/td&gt;
&lt;td&gt;0.76&lt;/td&gt;
&lt;td&gt;0.09&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0.87&lt;/td&gt;
&lt;td&gt;0.78&lt;/td&gt;
&lt;td&gt;0.09&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;0.85&lt;/td&gt;
&lt;td&gt;0.79&lt;/td&gt;
&lt;td&gt;0.06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;0.84&lt;/td&gt;
&lt;td&gt;0.77&lt;/td&gt;
&lt;td&gt;0.07&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;0.85&lt;/td&gt;
&lt;td&gt;0.82&lt;/td&gt;
&lt;td&gt;0.03&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;0.93&lt;/td&gt;
&lt;td&gt;0.85&lt;/td&gt;
&lt;td&gt;0.08&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;0.85&lt;/td&gt;
&lt;td&gt;0.79&lt;/td&gt;
&lt;td&gt;0.06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;0.85&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;0.05&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;0.86&lt;/td&gt;
&lt;td&gt;0.79&lt;/td&gt;
&lt;td&gt;0.07&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;0.91&lt;/td&gt;
&lt;td&gt;0.91&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;0.74&lt;/td&gt;
&lt;td&gt;0.06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;0.9&lt;/td&gt;
&lt;td&gt;0.82&lt;/td&gt;
&lt;td&gt;0.08&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;0.91&lt;/td&gt;
&lt;td&gt;0.83&lt;/td&gt;
&lt;td&gt;0.08&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;19&lt;/td&gt;
&lt;td&gt;0.82&lt;/td&gt;
&lt;td&gt;0.72&lt;/td&gt;
&lt;td&gt;0.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;0.83&lt;/td&gt;
&lt;td&gt;0.78&lt;/td&gt;
&lt;td&gt;0.05&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;td&gt;0.92&lt;/td&gt;
&lt;td&gt;0.82&lt;/td&gt;
&lt;td&gt;0.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;22&lt;/td&gt;
&lt;td&gt;0.85&lt;/td&gt;
&lt;td&gt;0.83&lt;/td&gt;
&lt;td&gt;0.02&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;23&lt;/td&gt;
&lt;td&gt;0.91&lt;/td&gt;
&lt;td&gt;0.88&lt;/td&gt;
&lt;td&gt;0.03&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;24&lt;/td&gt;
&lt;td&gt;0.92&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;0.12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;25&lt;/td&gt;
&lt;td&gt;0.86&lt;/td&gt;
&lt;td&gt;0.77&lt;/td&gt;
&lt;td&gt;0.09&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;26&lt;/td&gt;
&lt;td&gt;0.87&lt;/td&gt;
&lt;td&gt;0.78&lt;/td&gt;
&lt;td&gt;0.09&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;27&lt;/td&gt;
&lt;td&gt;0.79&lt;/td&gt;
&lt;td&gt;0.78&lt;/td&gt;
&lt;td&gt;0.01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;28&lt;/td&gt;
&lt;td&gt;0.84&lt;/td&gt;
&lt;td&gt;0.74&lt;/td&gt;
&lt;td&gt;0.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;29&lt;/td&gt;
&lt;td&gt;0.9&lt;/td&gt;
&lt;td&gt;0.81&lt;/td&gt;
&lt;td&gt;0.09&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;td&gt;0.84&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;0.04&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;A consumer group wants to determine if husbands and wives spend differing amounts of money on each other on Valentine&amp;rsquo;s Day. A sample of seven married couples who had been married at least one year was taken, and the data (in dollars spent) are shown in the table below. Compute a $95%$ confidence interval for the true average difference between husbands and wives in the amount of money spent on Valentine&amp;rsquo;s Day .&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Couple&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Husband&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Wife&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Difference&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;25&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;td&gt;42&lt;/td&gt;
&lt;td&gt;-21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;38&lt;/td&gt;
&lt;td&gt;55&lt;/td&gt;
&lt;td&gt;-17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;41&lt;/td&gt;
&lt;td&gt;23&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;52&lt;/td&gt;
&lt;td&gt;37&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;26&lt;/td&gt;
&lt;td&gt;-10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;26&lt;/td&gt;
&lt;td&gt;24&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;How objectively do we view ourselves in terms of attractiveness? A website offers to rate an individual based on facial characteristics on a scale from $1$ to $10$, with $10$ being most attractive, using a photo. The program incorporates current research on characteristics of human faces that are thought to be associated with attractiveness&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. A random sample of five people uploaded a photo the website. Before their scores were given, they were asked to predict what the computer would say. The data are shown below. Compute a $90%$ confidence interval for the true mean difference between a person&amp;rsquo;s self rating and the computer&amp;rsquo;s rating.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Couple&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Husband&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Wife&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Difference&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;25&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;td&gt;42&lt;/td&gt;
&lt;td&gt;-21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;38&lt;/td&gt;
&lt;td&gt;55&lt;/td&gt;
&lt;td&gt;-17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;64&lt;/td&gt;
&lt;td&gt;41&lt;/td&gt;
&lt;td&gt;23&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;52&lt;/td&gt;
&lt;td&gt;37&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;26&lt;/td&gt;
&lt;td&gt;-10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;26&lt;/td&gt;
&lt;td&gt;24&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Here is one research paper describing an attempt to build a     program to predict attractiveness:     &lt;a href=&#34;http://papers.nips.cc/paper/3111-a-humanlike-predictor-of-facial-attractiveness.pdf&#34;&gt;http://papers.nips.cc/paper/3111-a-humanlike-predictor-of-facial-attractiveness.pdf&lt;/a&gt;. Here&amp;rsquo;s a website that will rate your attractiveness with a photo:     &lt;a href=&#34;http://www.anaface.com/&#34;&gt;http://www.anaface.com/&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Test for Diff in Means, Proportions</title>
      <link>/courses/bana3363/9-1-test-for-diff-in-means-proportions/</link>
      <pubDate>Tue, 28 Jul 2020 15:19:27 +0000</pubDate>
      <guid>/courses/bana3363/9-1-test-for-diff-in-means-proportions/</guid>
      <description>&lt;h2 id=&#34;a-hypothesis-test-for-two-populationprocess-means-independent-samples&#34;&gt;A Hypothesis Test for Two Population/Process Means (Independent Samples)&lt;/h2&gt;
&lt;p&gt;Here is the procedure for testing the difference between two population/process means from independent samples. You can see that it follows the general pattern that we have seen.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Specify the hypotheses.&lt;/p&gt;
&lt;p&gt;a) $H_{0}:\mu_{1}-\mu_{2}\leq d\text{ vs. }H_{1}:\mu_{1}-\mu_{2}&amp;gt;d$ &lt;br&gt;
b) $H_{0}:\mu_{1}-\mu_{2}\geq d\text{ vs. }H_{1}:\mu_{1}-\mu_{2}&amp;lt;d$
c) $H_{0}:\mu_{1}-\mu_{2}=d\text{ vs. }H_{1}:\mu_{1}-\mu_{2}\neq d$&lt;/p&gt;
&lt;p&gt;where $-\infty &amp;lt;d$ $&amp;lt;\infty$ is some specified difference (usually     $0$, but it could be anything$)$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calculate the test statistic.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 1:&lt;/strong&gt; $\sigma _{1}=\sigma _{2}$ (the only case we willconsider in this class)&lt;/p&gt;
&lt;p&gt;$$t_{0}=\frac{\overline{x}_{1}-\overline{x}_{2}-d}{s_{p}\sqrt{\frac{1}{n_{1}}+%     \frac{1}{n_{2}}}}$$&lt;/p&gt;
&lt;p&gt;where $s_{p}=\sqrt{\frac{(n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2%     }}$ is the pooled standard deviation estimate.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 2:&lt;/strong&gt; $\sigma _{1}\neq \sigma _{2}$&lt;/p&gt;
&lt;p&gt;$$t_{0}=\frac{\overline{x}_{1}-\overline{x}_{2}-d}{\sqrt{\frac{s_{1}^{2}}{n_{1}%     }+\frac{s_{2}^{2}}{n_{2}}}}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the p-value&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 1:&lt;/strong&gt; $\sigma _{1}=\sigma _{2}$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}     \text{(a) }p &amp;amp;=&amp;amp;P(t_{n_{1}+n_{2}-2}&amp;gt;t_{0})      \text{(b) }p &amp;amp;=&amp;amp;P(t_{n_{1}+n_{2}-2}&amp;lt;t_{0})      \text{(c) }p &amp;amp;=&amp;amp;2\min [P(t_{n_{1}+n_{2}-2}&amp;gt;t_{0}),P(t_{n_{1}+n_{2}-2}&amp;lt;t_{0})]\end{aligned}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 2:&lt;/strong&gt; $\sigma _{1}\neq \sigma _{2}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p &amp;amp;=&amp;amp;P(t_{\nu }&amp;gt;t_{0})$&lt;/li&gt;
&lt;li&gt;$p &amp;amp;=&amp;amp;P(t_{\nu }&amp;lt;t_{0})$&lt;/li&gt;
&lt;li&gt;$p &amp;amp;=&amp;amp;2\min [P(t_{\nu }&amp;gt;t_{0}),P(t_{\nu }&amp;lt;t_{0})]$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where     $\nu =\frac{\left( \frac{s_{1}^{2}}{n_{1}}+\frac{s_{2}^{2}}{n_{2}}%     \right) ^{2}}{\frac{\left( \frac{s_{1}^{2}}{n_{1}}\right) ^{2}}{n_{1}-1}+%     \frac{\left( \frac{s_{2}^{2}}{n_{2}}\right) ^{2}}{n_{2}-1}}$.&lt;/p&gt;
&lt;p&gt;Alternatively, we can define the rejection region as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$t_{n_{1}+n_{2}-2} &amp;amp;:&amp;amp;t_{n_{1}+n_{2}-2}&amp;gt;\text{ }t_{\alpha,t_{n_{1}+n_{2}-2}}}$&lt;/li&gt;
&lt;li&gt;$t_{n_{1}+n_{2}-2} &amp;amp;:&amp;amp;t_{n_{1}+n_{2}-2}&amp;lt;-t_{\alpha,t_{n_{1}+n_{2}-2}}$&lt;/li&gt;
&lt;li&gt;$t_{n_{1}+n_{2}-2} &amp;amp;:&amp;amp;|t_{n_{1}+n_{2}-2}|&amp;gt;\text{ }t_{\alpha /2,t_{n_{1}+n_{2}-2}}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make a conclusion&lt;/p&gt;
&lt;p&gt;If the $p$-value $\leq \alpha$, we reject $H_{0}$. If the $p$-value     $%     &amp;gt;\alpha$, we fail to reject $H_{0}$. Equivalently, if $t_{0}$ lies     within the rejection region, we reject $H_{0}$. If $t_{0}$ lies     outside the rejection region, we fail to reject $H_{0}$.&lt;/p&gt;
&lt;p&gt;Here are some examples:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8,$ respectively. Determine at the $% \alpha =0.05$ level if there is a difference in the population mean amount of time spent studying per week between accounting and general business majors. Assume the population standard deviations are equal.&lt;/p&gt;
&lt;p&gt;In a recent survey of $121$ undergraduates, students reported the number of hours per week they spend on school work. Figure 
&lt;a href=&#34;#table1&#34;&gt;[table1]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;table1&amp;rdquo;} below summarizes the data for finance and marketing majors.&lt;/p&gt;
&lt;p&gt;A researcher wants to see if marketing majors and finance majors differ in the amount of time they study per week on average. Test the hypothesis at the $10%$ level of significance, assuming the two population standard deviations are equal. What could you say about a $90%$ confidence interval for the difference in the two group means?&lt;/p&gt;
&lt;p&gt;A clothing store wants to determine which of two methods, a security guard or a video surveillance system, would be more effective in reducing losses due to theft. For six months, the store hired a security guard and recorded monthly losses, and for another six months, the store used cameras. The table below summarizes the data. The manager of the store wants to know if the cameras are more effective at reducing the average loss, because they are cheaper to use than a guard. population standard deviations are the same.&lt;/p&gt;
&lt;p&gt;a). Test the hypothesis at the $\alpha =0.05$ level of significance, assuming the population standard deviations are the same.&lt;/p&gt;
&lt;p&gt;b). Find a $95%$ confidence interval for the true mean difference in average loss between guards and cameras, assuming the population standard deviations are the same.&lt;/p&gt;
&lt;h1 id=&#34;confidence-interval-for-difference-in-proportion&#34;&gt;Confidence Interval for Difference in Proportion&lt;/h1&gt;
&lt;p&gt;We might be interested in the difference in the proportions of two groups who meet a certain criterion. For example, we might want to determine what type of website banner ad (one that flashes or one that is steady) leads to a higher proportion of people clicking on it. We might want to determine which of two packaging designs leads to a higher proportion of people who indicate they would buy the product. An excellent example of inference for proportions comes from prescription drug testing. Researchers might be interested to see if a new cancer drug has a reduced chance of causing nausea in patients compared to an existing drug. Opinion polls on issues such as marijuana legalization can be analyzed using techniques for differences in proportions.&lt;/p&gt;
&lt;p&gt;The general notation for inference about two population/process proportions is similar to that of two population/process means. The big difference is that the data for both groups can only take on the values of $0$ or $1,$ with $1$ indicating a &amp;quot;success&amp;quot; (either a click on an ad, an indication to purchase, or a person indicating that they did not experience nausea, for example).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Group 1&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Group 2&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sample of size&lt;/td&gt;
&lt;td&gt;$n_{1}:$ $X_{11},X_{12},\ldots ,X_{1n_{1}}$ , all either $0$ or $1$&lt;/td&gt;
&lt;td&gt;$n_{2}:$ $X_{21},X_{22},\ldots ,X_{2n_{2}},$ all either $0$ or $1$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mean:&lt;/td&gt;
&lt;td&gt;$p_{1}$&lt;/td&gt;
&lt;td&gt;$p_{2}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sample Proportion&lt;/td&gt;
&lt;td&gt;$\widehat{p_{1}}=\frac{\sum_{j}x_{1j}}{n_{1}}$ (i.e., number of successes in Group 1)&lt;/td&gt;
&lt;td&gt;$\widehat{p_{2}}=\frac{\sum_{j}x_{2j}}{n_{2}}$ (i.e., number of successes in Group 2)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Estimated Standard Deviation&lt;/td&gt;
&lt;td&gt;$\sqrt{\frac{\widehat{p_{1}}(1-\widehat{p_{1}{n_1}}$&lt;/td&gt;
&lt;td&gt;$\sqrt{\frac{\widehat{p_{2}}(1-\widehat{p_{2}}{n_{2}}}$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can now introduce the confidence interval for the difference in two proportions.&lt;/p&gt;
&lt;p&gt;A $100(1-\alpha )%$ confidence interval for the difference in ¬†two population proportions $p_{1}-p_{2}$, if the two sample sizes $n_{1}$ and $% n_{2}$ are sufficiently large, is given by&lt;/p&gt;
&lt;p&gt;$$\widehat{p_{1}}-\widehat{p_{2}}\pm z_{\alpha /2}\sqrt{\frac{\widehat{p_{1}}% (1-\widehat{p_{1}})}{n_{1}}+\frac{\widehat{p_{2}}(1-\widehat{p_{2}})}{n_{2}}}$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s work with an example that should be familiar by now.&lt;/p&gt;
&lt;p&gt;The number of individuals for and against legalization of marijuana in the $% 18-30$ and $31$ and over age groups are given in the table below. Find a $% 95%$ confidence interval for the true difference in the proportion of people favoring marijuana legalization between the two groups.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Response (2012)&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Count (18-30)&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Count (31 &amp;amp; over)&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;LEGAL&lt;/td&gt;
&lt;td&gt;$131$&lt;/td&gt;
&lt;td&gt;$455$&lt;/td&gt;
&lt;td&gt;$586$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NOT LEGAL&lt;/td&gt;
&lt;td&gt;$104$&lt;/td&gt;
&lt;td&gt;$541$&lt;/td&gt;
&lt;td&gt;$645$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TOTAL&lt;/td&gt;
&lt;td&gt;$235$&lt;/td&gt;
&lt;td&gt;$996$&lt;/td&gt;
&lt;td&gt;$1231$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In a recent study, a drug manufacturer wanted to determine if a new antidepressant (Drug X) could also reduce pain that often comes with diabetes, called diabetic neuropathy. The data for the study are shown below. Calculate a $92%$ confidence interval for the difference in the proportion of people who experience relief from Drug X and the placebo.&lt;/p&gt;
&lt;p&gt;A researcher is interested to see if there is a difference between men and women when it comes to certain social behavior. The data from the study are shown below. Calculate a $90%$ confidence interval for the difference in the proportion of men and women who have had sex with an acquaintance last year.&lt;/p&gt;
&lt;p&gt;Just as with means, we might be interested in testing whether two population proportions are equal. We will discuss this in the next section.&lt;/p&gt;
&lt;h2 id=&#34;hypothesis-test-for-difference-in-proportions&#34;&gt;Hypothesis Test for Difference in Proportions&lt;/h2&gt;
&lt;p&gt;The general four-step procedure still applies. What changes are the specifics.¬†Let&amp;rsquo;s see how.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Specify the hypothesis:&lt;/p&gt;
&lt;p&gt;Now instead of $\mu &lt;em&gt;{1}-\mu &lt;em&gt;{2},$ we work with the population     proportions, $p&lt;/em&gt;{1}-p&lt;/em&gt;{2}:$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$H_{0} &amp;amp;:&amp;amp;p_{1}-p_{2}\leq 0\text{ vs. }H_{1}:p_{1}-p_{2}&amp;gt;0$&lt;/li&gt;
&lt;li&gt;$H_{0} &amp;amp;:&amp;amp;p_{1}-p_{2}\geq 0\text{ vs. }H_{1}:p_{1}-p_{2}&amp;lt;0$&lt;/li&gt;
&lt;li&gt;$H_{0} &amp;amp;:&amp;amp;p_{1}-p_{2}=0\text{ vs. }H_{1}:p_{1}-p_{2}\neq 0$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that now we explicitly say that $d=0$. It doesn&amp;rsquo;t have to be,     but if we consider nonzero differences, we must change the test     statistic. The additional burden of describing two cases ($d=0$     versus $d$ being nonzero) is not warranted, since testing the zero     difference is far more common.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calculate the &lt;strong&gt;test statistic&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The test statistic will always be a $z$ statistic since we assume     $n$ is large:&lt;/p&gt;
&lt;p&gt;$$z_{0}=\frac{\widehat{p_{1}}-\widehat{p_{2}}}{\sqrt{\widehat{p}_{p}(1-p_{p})\left( \frac{1}{n_{1}}+\frac{1}{n_{2}}\right) }}$$&lt;/p&gt;
&lt;p&gt;where $\widehat{p}&lt;em&gt;{p}$ is the &amp;quot;pooled&amp;quot; proportion of successes,     i.e., $\widehat{p}&lt;/em&gt;{p}=\frac{\sum_{j}x_{1j}+\sum_{j}x_{2j}}{n_{1}+n_{2}}=\frac{\text{Total Number of &amp;ldquo;Successes&amp;rdquo;}}{\text{Total Sample Size}}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the &lt;strong&gt;p-value&lt;/strong&gt; or &lt;strong&gt;rejection region&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$pval &amp;amp;=&amp;amp;P(Z&amp;gt;z_{0})$&lt;/li&gt;
&lt;li&gt;$pval &amp;amp;=&amp;amp;P(Z&amp;lt;z_{0})$&lt;/li&gt;
&lt;li&gt;$pval &amp;amp;=&amp;amp;2\min [P(Z&amp;gt;z_{0}),P(Z&amp;lt;z_{0})]$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The the rejection region (i.e., the &amp;quot;critical values&amp;quot;) for a test     with the $%     \alpha$ level of significance would be computed as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$z &amp;amp;:&amp;amp;z&amp;gt;\text{ }z_{\alpha }$&lt;/li&gt;
&lt;li&gt;$z &amp;amp;:&amp;amp;z&amp;lt;-z_{\alpha }$&lt;/li&gt;
&lt;li&gt;$z &amp;amp;:&amp;amp;|z|&amp;gt;\text{ }z_{\alpha /2}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make a conclusion&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the $p$-value $\leq \alpha$, we reject $H_{0}$. If the $p$-value $% &amp;gt;\alpha$, we fail to reject $H_{0}$. Equivalently, if $z_{0}$ lies within the rejection region, we reject $H_{0}$. If $z_{0}$ lies outside the rejection region, we fail to reject $H_{0}$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s go back to the issue of marijuana legalization. Test the hypothesis that the proportion of people in the $18-30$ group who favor legalization is different from the proportion favoring legalization in the $31$ and over group. Use $\alpha =0.10$.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Response (2012; 18-30)&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Count (18-30)&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Count (31 &amp;amp; over)&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;LEGAL&lt;/td&gt;
&lt;td&gt;$131$&lt;/td&gt;
&lt;td&gt;$455$&lt;/td&gt;
&lt;td&gt;$586$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;NOT LEGAL&lt;/td&gt;
&lt;td&gt;$104$&lt;/td&gt;
&lt;td&gt;$541$&lt;/td&gt;
&lt;td&gt;$645$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TOTAL&lt;/td&gt;
&lt;td&gt;$235$&lt;/td&gt;
&lt;td&gt;$996$&lt;/td&gt;
&lt;td&gt;$1231$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For the drug study example, test the hypothesis that the proportion getting relief from Drug X is greater than that getting relief from the placebo at the $\alpha =0.01$ level.&lt;/p&gt;
&lt;p&gt;For the behavior study, test the hypothesis of a difference in the proportion of men and women who have had sex with an acquaintance last year at the $\alpha =0.05$ level.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Test for Differences in Proportions--Independent Samples</title>
      <link>/courses/bana3363/10-1-test-for-differences-in-proportions-independent-samples/</link>
      <pubDate>Tue, 28 Jul 2020 15:19:27 +0000</pubDate>
      <guid>/courses/bana3363/10-1-test-for-differences-in-proportions-independent-samples/</guid>
      <description>&lt;h2 id=&#34;confidence-interval-for-difference-in-proportion&#34;&gt;Confidence Interval for Difference in Proportion&lt;/h2&gt;
&lt;p&gt;We might be interested in the difference in the proportions of two groups who meet a certain criterion. For example, we might want to determine what type of website banner ad (one that flashes or one that is steady) leads to a higher proportion of people clicking on it. We might want to determine which of two packaging designs leads to a higher proportion of people who indicate they would buy the product. An excellent example of inference for proportions comes from prescription drug testing. Researchers might be interested to see if a new cancer drug has a reduced chance of causing nausea in patients compared to an existing drug. Opinion polls on issues such as marijuana legalization can be analyzed using techniques for differences in proportions.&lt;/p&gt;
&lt;p&gt;The general notation for inference about two population/process proportions is similar to that of two population/process means. The big difference is that the data for both groups can only take on the values of $0$ or $1,$ with $1$ indicating a &amp;quot;success&amp;quot; (either a click on an ad, an indication to purchase, or a person indicating that they did not experience nausea, for example).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Group 1&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Group 2&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Sample of size&lt;/td&gt;
&lt;td&gt;$n_{1}:$ $X_{11},X_{12},\ldots ,X_{1n_{1}}$ , all either $0$ or $1$&lt;/td&gt;
&lt;td&gt;$n_{2}:$ $X_{21},X_{22},\ldots ,X_{2n_{2}},$ all either $0$ or $1$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mean&lt;/td&gt;
&lt;td&gt;$p_{1}$&lt;/td&gt;
&lt;td&gt;$p_{2}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Sample Proportion&lt;/td&gt;
&lt;td&gt;$\widehat{p_{1}}=\frac{\sum_{j}x_{1j}}{n_{1}}$ (i.e., number of successes in Group 1)&lt;/td&gt;
&lt;td&gt;$\widehat{p_{2}}=\frac{\sum_{j}x_{2j}}{n_{2}}$ (i.e., number of successes in Group 2)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Estimated Standard Deviation&lt;/td&gt;
&lt;td&gt;$\sqrt{\frac{\widehat{p_{1}}(1-\widehat{p_{1}})$&lt;/td&gt;
&lt;td&gt;$\sqrt{\frac{\widehat{p_{2}}(1-%   }{n_{1}}}\widehat{p_{2}})}{n_{2}}}$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can now introduce the confidence interval for the difference in two proportions.&lt;/p&gt;
&lt;p&gt;A $100(1-\alpha )%$ confidence interval for the difference in ¬†two population proportions $p_{1}-p_{2}$, if the two sample sizes $n_{1}$ and $% n_{2}$ are sufficiently large, is given by&lt;/p&gt;
&lt;p&gt;$$\widehat{p_{1}}-\widehat{p_{2}}\pm z_{\alpha /2}\sqrt{\frac{\widehat{p_{1}}% (1-\widehat{p_{1}})}{n_{1}}+\frac{\widehat{p_{2}}(1-\widehat{p_{2}})}{n_{2}}}$$&lt;/p&gt;
&lt;p&gt;The interval for the difference in proportions is interpreted in a similar manner as with the interval for the difference in two means. Because we are comparing two population proportions, we must phrase our interpretation in terms of being confident that the difference in proportions lies between the lower and upper bounds. Note that the interval will always fall between $-1$ and $1.$ Negative values will arise when the proportion in Group $2$ is higher than the proportion in Group $1.$ In a single sample interval for a proportion, we cannot have negative values, but when we discuss &lt;em&gt;differences in proportions,&lt;/em&gt; we certainly can.&lt;/p&gt;
&lt;p&gt;Another issue regarding interpretation should be mentioned here. Because proportions can be interpreted as percentages, it would be tempting to say, if your interval were $[0.03,0.05]$ for example, that the difference between Group 1 and Group 2 on an issue is between $3%$ and $5%$. &lt;em&gt;This is wrong because the difference in two percents is NOT the percentage difference.&lt;/em&gt; For example, if the interval above represented the difference in the percentage of people in two voting populations who favored a new tax initiative, we &lt;em&gt;cannot&lt;/em&gt; say that between $3%$ and $5%$ more of Group $% 1$ favors the initiative than Group $2$. If, in a population of $12,000,$ $% 1,440$ $(12%)$ of people in Group 2 really do favor the initiative, then saying $3%$ more people in Group 1 favor the initiative implies that $1,483$ people in that group should be in favor. But if the population size of Group 1 is, let&amp;rsquo;s say, $34,000$ (populations can be different sizes) and $5,100$ $% \left( \frac{5,100}{34,000}=15%\right)$ were in favor, a $3%$ increase would suggest that $5,253$ people in Group 1 would be in favor, not $1,483.$ Thus, we must always speak of &lt;em&gt;percentage-point differences&lt;/em&gt; when we interpret these intervals.&lt;/p&gt;
&lt;p&gt;The number of individuals for and against legalization of marijuana in the $% 18-30$ and $31$ and over age groups are given in the table below. Find a $% 95%$ confidence interval for the true difference in the proportion of people favoring marijuana legalization between the two groups.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Group&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Legal&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Not Legal&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;18-30 (Group 1)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;$131$&lt;/td&gt;
&lt;td&gt;$104$&lt;/td&gt;
&lt;td&gt;$235$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;31 &amp;amp; over (Group 2)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;$455$&lt;/td&gt;
&lt;td&gt;$541$&lt;/td&gt;
&lt;td&gt;$996$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TOTAL&lt;/td&gt;
&lt;td&gt;$586$&lt;/td&gt;
&lt;td&gt;$645$&lt;/td&gt;
&lt;td&gt;$1231$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;A website designer wants to examine the effectiveness of two different layouts (A and B) for a small retail client in terms of the proportion of individuals who find and click on a special coupon within the first five minutes of browsing the retailer&amp;rsquo;s site. Presumably, the more people who find and click the coupon, the more effective the layout is. For two weeks, whenever a customer accessed the retailer&amp;rsquo;s site, one of the two layouts (A or B) was randomly chosen to load on the customer&amp;rsquo;s screen. Customers were tracked by their IP addresses, numbers that uniquely identify devices over the internet. Of the customers who browsed the site for at least five minutes, the proportion who clicked the coupon using each layout was recorded. The data are below. Construct a $95%$ confidence interval for the difference in the two proportions and interpret the interval in context.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Layout&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Found Coupon&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Did Not Find Coupon&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;A&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;$675$&lt;/td&gt;
&lt;td&gt;$658$&lt;/td&gt;
&lt;td&gt;$1333$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;B&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;$690$&lt;/td&gt;
&lt;td&gt;$477$&lt;/td&gt;
&lt;td&gt;$1167$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Total&lt;/strong&gt;    $1365$&lt;/td&gt;
&lt;td&gt;$1135$&lt;/td&gt;
&lt;td&gt;$2500$&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In a recent study, a drug manufacturer wanted to determine if a new antidepressant (Drug X) could also reduce pain that often comes with diabetes, called diabetic neuropathy. The data for the study are shown below. Calculate a $92%$ confidence interval for the difference in the proportion of people who experience relief from Drug X and the placebo.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Dosage&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Some/Significant Relief&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;No Relief&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Drug X&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;$159$&lt;/td&gt;
&lt;td&gt;$122$&lt;/td&gt;
&lt;td&gt;$281$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Placebo&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;$114$&lt;/td&gt;
&lt;td&gt;$197$&lt;/td&gt;
&lt;td&gt;$311$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Total&lt;/td&gt;
&lt;td&gt;$273$&lt;/td&gt;
&lt;td&gt;$319$&lt;/td&gt;
&lt;td&gt;$592$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;A researcher is interested to see if there is a difference between married and unmarried people when it comes to certain social behavior, in particular, the frequency of sexual activity. The data from the study are shown below. Calculate a $90%$ confidence interval for the difference in the proportion of married and unmarried persons who have sex at least once per week.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Group&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;$\geq$&lt;strong&gt;Once/Week&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;$\mathbf{&amp;lt;}$ &lt;strong&gt;Once/Week&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Married&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;$608$                 $28$&lt;/td&gt;
&lt;td&gt;$636$&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Unmarried&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;$553$&lt;/td&gt;
&lt;td&gt;$7$&lt;/td&gt;
&lt;td&gt;$560$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Total&lt;/td&gt;
&lt;td&gt;$1161$&lt;/td&gt;
&lt;td&gt;$35$&lt;/td&gt;
&lt;td&gt;$1196$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Astrology is a system of belief that, broadly speaking, claims that astronomical events somehow influence human behavior. One aspect of astrology is belief that the zodiac sign that one is born under (e.g., Pisces, Libra, Leo, Scorpio, etc.) somehow influences a person&amp;rsquo;s behavior. Is belief in astrology related significantly to education level? The $2012$ General Social Survey asked adults $18$ and over whether they believed at all in the zodiac. The results are presented below for individuals with an associate&amp;rsquo;s degree or higher (&amp;quot;College&amp;quot;) and those with a high school diploma or less (&amp;quot;HS or Below&amp;quot;). Calculate a $98%$ confidence interval for the difference in proportions between the two groups and interpret the interval in context.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Dosage&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Believe&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Do Not Believe&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;College&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;$162$&lt;/td&gt;
&lt;td&gt;$198$&lt;/td&gt;
&lt;td&gt;$360$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;HS or Below&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;$330$&lt;/td&gt;
&lt;td&gt;$306$&lt;/td&gt;
&lt;td&gt;$636$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Total&lt;/td&gt;
&lt;td&gt;$492$&lt;/td&gt;
&lt;td&gt;$504$&lt;/td&gt;
&lt;td&gt;$996$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
    </item>
    
    <item>
      <title>Test for Diff in Means Independent Samples</title>
      <link>/courses/bana3363/10-2-test-for-diff-in-means-independent-samples/</link>
      <pubDate>Tue, 28 Jul 2020 15:19:27 +0000</pubDate>
      <guid>/courses/bana3363/10-2-test-for-diff-in-means-independent-samples/</guid>
      <description>&lt;h2 id=&#34;hypothesis-tests-for-difference-in-means-independent-samples&#34;&gt;Hypothesis Tests for Difference in Means (Independent Samples)&lt;/h2&gt;
&lt;p&gt;Just as in the single sample case, we might be interested in testing whether the difference in two population means is equal to a specified value. Usually&amp;ndash;but not always&amp;ndash;we specify that the difference is $0$ in the null hypothesis in which case the test becomes one of equal population means. Doing so makes the confidence interval for the difference in means correspond to the test: if $0$ is not in the $(1-\alpha )100%$ interval, you conclude that the two means differ. If you were to conduct the hypothesis test at level $\alpha ,$ you would reject $H_{0}.$&lt;/p&gt;
&lt;p&gt;Here is the general procedure for testing the difference between two population/process means from independent samples. You can see that it follows the general pattern that we have seen before.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Specify the hypotheses.&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} \text{(a) }H_{0} &amp;amp;:&amp;amp;\mu _{1}-\mu _{2}\leq d\text{ vs. }H_{1}:\mu _{1}-\mu _{2}&amp;gt;d\text{ } \ \text{(b) }H_{0} &amp;amp;:&amp;amp;\mu _{1}-\mu _{2}\geq d\text{ vs. }H_{1}:\mu _{1}-\mu _{2}&amp;lt;d \ \text{(c) }H_{0} &amp;amp;:&amp;amp;\mu _{1}-\mu _{2}=d\text{ vs. }H_{1}:\mu _{1}-\mu _{2}\neq d\end{aligned}$$&lt;/p&gt;
&lt;p&gt;where $-\infty &amp;lt;d$ $&amp;lt;\infty$ is some specified difference (usually $0$, but it could be anything$).$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calculate the test statistic.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 1:&lt;/strong&gt; $\sigma _{1}=\sigma _{2}$&lt;/p&gt;
&lt;p&gt;$$t=\frac{\overline{x}&lt;em&gt;{1}-\overline{x}&lt;/em&gt;{2}-d}{s_{p}\sqrt{\frac{1}{n_{1}}+% \frac{1}{n_{2}}$$&lt;/p&gt;
&lt;p&gt;where $s_{p}=\sqrt{\frac{(n_{1}-1)s_{1}^{2}+(n_{2}-1)s_{2}^{2}}{n_{1}+n_{2}-2% }}$ is the pooled standard deviation estimate.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 2:&lt;/strong&gt; $\sigma _{1}\neq \sigma _{2}$&lt;/p&gt;
&lt;p&gt;$$t=\frac{\overline{x}&lt;em&gt;{1}-\overline{x}&lt;/em&gt;{2}-d}{\sqrt{\frac{s_{1}^{2}}{n_{1}}+% \frac{s_{2}^{2}}{n_{2}}}}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Determine the rejection region (or calculate the $p$-value).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The rejection region comes from a Student&amp;rsquo;s $t$ distribution with the appropriate degrees of freedom. In Case $2$ (unequal population variances) the degrees of freedom $\nu =\frac{\left( \frac{s_{1}^{2}}{n_{1}}+\frac{% s_{2}^{2}}{n_{2}}\right) ^{2}}{\frac{\left( \frac{s_{1}^{2}}{n_{1}}\right) ^{2}}{n_{1}-1}+\frac{\left( \frac{s_{2}^{2}}{n_{2}}\right) ^{2}}{n_{2}-1}},$ which would always be calculated by software in practice. But for large samples, $\nu$ will be large as well, so we can just using the $&amp;rdquo;\infty &amp;ldquo;$ row of the table in this case. For Case $1$ (equal population variances)$,$ the rejection region is given by the following:&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} \text{(a)}{\text{ }t_{n_{1}+n_{2}-2} &amp;amp;:&amp;amp;t_{n_{1}+n_{2}-2}&amp;gt;\text{ }t_{\alpha ,t_{n_{1}+n_{2}-2}}} \ \text{(b) }{t_{n_{1}+n_{2}-2} &amp;amp;:&amp;amp;t_{n_{1}+n_{2}-2}&amp;lt;-t_{\alpha ,t_{n_{1}+n_{2}-2}}} \ \text{(c) }{t_{n_{1}+n_{2}-2} &amp;amp;:&amp;amp;|t_{n_{1}+n_{2}-2}|&amp;gt;\text{ }t_{\alpha /2,t_{n_{1}+n_{2}-2}}}\end{aligned}$$&lt;/p&gt;
&lt;p&gt;If software is available, the $p$-value calculations are as follows:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 1:&lt;/strong&gt; $\sigma _{1}=\sigma _{2}$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$$\begin{aligned} \text{(a) }p &amp;amp;=&amp;amp;P(t_{n_{1}+n_{2}-2}&amp;gt;t) \ \text{(b) }p &amp;amp;=&amp;amp;P(t_{n_{1}+n_{2}-2}&amp;lt;t) \ \text{(c) }p &amp;amp;=&amp;amp;2\min [P(t_{n_{1}+n_{2}-2}&amp;gt;t),P(t_{n_{1}+n_{2}-2}&amp;lt;t)]\end{aligned}$$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Case 2:&lt;/strong&gt; $\sigma _{1}\neq \sigma _{2}$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$$\begin{aligned} \text{(a) }p &amp;amp;=&amp;amp;P(t_{\nu }&amp;gt;t) \ \text{(b) }p &amp;amp;=&amp;amp;P(t_{\nu }&amp;lt;t) \ \text{(c) }p &amp;amp;=&amp;amp;2\min [P(t_{\nu }&amp;gt;t),P(t_{\nu }&amp;lt;t)]\end{aligned}$$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;where $\nu =\frac{\left( \frac{s_{1}^{2}}{n_{1}}+\frac{s_{2}^{2}}{n_{2}}% \right) ^{2}}{\frac{\left( \frac{s_{1}^{2}}{n_{1}}\right) ^{2}}{n_{1}-1}+% \frac{\left( \frac{s_{2}^{2}}{n_{2}}\right) ^{2}}{n_{2}-1}}.$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make a conclusion&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the $p$-value $\leq \alpha ,$ we reject $H_{0}.$ If the $p$-value $% &amp;gt;\alpha ,$ we fail to reject $H_{0}.$ Equivalently, if $t_{0}$ lies within the rejection region, we reject $H_{0}$. If $t_{0}$ lies outside the rejection region, we fail to reject $H_{0}.$&lt;/p&gt;
&lt;p&gt;Here are some examples:&lt;/p&gt;
&lt;p&gt;In a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8,$ respectively. Determine at the $% \alpha =0.10$ level if there is a difference in the population mean amount of time spent studying per week between accounting and general business majors. Assume the population standard deviations are equal.&lt;/p&gt;
&lt;p&gt;\vspace{2in} In a recent survey of $121$ undergraduates, students reported the number of hours per week they spend on school work. Figure 
&lt;a href=&#34;#table1&#34;&gt;[table1]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;table1&amp;rdquo;} below summarizes the data for finance and marketing majors. A researcher wants to see if marketing majors and finance majors differ in the amount of time they study per week on average. Test the hypothesis at the $5%$ level of significance, assuming the two population standard deviations are not equal, using the fact that $\nu =26.$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Major&lt;/strong&gt;     &lt;strong&gt;n&lt;/strong&gt;   &lt;strong&gt;Average&lt;/strong&gt;   &lt;strong&gt;Standard Deviation&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Finance (Group 1) 15  9.8   4.9&lt;br&gt;
Marketing (Group 2)   16  11.9  7.4&lt;/p&gt;
&lt;p&gt;The most recent General Social Survey asked American adults over $18$ years of age how many total weeks they worked in the previous year. The data is summarized in the table shown below, which was created using Excel&amp;rsquo;s pivot table feature&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Test the hypothesis that those with a junior college education¬†(Group 1) work more weeks per year on average than those with a high school diploma (Group 2) at the $0.01$ level of significance. Assume that the population standard deviations are equal.&lt;/p&gt;
&lt;p&gt;\FRAME{ftbphFU}{5.5054in}{1.5039in}{0pt}{\Qcb{Average number of weeks spent in part-time and full-time employement in the previous year by highest degree earned.}}{\Qlb{weeks}}{Table}{\special{language &amp;ldquo;Scientific Word&amp;rdquo;;type &amp;ldquo;GRAPHIC&amp;rdquo;;maintain-aspect-ratio TRUE;display &amp;ldquo;USEDEF&amp;rdquo;;valid_file &amp;ldquo;T&amp;rdquo;;width 5.5054in;height 1.5039in;depth 0pt;original-width 5.4483in;original-height 1.4684in;cropleft &amp;ldquo;0&amp;rdquo;;croptop &amp;ldquo;1&amp;rdquo;;cropright &amp;ldquo;1&amp;rdquo;;cropbottom &amp;ldquo;0&amp;rdquo;;tempfilename &amp;lsquo;N7C7J106.wmf&amp;rsquo;;tempfile-properties &amp;ldquo;XPR&amp;rdquo;;}}&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;See &lt;a href=&#34;http://office.microsoft.com/en-us/excel-help/pivottable-reports-101-HA001034632.aspx&#34;&gt;http://office.microsoft.com/en-us/excel-help/pivottable-reports-101-HA001034632.aspx&lt;/a&gt; for more information about pivot tables. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Confidence Interval for the Difference in Variances</title>
      <link>/courses/bana3363/10-3-test-for-diff-in-variances/</link>
      <pubDate>Tue, 28 Jul 2020 15:19:27 +0000</pubDate>
      <guid>/courses/bana3363/10-3-test-for-diff-in-variances/</guid>
      <description>&lt;h2 id=&#34;motivation-for-examining-variances&#34;&gt;Motivation for Examining Variances&lt;/h2&gt;
&lt;p&gt;The notion of &lt;strong&gt;quality&lt;/strong&gt; means different things to different people. Some popular definitions include: fitness for an intended purpose; conformance to specifications; value as perceived by the customer; meeting design specifications; meeting customer expectations; and many more. Quality is of particular interest to managers because, after all, everyone wants to make a &amp;ldquo;quality product.&amp;rdquo; There is even an organization called the 
&lt;a href=&#34;http://asq.org/index.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;American Society for Quality&lt;/a&gt; devoted to the lofty goal of transforming the world by promoting the notion of quality assurance as a key component of any organization&amp;rsquo;s strategic plan. This group offers certification and training in several areas, including 
&lt;a href=&#34;http://www.iso.org/iso/iso_9000&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IS09000&lt;/a&gt; , and conducts research on emerging issues that affect quality. It is a good bet that during your career you will hear about concepts such as Six Sigma, total quality management, quality at the source, process capability, kaizen, poka-yoke, jidoka, lean manufacturing, control charts, and people such as W. Edwards Deming, Walter Shewhart, and Joseph Juran. In an era where traditional barriers to competitor entry¬†(such as access to customer base and capital) have been weakened, the notion of quality has become a key competitive tool for businesses around the world.&lt;/p&gt;
&lt;p&gt;You will notice that one of the definitions of quality is &amp;ldquo;conformance to specifications.&amp;rdquo; We can map this idea directly to the notion of variability. When things &amp;ldquo;conform,&amp;rdquo; by definition, they exhibit little variability. Thus, one might consider a service or product as being of poor quality if the performance of the product or service varies highly from one experience to the next. Conversely, a high-quality product or service might be defined as one that offers consistent performance from one experience to the next. Of course, consistency is not enough; after all, for example, a car that consistently overheats cannot be said to be high quality. Thus, it is important to set the standards of performance properly before the product or service is offered to the marketplace. Proper standard setting begins and ends with the customer in mind, and is often accomplished through observational studies, surveys, and focus groups. Product development is a key component of marketing science.&lt;/p&gt;
&lt;p&gt;Assuming that we do have some idea of a standard, how can we use statistics to help us see if our product is of high quality? If we are willing to make some assumptions about the populations, we can derive a confidence interval to determine whether the variability of two populations is the same. The two populations could be defined by the time of collection (say, yesterday&amp;rsquo;s production run versus today&amp;rsquo;s) or by process changes¬†(comparing an old production method versus a new one). The key assumptions will be that we have &lt;strong&gt;two independent, normally distributed populations&lt;/strong&gt; from which we are taking samples of size $n_{1}$ and $n_{2}.$ &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; We then construct a confidence interval, not for the difference but for the *ratio of the two variances.* Using the general relationship between confidence intervals and hypothesis tests, we can use the confidence interval to perform the test.&lt;/p&gt;
&lt;h2 id=&#34;the-confidence-interval-for-differences-in-two-variances--normal-populations&#34;&gt;The Confidence Interval for Differences in Two Variances&amp;ndash;Normal Populations&lt;/h2&gt;
&lt;p&gt;The distribution used to form the confidence interval is the $F$ &lt;strong&gt;distribution.&lt;/strong&gt; This distribution arises when we calculate the variances from samples from two normally distributed populations and make a quotient out of them in a certain way. The following theorem spells out the details.&lt;/p&gt;
&lt;p&gt;If $s_{1}^{2}$ and $s_{2}^{2}$ are two sample variances calculated by taking samples of size $n_{1}$ and $n_{2}$ from two independent normal populations with variances $\sigma _{1}^{2}$ and $\sigma _{2}^{2},$ then the quantity&lt;/p&gt;
&lt;p&gt;$$F_{\nu _{1},\nu _{2}}=\frac{s_{1}^{2}/\sigma _{1}^{2}}{s_{2}^{2}/\sigma _{2}^{2}}$$&lt;/p&gt;
&lt;p&gt;has an $F$ distribution with $\nu &lt;em&gt;{1}=n&lt;/em&gt;{1}-1$ numerator degrees of freedom and and $\nu &lt;em&gt;{2}=n&lt;/em&gt;{2}-1$ denominator degrees of freedom. The $F$ distribution takes on a variety of appearances depending on these two parameters, as you can see in Figure 
&lt;a href=&#34;#fdist&#34;&gt;[fdist]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;fdist&amp;rdquo;}.&lt;/p&gt;
&lt;p&gt;Notice that the $F$ distribution, unlike the Student&amp;rsquo;s $t$ and the standard normal, is not, in general, symmetric. In fact, it is &lt;strong&gt;right-skewed&lt;/strong&gt;, meaning it has a &amp;ldquo;tail&amp;rdquo;¬†to its right. Therefore, we must find two values, $F_{\alpha /2,\nu _{1},\nu _{2}}$ and $F_{1-\alpha /2,v_{1},\nu _{2}}$ to specify the confidence interval. If software were not available, we would use an $F$ table, an excerpt of which is shown in Figure 
&lt;a href=&#34;#ftable&#34;&gt;[ftable]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;ftable&amp;rdquo;}. Then, using the theorem above, we can say that&lt;/p&gt;
&lt;p&gt;$$P(F_{1-\alpha /2,v_{1},\nu _{2}}\leq \frac{s_{1}^{2}/\sigma _{1}^{2}}{% s_{2}^{2}/\sigma _{2}^{2}}\leq F_{\alpha /2,\nu _{1},\nu _{2}})=1-\alpha$$&lt;/p&gt;
&lt;p&gt;By manipulating this equation to get $\frac{\sigma _{1}^{2}}{% \sigma _{2}^{2}}$ in the middle (details omitted) we get the confidence interval.&lt;/p&gt;
&lt;p&gt;A $(1-\alpha )100%$ confidence interval for $\frac{\sigma &lt;em&gt;{1}^{2}}{\sigma &lt;em&gt;{2}^{2}}$ is given by $\left[ \frac{s&lt;/em&gt;{1}^{2}}{s&lt;/em&gt;{2}^{2}}\frac{1}{F_{\alpha /2,\nu _{1},\nu _{2}}},\frac{s_{1}^{2}}{s_{2}^{2}}F_{\alpha /2,\nu _{2},\nu _{1}}\right]$&lt;/p&gt;
&lt;p&gt;Notice that on the upper bound the degrees of freedom are switched, with $\nu _{2}$ becoming the numerator degrees of freedom and $\nu _{1}$ now the denominator degrees of freedom. This interval can be used to test the two-sided hypothesis&lt;/p&gt;
&lt;p&gt;$$\text{ }H_{0}:\frac{\sigma _{1}^{2}}{\sigma _{2}^{2}}=1\text{ vs. }H_{1}:% \frac{\sigma _{1}^{2}}{\sigma _{2}^{2}}\neq 1\text{ }$$&lt;/p&gt;
&lt;p&gt;To use the confidence interval, we simply look to see if it contains $1.$ The logic is that if the two population variances are equal, their ratio will be exactly $1$ (any number divided by itself is $1).$ Therefore, if $1$ is in the interval, it is plausible that the two variances are equal.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s work some examples.&lt;/p&gt;
&lt;p&gt;A golf club manufacturer is designing a new driver and wants to market it as giving more consistent results than the competitor&amp;rsquo;s club. To test this, the manufacturer used a specially designed machine to precisely hit $51$ golf balls with the new club $($Group $1)$ and $25$ golf balls with the competitor&amp;rsquo;s club $($Group $2)$. The new club resulted in an average drive length of $150.1$ yards with a standard deviation of $3.1$ yards, while the competitor&amp;rsquo;s club gave an average drive length of $150.5$ yards with a standard deviation of $5.8$ yards. Can the manufacturer conclude at the $\alpha =0.05$ level that the two population variances are not equal?&lt;/p&gt;
&lt;p&gt;The critical value of $F$ that we need is $F_{0.025,50,24}=2.11$, which gives $F_{0.025,24,50}=1.93$ for the upper bound (notice the $50$ and $24$ have switched places). So the interval is $\left[ \frac{3.1^{2}}{5.8^{2}}% \frac{1}{2.11},\frac{3.1^{2}}{5.8^{2}}1.93\right] =[0.14,0.55].$ This interval does not include $1,$ so we would conclude that the two population variances are different, with evidence that the variance of the old club is larger, making it less consistent.&lt;/p&gt;
&lt;p&gt;A fertilizer plant produces $60$-pound bags to sell at a nationwide home-improvement store. To minimize costs, it buys a filling machine that has a tolerance of $+/-0.03$pounds. One day, the store weighs a sample of $13$ bags using a precision scale and finds that the average weight of the bags is $60.001$ pounds, with a standard deviation of $0.045$ pounds. The next day, they weigh another sample of $13$ bags and find that the average weight is $60.002$ pounds, with a standard deviation of $0.058$ pounds. Can the manufacturer conclude that the process variability differs between the two days at the $0.10$ level?&lt;/p&gt;
&lt;p&gt;In a recent survey of $29$ accounting majors and $38$ general business majors, researchers asked the students to report the amount of time spent studying per week. The sample mean hours for accounting majors was $13.5$ and the sample mean hours for general business majors was $14$. The sample standard deviations are $10$ and $9.8,$ respectively. Determine at the $\alpha =0.10$ level if there is a difference in the variability in study time between accounting and general business majors.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Other tests are more sensitive to differences in the population variances when the assumption of normality in the populations is violated. The violation only has to be slight for the test to lose     sensitivity, or &amp;ldquo;power,&amp;rdquo; to detect differences in variances. For this reason, this test is considered &amp;ldquo;not robust&amp;rdquo; to violations of normality. The 
&lt;a href=&#34;http://en.wikipedia.org/wiki/Levene_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Levene and Brown-Forsythe tests&lt;/a&gt; work better in this case. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>ANOVA Basics</title>
      <link>/courses/bana3363/11-anova-basics/</link>
      <pubDate>Tue, 28 Jul 2020 15:19:27 +0000</pubDate>
      <guid>/courses/bana3363/11-anova-basics/</guid>
      <description>&lt;h2 id=&#34;anova-basics&#34;&gt;ANOVA Basics&lt;/h2&gt;
&lt;p&gt;In the last couple of chapters, we have discussed how to make inferences about the differences in two populations in terms of means and proportions. What if we wanted to compare more than two populations simultaneously? For example, what if we wanted to compare the abilities of four different real estate appraisal firms in terms of how accurate they are in predicting the sale price of a home? For convenience, we could label the firms with numbers (Firm $1$, Firm $2$, Firm $3$, Firm $4$). To compare the firms&amp;rsquo; accuracy, you might randomly assign each firm $10$ houses to appraise and then calculate an accuracy measure, for example,&lt;/p&gt;
&lt;p&gt;$$Y_{ij}=\frac{|\text{Appraised Price for home }j\text{ for Firm }i\text{ }-% \text{ Sale Price for home }j\text{ for Firm }i|}{\text{Sale Price for home }% j\text{ for Firm }i}\times 100$$&lt;/p&gt;
&lt;p&gt;The question would then be whether the mean accuracy differs among the four firms. Notice that double subscripts are now required to completely identify a single observation. We must specify the group number $(i=1,2,3,4)$ and the observation number within the group $(j=1,2,\ldots ,10)$. For example, $% y_{34}=2.3$ would be the fourth observation for Firm $3$. The observed data, in general, would appear as in Table 
&lt;a href=&#34;#anova_general&#34;&gt;[anova_general]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;anova_general&amp;rdquo;} below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Firm 1&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Firm 2&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Firm 3&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Firm 4&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$y_{11}$&lt;/td&gt;
&lt;td&gt;$y_{21}$&lt;/td&gt;
&lt;td&gt;$y_{31}$&lt;/td&gt;
&lt;td&gt;$y_{41}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$y_{12}$&lt;/td&gt;
&lt;td&gt;$y_{22}$&lt;/td&gt;
&lt;td&gt;$y_{32}$&lt;/td&gt;
&lt;td&gt;$y_{42}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$\vdots$&lt;/td&gt;
&lt;td&gt;$\vdots$&lt;/td&gt;
&lt;td&gt;$\vdots$&lt;/td&gt;
&lt;td&gt;$\vdots$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$y_{1,10}$&lt;/td&gt;
&lt;td&gt;$y_{2,10}$&lt;/td&gt;
&lt;td&gt;$y_{3,10}$&lt;/td&gt;
&lt;td&gt;$y_{4,10}$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;General appearance of an ANOVA data table for the real estate   appraisal example.&lt;/p&gt;
&lt;p&gt;[[anova_general]]{#anova_general label=&amp;quot;anova_general&amp;rdquo;}&lt;/p&gt;
&lt;p&gt;One way to do the comparisons would be to evaluate each pair of group means using the two-sample $t$ test or confidence interval that we have already studied. Thus, you would compare Firm 1 to Firm 2, Firm 1 to Firm 3, Firm 1 to Firm 4, Firm 2 to Firm 3, Firm 2 to Firm 4, and finally Firm 3 to Firm 4. The total number of comparisons would be $\binom{4}{2}=\frac{4!}{(4-2)!2!}=6$, using the combination formula from the binomial distribution (see Handout 1). In general, the number of pairs of means you would have to test if you were dealing with $g$ groups would be $\binom{g}{2}=\frac{g(g-1)}{2}$.&lt;/p&gt;
&lt;p&gt;The problem is that simply conducting multiple two-sample $t$ tests is highly error prone if you look at the collection of tests as a whole. If you think about it, it makes some sense: each test has probability $\alpha$ of giving a Type I¬†error, and if you have a collection of tests, each with a probability of error, the chance of making at least one error increases dramatically. Since making errors is generally not what we want to do when we attempt statistical inference, we need another approach. Enter ANOVA.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ANOVA(ANalysis Of Variance)&lt;/strong&gt; is a broad category of statistical techniques used to determine if there is a difference between two or more population/process means&lt;/p&gt;
&lt;p&gt;In its simplest form, it is an extension of the two-sample $t$ test to more than two samples. You could certainly use ANOVA for the case of two samples, but it would be an unnecessary complication. Before we move on, we need to get some vocabulary out of the way.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;factor&lt;/strong&gt; is a qualitative criterion by which we may segment the population. The populations can be segmented by several criteria.&lt;/p&gt;
&lt;p&gt;In the appraiser example, there is only one factor, firm. ¬†We could add another factor such as &amp;ldquo;firm experience.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Each factor has several &lt;strong&gt;levels&lt;/strong&gt; (or &lt;strong&gt;treatments&lt;/strong&gt;), which are possible values of the factor. We assume each combination of factor levels defines a population or process.&lt;/p&gt;
&lt;p&gt;There are four levels in the appraiser example $($Firm $1$,Firm $2$,Firm $3$, and Firm $4)$. If we add &amp;ldquo;firm experience,&amp;rdquo; the levels of that factor could be &amp;ldquo;little,&amp;rdquo; &amp;ldquo;moderate,&amp;rdquo; and &amp;ldquo;significant.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;response variable&lt;/strong&gt; is the numeric variable that we examine within each factor level combination.&lt;/p&gt;
&lt;p&gt;In the appraiser example, the response variable is the appraisal accuracy.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;experimental unit&lt;/strong&gt; is the entity that each value of the response variable represents.&lt;/p&gt;
&lt;p&gt;In the appraiser example, the experimental unit is an individual house.&lt;/p&gt;
&lt;p&gt;The design that examines only one factor with $g$ $(&amp;gt;2)$ levels is known as the &lt;strong&gt;one-way ANOVA.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We also need to get a bit of notation out of the way. Fortunately, it is a natural extension of the two-sample case to the case of $g$ populations. See Figure 
&lt;a href=&#34;#anova_notation&#34;&gt;[anova_notation]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;anova_notation&amp;rdquo;}.&lt;/p&gt;
&lt;p&gt;An additional important assumption that we will make is that the variance is the same for each of the $g$¬†groups, equal to $% \sigma ^{2}$. That is&lt;/p&gt;
&lt;p&gt;$$\sigma_{1}^{2}=\sigma_{2}^{2}=\ldots =\sigma_{g}^{2}=\sigma ^{2}$$&lt;/p&gt;
&lt;p&gt;In the one-way ANOVA, we divide the total variability in the data into two parts: that which is explained by the differences between group means and the overall mean, and that which is not. If a lot of the variability is explained by the difference between the group means and the overall mean, we have evidence that there is a difference (somewhere) in the population means. Figure 
&lt;a href=&#34;#anova_concept&#34;&gt;[anova_concept]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;anova_concept&amp;rdquo;} gives a visual representation of the idea. Here, we see that most of the total variability is accounted for by the differences between the group means and the overall mean (the shaded area). Therefore, we would say that there is a difference (somewhere) among the group means.&lt;/p&gt;
&lt;p&gt;The mathematical representation of Figure 
&lt;a href=&#34;#anova_concept&#34;&gt;[anova_concept]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;anova_concept&amp;rdquo;} is in terms of &amp;ldquo;&lt;strong&gt;sums of squares&lt;/strong&gt;.&amp;rdquo; The equation is&lt;/p&gt;
&lt;p&gt;$$SSTOT=SST+SSE$$&lt;/p&gt;
&lt;p&gt;where $SSTOT$ is the &lt;strong&gt;total sum of squares&lt;/strong&gt; (a measure of total variability in the data), $SST$ is called the &lt;strong&gt;sum of squares for treatments&lt;/strong&gt; (a measure of variability between group means and overall mean), and $SSE$ is called the &lt;strong&gt;sum of squares for error&lt;/strong&gt;, a measure of variability that is left unexplained.&lt;/p&gt;
&lt;p&gt;To construct the eventual test statistic, we need formulas for finding $SST$ and $SSE$. First, we need the &lt;strong&gt;grand mean,&lt;/strong&gt; the overall average of the data. The formula and notation for that value is the following:&lt;/p&gt;
&lt;p&gt;$$\overline{\overline{y}}=\frac{1}{N}\sum_{i=1}^{g}\sum_{j=1}^{n_{i}}y_{ij} \label{full_data_grand_mean}$$&lt;/p&gt;
&lt;p&gt;where $N$ is the total number of data points in the sample. If only the individual group means $\overline{y}&lt;em&gt;{1},\overline{y}&lt;/em&gt;{2},\ldots ,% \overline{y}_{g}$ are known, the grand mean can be found by taking a weighted (by the individual sample sizes) average of the group means, that is,&lt;/p&gt;
&lt;p&gt;$$\overline{\overline{y}}=\frac{n_{1}\overline{y}_{1}+n_{2}\overline{y}%_{2}+\ldots +n_{g}\overline{y}_{g}}{n_{1}+n_{2}+\ldots +n_{g}} \label{grand_mean_group_means_only}$$&lt;/p&gt;
&lt;p&gt;Now we can define the other formulas, starting with $SST$:&lt;/p&gt;
&lt;p&gt;$$SST=\sum_{i=1}^{g}n_{i}(\overline{y_{i}}-\overline{\overline{y}})^{2} \label{SST}$$&lt;/p&gt;
&lt;p&gt;If all of the group sample sizes are the same, then $n_{i}=n$ for all $i$, so the above can be simplified a bit:&lt;/p&gt;
&lt;p&gt;$$SST=n\sum_{i=1}^{g}(\overline{y_{i}}-\overline{\overline{y}})^{2}\text{ [if all group sample sizes are equal].}  \label{SST_equal_n}$$&lt;/p&gt;
&lt;p&gt;Next we have the formula for $SSE$:&lt;/p&gt;
&lt;p&gt;$$SSE=\sum_{i=1}^{g}\sum_{j=1}^{n_{i}}(y_{ij}-\overline{y}_{i})^{2} \label{SSE}$$&lt;/p&gt;
&lt;p&gt;which can be simplified if we note that for each $i$, the inner terms, $\sum_{j=1}^{n_{i}}(y_{ij}-\overline{y}_{i})^{2}$, are really just the numerators of the sample variances for each group, $s_{i}^{2}$. So we have $(n_{i}-1)s_{i}^{2}=\sum_{j=1}^{n_{i}}(y_{ij}-\overline{y}_{i})^{2}$, which means we can simplify the formula for $SSE$ to&lt;/p&gt;
&lt;p&gt;$$SSE=\sum_{i=1}^{g}(n_{i}-1)s_{i}^{2}\text{ \ \ \ [simplified version of equation \ref{SSE}]}$$&lt;/p&gt;
&lt;p&gt;Finally we, have $SSTOT$:&lt;/p&gt;
&lt;p&gt;$$SSTOT=\sum_{i=1}^{g}\sum_{j=1}^{n_{i}}(y_{ij}-\overline{\overline{y}})^{2} \label{SSTot}$$&lt;/p&gt;
&lt;p&gt;which is just the total of $SST$ and $SSE$. The more of $SSTOT$ that is made up of $SST$, the more we begin to suspect that there are differences in group means.&lt;/p&gt;
&lt;p&gt;The next concept we need is &lt;strong&gt;degrees of freedom,&lt;/strong&gt; which has to do with the number of independent pieces of information you are combining. Think of the ordinary sample variance formula: $s^{2}=\frac{% \sum_{i=1}^{n}(y_{i}-\overline{y})^{2}}{n-1}$. The reason we divide by $n-1$ is that if you know $n-1$ of the numbers and the sample mean, you can figure out the remaining number. For instance, if I tell you the sample mean is $% \overline{y}=6$ ¬†for a sample of $4$ numbers, and three of them are $1,8$, and $3$, then the fourth one must be $24-1-8-3=12$¬†(verify this for yourself). In ANOVA, degrees of freedom can be broken up by sums of squares:&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} \text{Total Degrees of Freedom}\text{: } &amp;amp;&amp;amp;DF_{TOT}=N-1 \ \text{Degrees of Freedom for Treatment}\text{: } &amp;amp;&amp;amp;DF_{SST}=g-1 \ \text{Degrees of Freedom for Error}\text{: } &amp;amp;&amp;amp;DF_{SSE}=N-g\end{aligned}$$&lt;/p&gt;
&lt;p&gt;so we have&lt;/p&gt;
&lt;p&gt;$$DF_{TOT}=DF_{SST}+DF_{SSE}$$&lt;/p&gt;
&lt;p&gt;We will see how this all fits together when we work a few examples. First we have to define the ANOVA test. The same general four-step procedure still applies. What changes are the specifics.&lt;/p&gt;
&lt;p&gt;The general four-step procedure still applies. What changes are the specifics.¬†Let&amp;rsquo;s see how.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Specify the hypothesis:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now instead of one $\mu$ we work with several $\mu ^{\prime }s$, one for each of the $g$ groups. The ANOVA hypothesis, like the test for equal variances, is two sided by design:&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} H_{0} &amp;amp;:&amp;amp;\mu_{1}=\mu_{2}=\ldots =\mu_{g} \ H_{1} &amp;amp;:&amp;amp;\text{ not all }\mu_{i}\text{ are equal }\end{aligned}$$&lt;/p&gt;
&lt;p&gt;Note that in the null hypothesis, you do not specify what all of the $\mu ^{\prime }s$ are equal to; you only state that they are all equal to one another. &lt;em&gt;Note also that the complement of&lt;/em&gt; $H_{0}$¬†is not that all the means are different, but just that at least two are different* (or equivalently, not all are equal).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Calculate the &lt;strong&gt;test statistic&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Under the assumption that the data arise from $g$ normal distributions with possibly different means but identical standard deviations, the test statistic will be an¬†&lt;strong&gt;$F$&lt;/strong&gt;¬†statistic,** which follows an $F$ distribution when the null hypothesis is true. Similar to the Student&amp;rsquo;s $t$ distribution, the $F$ distribution is defined by degrees of freedom. The difference is that we have **numerator degrees of freedom,** which we call **¬†**$\nu_{1}$,**¬†**and **denominator degrees of freedom**, which we call.**¬†**The $% F_{v_{1},v_{2}}$ distribution, then is the $F$ distribution with $\nu_{1}$ numerator degrees of freedom and $\nu_{2}$ denominator degrees of freedom. The $F$ distribution takes on a variety of appearances depending on these two parameters, as you can see in Figure 
&lt;a href=&#34;#fdist&#34;&gt;[fdist]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;fdist&amp;rdquo;}.&lt;/p&gt;
&lt;p&gt;To calculate the test statistic, we compute the following:&lt;/p&gt;
&lt;p&gt;$$f=\frac{SST/(g-1)}{SSE/(N-g)}=\frac{MST}{MSE}$$&lt;/p&gt;
&lt;p&gt;where $MST$ stands for &amp;ldquo;mean square for treatments&amp;rdquo; and $MSE$ stands for &amp;ldquo;mean square for error.&amp;rdquo;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Determine the rejection region (or compute the $p$-value)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Like any test for means, unless we have software, we usually go with the rejection region approach. The the rejection region for a test with the $% \alpha$ level of significance would be computed as follows:&lt;/p&gt;
&lt;p&gt;$${f_{g-1},_{N-g}:f_{g-1},_{N-g}&amp;gt;f_{g-1,N-g,\alpha }}$$&lt;/p&gt;
&lt;p&gt;If we have access to software, we can compute the $p$-value as follows: $$p-val=P(F_{g-1,N-g}&amp;gt;f)$$&lt;/p&gt;
&lt;p&gt;Notice that there is only one rejection region for this test; it is a right-tailed test. We only reject for large values of $f$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make a conclusion&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the $p$-value $\leq \alpha$, we reject $H_{0}$. If the $p$-value $% &amp;gt;\alpha$, we fail to reject $H_{0}$. Equivalently, if $f$ lies within the rejection region, we reject $H_{0}$. If $f$ lies outside the rejection region, we fail to reject $H_{0}$.&lt;/p&gt;
&lt;p&gt;In practice, the sums of squares, mean squares, $F$ statistic, $p$-value, and other information are often given in an &lt;strong&gt;ANOVA table&lt;/strong&gt; similar to that in Figure 
&lt;a href=&#34;#anovatable&#34;&gt;[anovatable]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;anovatable&amp;rdquo;}, which is the form given in Microsoft Excel. &amp;ldquo;Between groups&amp;rdquo; refers to the explained variability in group means and &amp;ldquo;within groups&amp;rdquo; refers to the unexplained variability. We will use this table extensively in the examples.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s work an example putting all of this together.&lt;/p&gt;
&lt;p&gt;The following statistics were calculated for an experiment:&lt;/p&gt;
&lt;p&gt;a). Develop the ANOVA table and fill in the information below.&lt;/p&gt;
&lt;p&gt;b). State the ANOVA hypothesis&lt;/p&gt;
&lt;p&gt;c). State the conclusion.&lt;/p&gt;
&lt;p&gt;A study was done to determine if the amount of time people spend on the internet varies by martial status. The data, from the $2012$ General Social Survey, are summarized below.&lt;/p&gt;
&lt;p&gt;a). Develop the ANOVA table and fill in the information below.&lt;/p&gt;
&lt;p&gt;b). State the ANOVA hypothesis&lt;/p&gt;
&lt;p&gt;c). State the conclusion in the context of the problem.&lt;/p&gt;
&lt;p&gt;The ANOVA table from the example in the PowerPoint regarding the average time students in different majors spend studying per week is reproduced below.&lt;/p&gt;
&lt;p&gt;a). How many groups were examined?&lt;/p&gt;
&lt;p&gt;b). Is this a balanced design?&lt;/p&gt;
&lt;p&gt;c). How many total observations were there?&lt;/p&gt;
&lt;p&gt;d). Show how the F statistic was calculated&lt;/p&gt;
&lt;p&gt;f). What important assumption seems to be violated here?&lt;/p&gt;
&lt;h1 id=&#34;bonferroni-method-of-multiple-comparisons&#34;&gt;Bonferroni Method of Multiple Comparisons&lt;/h1&gt;
&lt;p&gt;Recall that the ANOVA hypotheses are&lt;/p&gt;
&lt;p&gt;$$H_{0}: \mu_{1}=\mu_{2}=\ldots =\mu_{g}\text{ vs. }H_{1}:\text{not all }\mu ^{\prime }s\text{ are equal}$$&lt;/p&gt;
&lt;p&gt;Suppose you have rejected the ANOVA hypothesis. You can then only conclude that not all of the group means are identical. However, you don&amp;rsquo;t know which ones differ from each other, which is a natural question to ask. As stated at the beginning of Handout 11, simply conducting several $t$ tests in the usual way will increase your chance of making at least one type I¬†error, so we have to adjust our approach slightly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Multiple comparison techniques&lt;/strong&gt; allow one to make conclusions about a collection, or &lt;strong&gt;family&lt;/strong&gt;, of hypotheses while keeping the overall error rate (&lt;strong&gt;familywise error rate&lt;/strong&gt;) at a given (small) level, $\alpha_{family}.$&lt;/p&gt;
&lt;p&gt;Many methods exist, and I 
&lt;a href=&#34;http://repositories.tdl.org/ttu-ir/bitstream/handle/2346/ETD-TTU-2011-08-1608/Henning_Kevin_Dissertation.pdf?sequence=2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;wrote my disseration on one&lt;/a&gt; . We will only cover one here because it is the simplest. The basic idea of all methods is that we increase the burden of proof over a regular test or interval. We do this because each test has an $\alpha$ probability of being incorrect.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Bonferroni adjustment&lt;/strong&gt; forms confidence intervals and conducts tests at the $\frac{\alpha_{family}}{k}$ level, where $k$ is the number of tests or intervals you want to construct and control the error rate for.&lt;/p&gt;
&lt;p&gt;To compare all of the groups to one another, just set $k=\binom{g}{2}=\frac{% g(g-1)}{2}.$ For example, if you have $5$ groups and you wish to compare all group means, you would set $k=\frac{5(5-1)}{2}=10.$ If $\alpha_{family}=0.05$, then you would form intervals and conduct tests at the $% \frac{0.05}{10}=0.005$ level. However, you just wanted to compare, say, $4$ pairs of means, you would set $k=4$ and you would form intervals and conduct tests at the $\frac{0.05}{4}=0.0125$ level.&lt;/p&gt;
&lt;p&gt;Confidence intervals for differences between means $\mu_{i}$ and $\mu_{j}$ can be constructed by the following method&lt;/p&gt;
&lt;p&gt;$$\overline{y_{i}}-\overline{y_{j}}\pm t_{\frac{_{\alpha_{family}}}{2k}% },_{N-g}\times \sqrt{MSE\left( \frac{1}{n_{i}}+\frac{1}{n_{j}}\right) }$$&lt;/p&gt;
&lt;p&gt;What this interval allows us to conclude is that, with $(1-\alpha_{family})100%$ confidence **in the entire set of confidence intervals we calculate**, the difference in the two group means $\mu_{1}-\mu_{j}$ lies within the upper and lower bounds. So, if we want $95%$ confidence in the entire set of intervals, we would set $\alpha_{family}=0.05.$ The $&amp;quot;2&amp;quot;$ in the denominator of the above equation comes from the fact that when we form a two-sided confidence interval, we divide the $\alpha$ by 2 to obtain the correct value of $t.$&lt;/p&gt;
&lt;p&gt;We can also define a hypothesis test using the Bonferroni method. What changes are the specifics.¬†Let&amp;rsquo;s see how.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Specify the hypothesis:&lt;/p&gt;
&lt;p&gt;In general, we want to test the relationship between two means,     $\mu_{i}$ and $\mu_{j}.$&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}     \text{(a) }H_{0} &amp;amp;:&amp;amp;\mu_{i}-\mu_{j}\leq d\text{ vs. }H_{1}:\mu_{i}-\mu    _{j}&amp;gt;d\text{ } \     \text{(b) }H_{0} &amp;amp;:&amp;amp;\mu_{i}-\mu_{j}\geq d\text{ vs. }H_{1}:\mu_{i}-\mu    _{j}&amp;lt;d \     \text{(c) }H_{0} &amp;amp;:&amp;amp;\mu_{i}-\mu_{j}=d\text{ vs. }H_{1}:\mu_{i}-\mu    _{j}\neq d\end{aligned}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calculate the &lt;strong&gt;test statistic&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The test statistic will be a $t$ statistic.&lt;/p&gt;
&lt;p&gt;$$t_{0}=\frac{\overline{y}_{i}-\overline{y}_{j}-d}{\sqrt{MSE(\frac{1}{n_{i}}+%     \frac{1}{n_{j}})}}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the &lt;strong&gt;p-value&lt;/strong&gt; or &lt;strong&gt;rejection region&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}     \text{(a) }p &amp;amp;=&amp;amp;P(t_{N-g}&amp;gt;t_{0}) \     \text{(b) }p &amp;amp;=&amp;amp;P(t_{N-g}&amp;lt;t_{0}) \     \text{(c) }p &amp;amp;=&amp;amp;2\min [P(t_{N-g}&amp;gt;t_{0}),P(t_{N-g}&amp;lt;t_{0})]\end{aligned}$$&lt;/p&gt;
&lt;p&gt;Alternatively, we can define the rejection region as follows:&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}     \text{(a)}{\text{ }t_{N-g} &amp;amp;:&amp;amp;t_{N-g}&amp;gt;\text{ }t_{\alpha_{family}/k,N-g}}     \     \text{(b) }{t_{N-g} &amp;amp;:&amp;amp;t_{N-g}&amp;lt;-t_{\alpha_{family}/k,N-g}} \     \text{(c) }{t_{N-g} &amp;amp;:&amp;amp;|t_{N-g}|&amp;gt;\text{ }t_{\alpha_{family}/2k,N-g}}\end{aligned}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make a conclusion&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the $p$-value $\leq \frac{\alpha_{family}}{k}$, we reject $H_{0}.$ If the $p$-value $&amp;gt;\frac{\alpha_{family}}{k}$, we fail to reject $H_{0}.$ Equivalently, if $t_{0}$ lies within the rejection region, we reject $H_{0}$ . If $t_{0}$ lies outside the rejection region, we fail to reject $H_{0}.$&lt;/p&gt;
&lt;p&gt;In general, getting the values of $t$ requires a computer since rarely will $% \frac{\alpha_{family}}{2k}$ be a value in a table. To get the values of $t$ using Excel, you can follow Figure 
&lt;a href=&#34;#bonf&#34;&gt;[bonf]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;bonf&amp;rdquo;} below. You would substitute in a number for &amp;ldquo;alpha/k&amp;rdquo; and &amp;ldquo;N-g.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Excel already knows to divide by&lt;/strong&gt; $2$**¬†since it assumes you want a value of** $t$**¬†for a two-sided confidence interval. Therefore, you just simply specify** $\frac{\alpha_{family}}{k}.$&lt;/p&gt;
&lt;p&gt;The following statistics were calculated for an experiment:&lt;/p&gt;
&lt;p&gt;Form all $95%$ confidence intervals between the four groups so that we can be $95%$ confident in the entire set of intervals. Use the fact that $% t_{0.05/(2\ast 6),49}=2.75.$&lt;/p&gt;
&lt;p&gt;There are a total of six groups to compare ($\binom{4}{2}=\frac{4(3)}{2}=6).$ From the ANOVA table, If we want to be $95%$ confident in the entire set of intervals, we use $\alpha_{family}=\frac{0.05}{6}=0.0083.$ The value of $t$ we need is $t_{0.0083/2,49}=2.75$ since there are $53$ total observations and $4$ groups, giving $N-g=53-4=49.$ MSE can be calculated as $102.47$ (check this for yourself).&lt;/p&gt;
&lt;p&gt;Now we can compare the groups. The calculation for the first comparison, between Group 1 and Group 2, is shown here. The other calculations are similar; all that changes are the two means you are comparing and the two sample sizes under the square root.&lt;/p&gt;
&lt;p&gt;$(30-35)\pm 2.75\sqrt{102.47\left( \frac{1}{10}+\frac{1}{14}\right) }% =[-16.5,6.5].$ Since this interval contains $0$, at the $\alpha_{family}=0.05$ level, we would fail to reject $H_{0}:\mu_{1}-\mu_{2}$; that is, we could not conclude that there was a difference in Group 1 and Group 2.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Group&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Interval&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$1$ vs. $2$&lt;/td&gt;
&lt;td&gt;$[-16.5,6.5]$ (no difference)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$1$ vs. $3$&lt;/td&gt;
&lt;td&gt;$[-15.2,9.2]$(no difference)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$1$ vs. $4$&lt;/td&gt;
&lt;td&gt;$[-21,1]$(no difference)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$2$ vs. $3$&lt;/td&gt;
&lt;td&gt;$[-9.2,13.2]$(no difference)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$2$ vs. $4$&lt;/td&gt;
&lt;td&gt;$[-15,5]$(no difference)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$3$ vs. $4$&lt;/td&gt;
&lt;td&gt;$[-17.7,3.7]$(no difference)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Thus, we cannot conclude at the familywise level of $0.05$ (or state with the familywise confidence level of $0.95)$ that there is a difference in any of the groups.&lt;/p&gt;
&lt;p&gt;A study was done to determine if the amount of time people spend on the internet varies by martial status. The data, from the $2012$ General Social Survey, are summarized below.&lt;/p&gt;
&lt;p&gt;Form all $99%$ confidence intervals for comparing Divorced to the other groups such that we can be $99%$ confident in the entire set of intervals. Use the fact that $t_{0.01/(2\ast 3),1807}=2.939.$&lt;/p&gt;
&lt;p&gt;There are a total of three groups to compare (Divorced vs. Married, Divorced vs. Never Married, and Divorced vs. Separated). If we want to be $99%$ confident in the entire set of intervals, we use $\alpha_{family}=\frac{0.01% }{3}=0.0033.$ The value of $t$ we need is $t_{0.0033/2,1807}=2.939$, since there are $1811$ total observations and $4$ groups, giving $N-g=1811-4=1807.$ MSE can be calculated as $210.67$ (check this for yourself). The first interval, Divorced Vs. Married, is calculated as follows:&lt;/p&gt;
&lt;p&gt;$$(8.4-9.4)\pm 2.939\sqrt{210.67\left( \frac{1}{317}+\frac{1}{900}\right) }% =[-3.8,1.8]$$&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Group&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Interval&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Divorced vs. Married&lt;/td&gt;
&lt;td&gt;$[-3.8,1.8]$(no difference)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Divorced vs. Never Married&lt;/td&gt;
&lt;td&gt;$[-8.1,-2.1]$(difference)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Divorced vs. Separated&lt;/td&gt;
&lt;td&gt;$[-8.2,3.2]$(no difference)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Thus, we can conclude at the familywise level of $0.01$ (or state with the familywise confidence level of $0.99)$ that there is a difference in the average amount of time spent online per week between divorced persons and persons who have never married. Specifically, we conclude that persons who have never married spend, on average, between $2$ and $8$ more hours per week online than those who have been divorced.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ANOVA - Bonferroni Method of Multiple Comparisons</title>
      <link>/courses/bana3363/12-anova-mc/</link>
      <pubDate>Tue, 28 Jul 2020 15:19:27 +0000</pubDate>
      <guid>/courses/bana3363/12-anova-mc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Chi Squared Tests</title>
      <link>/courses/bana3363/13-chi-squared-goodness-of-fit-test/</link>
      <pubDate>Tue, 28 Jul 2020 15:19:27 +0000</pubDate>
      <guid>/courses/bana3363/13-chi-squared-goodness-of-fit-test/</guid>
      <description>&lt;h2 id=&#34;chi-squared-goodness-of-fit-test&#34;&gt;Chi Squared Goodness-of-Fit Test&lt;/h2&gt;
&lt;p&gt;Beginning with our discussion of ANOVA, we have moved away from making inferences about single parameters such as $\mu$ and $p$ and have instead been examining a¬†general or &lt;strong&gt;overall model.&lt;/strong&gt; The ANOVA $F$ test is testing whether the means of $g&amp;gt;1$ groups $\mu_{1},\ldots, \mu_{g}$ are identical against the alternative hypothesis that at least one of the means is different from the rest. The ANOVA $F$ test is used to examine numerical data divided into groups.&lt;/p&gt;
&lt;p&gt;To examine nominal or categorical data (i.e., data that we cannot directly summarize using mathematical measures such as the sample mean and standard deviation), we could examine a graph. The primary tools used to graph nominal or categorical variables are &lt;strong&gt;bar charts&lt;/strong&gt; and &lt;strong&gt;pie charts.&lt;/strong&gt; For example, the 2012 General Social Survey (GSS) asked respondents to what degree they agreed with the statement &amp;ldquo;Both men and women contribute to household income.&amp;rdquo; The results are summarized below in Figure 
&lt;a href=&#34;#husweef&#34;&gt;[husweef]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;husweef&amp;rdquo;}. We can easily see that most people agree or strongly agree with the statement.&lt;/p&gt;
&lt;p&gt;What if we wanted to determine whether attitudes on this issue have changed over time, say, since 1994? One way we could analyze this is to make side-by-side bar charts and examine the patterns for the two years. If the patterns differed, we would have evidence of a shift. However, the problem with the graphical approach is that it is based solely on individual judgement. What one person might think is a pattern shift worth investigating another person might dismiss as unimportant. Therefore, we need another tool to remove some (not all) of the subjectivity. A hypothesis test is one such tool.&lt;/p&gt;
&lt;p&gt;Before we continue, we need to discuss the concept of a multinomial experiment.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;multinomial experiment&lt;/strong&gt; is one that is conducted such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the experiment consists of a fixed number of &amp;ldquo;trials,&amp;rdquo; $n$&lt;/li&gt;
&lt;li&gt;the outcome of each trial can be classified into one and only one of $k&amp;gt;2$ categories&lt;/li&gt;
&lt;li&gt;the probability $p_{i}$ of an outcome falling into category $i$ is constant for each trial&lt;/li&gt;
&lt;li&gt;the sum of all the probabilities is one, that is, $\dsum p_{i}=1;$&lt;/li&gt;
&lt;li&gt;each trial is independent of the other trials&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A binomial experiment is one you are familiar with. There are $n$ trials, the outcome of each trial can be classified as a &amp;ldquo;success&amp;rdquo; or &amp;ldquo;failure,&amp;rdquo; and the probability of success remains the same on each trial. With a binomial experiment, the probability of success is $p$, which means that the probability of failure is $1-p$ because, by design, there are only two outcomes, and one of them has to happen. I could simply rename the probability of success as $p_{1}$ and the probability of failure $p_{2}=1-p_{1}$. Then $p_{1}+p_{2}=p_{1}+(1-p_{1})=1$. In a binomial experiment, we further assume that each of the $n$ trials is independent (see Handout 1). Therefore, we can view the binomial experiment as a special case of the multinomial experiment when $k=2$. If we let the random variable $X$ be the number of successes out of $n$ trials, we say that &amp;ldquo;$X$ has a binomial distribution with parameters $n$ and $p$.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;A multinomial experiment simply adds more categories. Instead of only two (&amp;ldquo;success&amp;rdquo; and &amp;ldquo;failure&amp;rdquo;), we have three, four, five, six, or, in general, $k$ different categories. To see the idea, suppose the experiment is to ask a random sample of $n$ Americans to report their job classification as one of the following: ${$working full time, working part time, keeping house, in school, other$}$. Then, there are five probabilities $p_{1},p_{2},p_{3},p_{4}$, and $p_{5}$, one for every possible category. The probability that someone answers &amp;ldquo;working full time&amp;rdquo; is $p_{1}$, the probability that someone answers &amp;ldquo;working part time&amp;rdquo; is $p_{2}$, and so on. By including the &amp;ldquo;other&amp;rdquo; category, we ensure that each person we ask can be classified into exactly one of the five categories. Therefore, $p_{1}+p_{2}+p_{3}+p_{4}+p_{5}=1$. Just like with the binomial model, however, we can say that $p_{5}=1-(p_{1}+p_{2}+p_{3}+p_{4})$, so if we know four of the probabilities, we can get the fifth by subtraction. This fact will play a part in understanding the test to be presented.&lt;/p&gt;
&lt;p&gt;The generalization of the binomial distribution to $k&amp;gt;2$ categories is called the &lt;strong&gt;multinomial distribution&lt;/strong&gt;, which is defined now.&lt;/p&gt;
&lt;p&gt;If $X_{1}$ is the number of outcomes classified in Category $1$, $X_{2}$ is the number of outcomes classified in Category $2,\ldots , and X_{k}$ is the number of outcomes classified in Category $k$, then&lt;/p&gt;
&lt;p&gt;$$P(X_{1}=x_{1},X_{2}=x_{2},\ldots ,X_{k}=x_{k})=\frac{n!}{x_{1}!x_{2}!\ldots x_{k}!}p_{1}^{x_{1}}p_{2}^{x_{2}}\cdots p_{k}^{x_{k}}$$&lt;/p&gt;
&lt;p&gt;Here is a simple example.&lt;/p&gt;
&lt;p&gt;In a certain town, $40%$ of the eligible voters prefer candidate A, $10%$ prefer candidate B, and the remaining $50%$ have no preference. You randomly sample $10$ eligible voters. What is the probability that $4$ will prefer candidate A, $1$ will prefer candidate B, and the remaining $5$ will have no preference?&lt;/p&gt;
&lt;p&gt;According to the 
&lt;a href=&#34;http://www.chron.com/opinion/outlook/article/Why-juries-don-t-reflect-the-demographics-of-3761368.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Houston Chronicle,&lt;/a&gt; the race/ethnicity distribution in 2010 for Harris county is as shown in Figure 
&lt;a href=&#34;#race&#34;&gt;[race]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;race&amp;rdquo;}.&lt;/p&gt;
&lt;p&gt;Suppose that a jury of twelve members is chosen from the adult population in Harris County in such a way that each resident has an equal probability of being selected independently of every other resident. What is the probability of obtaining a jury with $5$ people who are non-hispanic white, $2$ people who are Hispanic, $3$ people who are African American, $1$ person who is Asian, and $1$ person who belongs to the &amp;ldquo;other&amp;rdquo; category?&lt;/p&gt;
&lt;p&gt;The connection between the multinomial distribution and Figure 
&lt;a href=&#34;#husweef&#34;&gt;[husweef]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;husweef&amp;rdquo;} is that we can imagine that, in 2012, there were &amp;ldquo;true&amp;rdquo; proportions $p_{1}$, $p_{2},\ldots ,p_{5}$ corresponding the probability that an adult $18$ and over would respond with one of the categories shown. Therefore, we can view Figure 
&lt;a href=&#34;#husweef&#34;&gt;[husweef]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;husweef&amp;rdquo;} as the outcome of a (large) multinomial trial where the bar heights are the number of people who fell into each category.&lt;/p&gt;
&lt;p&gt;In practice, we don&amp;rsquo;t know these true values, but we can estimate them using data. The idea is exactly the same as estimating $p$ using the sample proportion, except now we have $k$ different sample proportions. Specifically, we can say that $\widehat{p}&lt;em&gt;{1}$ is the number of people who &amp;ldquo;strongly agree,&amp;rdquo; call it $n&lt;/em&gt;{1}$, out of the total number of people surveyed, $n$. That is $\widehat{p}&lt;em&gt;{1}=\frac{n&lt;/em&gt;{1}}{n}$. Similarly, $\widehat{p}&lt;em&gt;{2}$ is the number of people who &amp;ldquo;Agree,&amp;rdquo; call it $n&lt;/em&gt;{2}$, out of the total number of people surveyed. That is, $\widehat{p}&lt;em&gt;{2}=\frac{n&lt;/em&gt;{2}}{n}$. In this data set, there were $1,267$ observations, and $249$ people &amp;ldquo;strongly agreed.&amp;rdquo; Therefore, $\widehat{p}_{1}=\frac{249}{1267}=0.197$. The sample proportions are shown in the following table.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Category&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Sample Proportion&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Strongly Agree&lt;/td&gt;
&lt;td&gt;$0.197$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Agree&lt;/td&gt;
&lt;td&gt;$0.452$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Neutral&lt;/td&gt;
&lt;td&gt;$0.242$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Disagree&lt;/td&gt;
&lt;td&gt;$0.096$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Strongly Disagree&lt;/td&gt;
&lt;td&gt;$0.013$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;These sample proportions form an &lt;em&gt;estimated&lt;/em&gt; multinomial distribution. It is estimated because it was formed using data, not the true parameters. The true distribution could be summarized in a table. The true probabilities are, of course, unknown.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Category&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Pr(Category)&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Strongly Agree&lt;/td&gt;
&lt;td&gt;$p_{1}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Agree&lt;/td&gt;
&lt;td&gt;$p_{2}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Neutral&lt;/td&gt;
&lt;td&gt;$p_{3}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Disagree&lt;/td&gt;
&lt;td&gt;$p_{4}$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Strongly Disagree&lt;/td&gt;
&lt;td&gt;$p_{5}$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can use the sample proportions that we have to determine the &amp;ldquo;goodness of fit&amp;rdquo; between a theorized distribution and an observed distribution. If the observed distribution is &amp;ldquo;too different&amp;rdquo; from the theorized distribution, the &amp;ldquo;fit&amp;rdquo; will be poor and we would have objective evidence to claim that the two distributions are different. The mechanism by which this works is the subject of the &lt;strong&gt;chi-squared goodness-of-fit test.&lt;/strong&gt; It is a hypothesis test just like any other, but, as always, the specifics differ. Here is a rundown of the test.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Specify the hypothesis:&lt;/p&gt;
&lt;p&gt;Now, instead of one or two proportions we have several. The null hypothesis specifies that the different $p^{\prime }s$ are equal to specified values, which we will denote as $p_{0i}$. Note that they *do not* have to be equal to one another.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$H_{0}: p_{1}=p_{01},p_{2}=p_{02},\ldots, p_{k}=p_{0k}$&lt;/li&gt;
&lt;li&gt;$H_{1}: \text{ at least one }p\text{ is not equal to its specified value}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calculate the &lt;strong&gt;test statistic&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The test statistic is a bit different:&lt;/p&gt;
&lt;p&gt;$$\chi ^{2}=\frac{\dsum (f_{i}-e_{i})^{2}}{e_{i}}$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;where $f_{i}$ is the &amp;ldquo;observed frequency&amp;rdquo; in category $i$ and $e_{i}$ is the &amp;ldquo;expected frequency&amp;rdquo; in category $i$. The observed frequencies are given by the data. To find the $e_{i}$, note that if we have a total of $n$ observations, and there is probability $p_{i0}$ that an observation falls into category $i$ under the null hypothesis, it is natural to say that the expected number of observations falling into category $i$ is&lt;/p&gt;
&lt;p&gt;$$e_{i}=np_{i0}$$&lt;/p&gt;
&lt;p&gt;Note the logic behind the test statistic. What it essentially is doing is comparing the actual frequencies we have to what we should see if the null hypothesis were true. If $\chi ^{2}$ is &amp;ldquo;too large,&amp;rdquo; this implies that the estimated distribution is far from the theorized distribution, and therefore we doubt that the theorized distribution is the one that gave rise to the data that we have.&lt;/p&gt;
&lt;p&gt;The test statistic has what is called a &lt;strong&gt;&amp;ldquo;chi-squared distribution.&amp;quot;&lt;/strong&gt; This is a new distribution to us.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;chi-squared random variable&lt;/strong&gt; with $k$ degrees of freedom, denoted $\chi_{k}^{2}$, arises as the sum of $k$ independent squared standard normal random variables, $Z_{i}^{2}$.&lt;/p&gt;
&lt;p&gt;The distribution takes on various shapes according to $k$ as you can see in Figure 
&lt;a href=&#34;#chisq&#34;&gt;[chisq]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;chisq&amp;rdquo;}. Importantly, it always ranges from $0$ to $\infty$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Compute the &lt;strong&gt;p-value&lt;/strong&gt; or &lt;strong&gt;rejection region&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$pval=P(\chi_{k-1}^{2}&amp;gt;\chi ^{2})$$&lt;/p&gt;
&lt;p&gt;Like the test for means, unless we have software, we usually go with the rejection region approach. The the rejection region (i.e., the &amp;ldquo;critical values&amp;rdquo;) for a test with the $\alpha$ level of significance would be computed as follows:&lt;/p&gt;
&lt;p&gt;$${\chi_{k-1}^{2}:\chi_{k-1}^{2}&amp;gt;\chi_{k-1,\alpha }^{2}}$$&lt;/p&gt;
&lt;p&gt;The reason we use $k-1$ rather than $k$ is that there is a restriction on the $p_{i}:$ they must sum to $1$. So, if we know $k-1$ of the probabilities, we can get the $k^{th}$ one.&lt;/p&gt;
&lt;p&gt;As with the ANOVA $F$ test, there is only one &amp;ldquo;guard&amp;rdquo; for the rejection region, since the test is one-sided. This reflects the nature of the test statistic above. If $\chi ^{2}$ is small, we are inclined to believe that the observed distribution could have been produced by the theorized distribution. If $\chi ^{2}$ is large, we would conclude the opposite. So we only look for &amp;ldquo;large&amp;rdquo; values.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make a conclusion&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the $p$-value $\leq \alpha$, we reject $H_{0}$. If the $p$-value $&amp;gt;\alpha$, we fail to reject $H_{0}$. Equivalently, if $\chi ^{2}$ lies within the rejection region, we reject $H_{0}$. If $\chi^{2}$ lies outside the rejection region, we fail to reject $H_{0}$.&lt;/p&gt;
&lt;p&gt;This test is known as an &lt;strong&gt;asymptotic test&lt;/strong&gt; because it is only valid if the sample size is &amp;ldquo;large enough.&amp;rdquo; and the $e_{i}$ are all &amp;ldquo;large enough.&amp;rdquo; There is some debate about how &amp;ldquo;large&amp;rdquo; these have to be in practice for the test to work properly, but one simple rule that has been shown to work is to require that $n\geq$ $10,\frac{n^{2}}{c}\geq 10$, and all $e_{i}\geq 0.25$. Usually the last requirement will be the problem. An easy fix is to simply combine categories.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s work an example putting all of this together.&lt;/p&gt;
&lt;p&gt;In $1994$, the distribution of responses to the statement &amp;ldquo;Both husband and wife should contribute to household income&amp;rdquo; was as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Category&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Pr(Category)&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Strongly Agree&lt;/td&gt;
&lt;td&gt;$0.197$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Agree&lt;/td&gt;
&lt;td&gt;$0.379$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Neutral&lt;/td&gt;
&lt;td&gt;$0.296$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Disagree&lt;/td&gt;
&lt;td&gt;$0.115$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Strongly Disagree&lt;/td&gt;
&lt;td&gt;$0.013$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;$1,267$¬†people were surveyed in $2012$¬†and the numbers of responses in each category are shown below. Do we have evidence at the $\alpha =0.05$¬†level of significance that opinion on this issue has changed since $1994$?&lt;/p&gt;
&lt;p&gt;According to the 
&lt;a href=&#34;http://www.chron.com/opinion/outlook/article/Why-juries-don-t-reflect-the-demographics-of-3761368.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Houston Chronicle,&lt;/a&gt; the race/ethnicity distribution in 2010 for Harris county is as shown in Figure 
&lt;a href=&#34;#race&#34;&gt;[race]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;race&amp;rdquo;}. From January through May of 2013, a total of $63,207$ people were called for jury service. The distribution by race is as shown in Figure 
&lt;a href=&#34;#jury_call&#34;&gt;[jury_call]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;jury_call&amp;rdquo;}.&lt;/p&gt;
&lt;p&gt;Do we have evidence at the $\alpha =0.10$ level that juries in Harris county do not reflect the true demographics of the county?&lt;/p&gt;
&lt;p&gt;According to the 
&lt;a href=&#34;http://www.statisticbrain.com/eye-color-distribution-percentages/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;American Academy of Ophthalmology&lt;/a&gt;, the distribution of eye color in the United States is as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Color&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Percent of U. S. Pop&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Blue /Grey Irises&lt;/td&gt;
&lt;td&gt;$32$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Blue / Grey / Green Irises with Brown / Yellow Specks&lt;/td&gt;
&lt;td&gt;$15$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Green / Light Brown Irises with Minimal Specks&lt;/td&gt;
&lt;td&gt;$12$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Brown Irises with Specks&lt;/td&gt;
&lt;td&gt;$16$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dark Brown Irises&lt;/td&gt;
&lt;td&gt;$25$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Record the number of people in our class who fall into each category in the table below. Do we have evidence at the $\alpha =0.05$¬†level to conclude that our class has the same distribution of eye color as the U. S. population? Note: You should verify that the assumptions are met first.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Color&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Percent of U. S. Pop&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Blue /Grey Irises&lt;/td&gt;
&lt;td&gt;$32$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Blue / Grey / Green Irises with Brown / Yellow Specks&lt;/td&gt;
&lt;td&gt;$15$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Green / Light Brown Irises with Minimal Specks&lt;/td&gt;
&lt;td&gt;$12$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Brown Irises with Specks&lt;/td&gt;
&lt;td&gt;$16$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dark Brown Irises&lt;/td&gt;
&lt;td&gt;$25$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;another-example-of-goodness-of-fit&#34;&gt;Another Example of Goodness-of-Fit&lt;/h2&gt;
&lt;p&gt;We begin with another example of the chi-squared goodness-of-fit test. The sources for the data are the 
&lt;a href=&#34;http://www.nhtsa.gov/NCSA&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;National Highway Traffic Safety Administration&lt;/a&gt; and the 
&lt;a href=&#34;http://ftp.dot.state.tx.us/pub/txdot-info/trf/crash_statistics/2012/15_2012.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Texas Department of Transportation.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The percentage of fatal accidents by day of the week for the United States as a whole is shown in Figure 
&lt;a href=&#34;#accidents&#34;&gt;[accidents]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;accidents&amp;rdquo;} below, and the number of accidents in Texas by day of the week is shown in Figure 
&lt;a href=&#34;#texas_crah&#34;&gt;[texas_crah]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;texas_crah&amp;rdquo;}. Can we conclude at the $\alpha =0.05$ level that Texas differs from the rest of the United States in the distribution of crashes over the days of the week?&lt;/p&gt;
&lt;h2 id=&#34;chi-squared-test-of-independence&#34;&gt;Chi-Squared Test of Independence&lt;/h2&gt;
&lt;p&gt;A common question in research is what, if any, relationship two variables have with one another. If the two variables are numeric, then the most common measure of dependence is &lt;strong&gt;correlation&lt;/strong&gt;, which we will discuss later. Correlation specifically measures the degree of linear relationship between two numeric variables. The central question in this handout is: how can we examine the relationship between two nominal/categorical variables?&lt;/p&gt;
&lt;p&gt;Just as with the chi-squared goodness-of-fit test, we can employ two basic (and complementary) techniques: graphs and a hypothesis test. Graphs such as &lt;strong&gt;side-by-side bar charts&lt;/strong&gt; allow us to examine the distribution of one variable (called¬†the¬†response variable) for each level of another variable that we believe somehow influences the response variable; this latter variable is called the**¬†explanatory (independent) variable. If there is a relationship, the pattern of the response variable will change at different levels of the explanatory variable.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s look again at the question on the 2012 General Social Survey (GSS) that asked respondents to what degree they agreed with the statement &amp;ldquo;Both men and women contribute to household income.&amp;rdquo; The results are summarized below in Figure 
&lt;a href=&#34;#husweef&#34;&gt;[husweef]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;husweef&amp;rdquo;}. We can easily see that most people agree or strongly agree with the statement.&lt;/p&gt;
&lt;p&gt;This graph shows the responses of all $1,974$ people in the 2012 survey added across all age groups,income levels, ages, and political affiliations. What if we were interested in examining whether there was a relationship between belief in both men and women contributing to household income and gender? Figure 
&lt;a href=&#34;#two_inc_gen&#34;&gt;[two_inc_gen]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;two_inc_gen&amp;rdquo;}¬†shows the distribution of agreement on the question by gender. What we look for when we examine a side-by-side bar chart is a change in the &lt;em&gt;pattern&lt;/em&gt; of responses by the levels of the independent variable. The heights of the bars will usually differ, as there are typically different sample sizes in the two categories of the independent variable. In the figure below, we do not see a clear change to the pattern of responses between men and women, and would therefore¬†(informally) conclude that there is little or no relationship between gender and belief in both men and women contributing to household income.&lt;/p&gt;
&lt;p&gt;How can we assess this relationship formally? As you might have gathered, we can accomplish this by the use of a hypothesis test. In order to understand the test, we have to recall the definition of joint and marginal probabilities.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;joint probability distribution&lt;/strong&gt; for two discrete random variables is a specification of the possible values that the two variables can take on simultaneously and their simultaneous, or &amp;ldquo;joint,&amp;rdquo; probabilities.&lt;/p&gt;
&lt;p&gt;As a simple example, suppose the random variable $X$ can take on the values of $1,2$, or $3$, ¬†and the random variable $Y$ can take on the values $1$ and $2$. Then a joint probability distribution for $X$ and $Y$ might be as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;X&lt;/th&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;.24&lt;/td&gt;
&lt;td&gt;.12&lt;/td&gt;
&lt;td&gt;.06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;.30&lt;/td&gt;
&lt;td&gt;.14&lt;/td&gt;
&lt;td&gt;.14&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The numbers inside the table are the &lt;strong&gt;joint probabilities,&lt;/strong&gt; the probabilities that $X$ will take on a certain value $x$ and $Y$ will take on a certain value $y,$ which we write as $P(X=x,Y=y)$. For instance, we could say that the probability that $X$ $=$ $1$ &lt;em&gt;and&lt;/em&gt; $Y=2$ is $% P(X=1,Y=2)=0.12,$ since this number is in the intersection of the $&amp;quot;1&amp;quot;$ row and and $&amp;quot;2&amp;quot;$ column. Similarly, $P(X=2,Y=1)=0.28$. Let&amp;rsquo;s check our understanding.&lt;/p&gt;
&lt;p&gt;[[joint_prob_ex]]{#joint_prob_ex label=&amp;quot;joint_prob_ex&amp;rdquo;}Find the following joint probabilities.&lt;/p&gt;
&lt;p&gt;a). $P(X=1,Y=1)$&lt;/p&gt;
&lt;p&gt;b). $P(X=1,Y=2)$&lt;/p&gt;
&lt;p&gt;c). $P(X=1,Y=3)$&lt;/p&gt;
&lt;p&gt;d). $P(X=2,Y=1)$&lt;/p&gt;
&lt;p&gt;e). $P(X=2,Y=2)$&lt;/p&gt;
&lt;p&gt;f). $P(X=2,Y=3)$&lt;/p&gt;
&lt;p&gt;From a joint distribution, we can always obtain the distribution of each variable individually by adding across the levels of the other variable. These distributions are called &lt;strong&gt;marginal distributions&lt;/strong&gt; because they can be computed in the margins of a table like the one above. Here is the table above, with the totals of the rows and columns in the margins:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;$X$&lt;/th&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;.24&lt;/td&gt;
&lt;td&gt;.12&lt;/td&gt;
&lt;td&gt;.06&lt;/td&gt;
&lt;td&gt;.42&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;.30&lt;/td&gt;
&lt;td&gt;.14&lt;/td&gt;
&lt;td&gt;.14&lt;/td&gt;
&lt;td&gt;.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;.54&lt;/td&gt;
&lt;td&gt;.26&lt;/td&gt;
&lt;td&gt;.20&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The marginal distributions for $X$ and $Y$ can then be listed separately. The marginal distribution of $X$ is&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;$x$&lt;/th&gt;
&lt;th&gt;$P(X=x)$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$1$&lt;/td&gt;
&lt;td&gt;$.42$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$2$&lt;/td&gt;
&lt;td&gt;$.58$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;,&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;and the marginal distribution of $Y$ is&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;$y$&lt;/th&gt;
&lt;th&gt;$P(Y=y)$&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$1$&lt;/td&gt;
&lt;td&gt;$.54$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$2$&lt;/td&gt;
&lt;td&gt;$.26$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$3$&lt;/td&gt;
&lt;td&gt;$.20$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;.&lt;/p&gt;
&lt;p&gt;We can now define the concept of independence.&lt;/p&gt;
&lt;p&gt;[[independence_def]]{#independence_def label=&amp;quot;independence_def&amp;rdquo;}Two (discrete) random variables $X$ and $Y$ are independent if (and only if) $P(X=x,Y=y)=P(X=x)P(Y=y)$ for all possible $x$ and $y$. That is, $X$ and $Y$ are independent if every one of the joint probabilities equals the product of its associated marginal probabilities.&lt;/p&gt;
&lt;p&gt;Are $X$ and $Y$ independent in the table above? To establish independence, we need to check all of the probabilities in Exercise 
&lt;a href=&#34;#joint_prob_ex&#34;&gt;[joint_prob_ex]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;joint_prob_ex&amp;rdquo;} to see if the relation $P(X=x,Y=y)=P(X=x)P(Y=y)$ holds for each one. If we find even one instance where this relation does not hold, then we can say that $X$ and $Y$ are &lt;em&gt;not independent&lt;/em&gt; (or, more simply, we call them &lt;strong&gt;dependent).&lt;/strong&gt; Let&amp;rsquo;s find $P(X=1)P(Y=1)$ and compare to $P(X=1,Y=1)$. From the tables above we see that&lt;/p&gt;
&lt;p&gt;$$P(X=1)P(Y=1)=(0.42)(0.58)=0.2436$$&lt;/p&gt;
&lt;p&gt;and that&lt;/p&gt;
&lt;p&gt;$$P(X=1,Y=1)=0.24$$&lt;/p&gt;
&lt;p&gt;Since $0.2436\neq 0.24,$ we can conclude that $X$ and $Y$ are not independent (that is, they are dependent). We need not check the rest of the joint probabilities.&lt;/p&gt;
&lt;p&gt;Of course, in practice, we do not have tables of probabilities given to us. The contingency tables we see are formed from observed data on two variables. If we have a sufficient number of observations, we can estimate joint and marginal probabilities. Since the probabilities will be estimated, the requirement of Definition 
&lt;a href=&#34;#independence_def&#34;&gt;[independence_def]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;independence_def&amp;rdquo;}, that the joint probabilities be &lt;em&gt;exactly&lt;/em&gt; equal to the product of the marginal probabilities, will be too strict. The question we will need to ask is not &lt;em&gt;if&lt;/em&gt; there is a difference between the two probabilities, but rather &lt;em&gt;how large&lt;/em&gt; the difference is. The basis of the &lt;strong&gt;chi-squared test of independence&lt;/strong&gt; is how closely the estimated probabilities conform to the requirements of Definition 
&lt;a href=&#34;#independence_def&#34;&gt;[independence_def]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;independence_def&amp;rdquo;} when given a table of data from which these probabilities are estimated.&lt;/p&gt;
&lt;p&gt;Let us now define the test.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Specify the hypothesis:&lt;/p&gt;
&lt;p&gt;To set up the test, we assume we have two &lt;em&gt;nominal/categorical&lt;/em&gt; random variables $X$ and $Y$. There are $r&amp;gt;1$ categories in $X$ and $c&amp;gt;1$ categories in $Y$. The general contingency table looks like     the following.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;$Y$ $y_{1}$    $y_{2}$   $\ldots$   $y_{c}$                     &lt;br&gt;
$x_{1}$              $p_{12}$   $\ldots$   $p_{1c}$      $X$   $x_{2}$              $p_{21}$   $\ldots$   $%                                                             p_{2c}$             $\vdots$             $\ldots$   $\ldots$   $%                                                             \vdots$             $x_{r}$              $p_{r2}$   $\ldots$   $p_{rc}$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Here, $p_{ij}$ is the joint probability that the variable $X$ will take on the category $x_{i}$ and $Y$ will take on category $j$. For example, $p_{21}$ is the probability that $X$ takes on category $x_{2}$ and $Y$ takes on category $y_{1},$ that is, $p_{21}=P(X=x_{2},Y=y_{1})$. The null hypothesis is that $X$ and $Y$ are independent, which, according to Definition 
&lt;a href=&#34;#independence_def&#34;&gt;[independence_def]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;independence_def&amp;rdquo;}, means that the joint probabilities should equal the marginal probabilities. So, if the null hypothesis is true, then $p_{21}=P(X=x_{2},Y=y_{1})=P(X=x_{2})P(Y=y_{1}),$ and similarly for all $(x,y)$ combinations. So, we have the null hypothesis:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$H_{0}: p_{ij}=P(X=x_{i})P(Y=y_{j})\text{ for all }i,j$&lt;/li&gt;
&lt;li&gt;$H_{1}: \text{ }p_{ij}\neq P(X=x_{i})P(Y=y_{j})\text{ for some }i,j$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For our purposes, it will be sufficient to write the following (equivalent, but less precise) statements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$H_{0}: \text{ \TEXTsymbol{&amp;lt;}Variable }X&amp;gt;\text{ is independent of  \TEXTsymbol{&amp;lt;} Variable }Y&amp;gt;$&lt;/li&gt;
&lt;li&gt;$H_{1}: \text{ The two variables are dependent}\end{aligned}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where we would substitute the name of the variables for the text in the brackets.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Calculate the &lt;strong&gt;test statistic&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The test statistic is a similar to the goodness-of-fit statistic:&lt;/p&gt;
&lt;p&gt;$$\chi^{2}=\frac{\sum_{i=1}^{r}\sum_{j=1}^{c}(f_{ij}-e_{ij})^{2}}{e_{ij}}$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;where $f_{ij}$ is the &amp;ldquo;observed frequency&amp;rdquo; in cell $(i,j)$ and $e_{ij}$ is the &amp;ldquo;expected number of observations&amp;rdquo; in cell $(i,j)$ under the null hypothesis. The observed frequencies $f_{ij}$ are given by the data in the table. If the null hypothesis is true, the expected number of observations to fall in cell $(i,j)$ would be the total sample size, $n,$ times the marginal probabilities for category $x_{i}$ and category $y_{j}$. To find the $e_{ij},$ note that we can estimate the marginal probability of category $x_{i}$ by defining $\widehat{p}_{i,x}=\dsum_{j=1}^{c}\frac{f_{ij}}{n}$, that is, the total number of observations in row $i$ divided by the total number of observations in the data set. Similarly, we can estimate the marginal probability of category $\widehat{p}_{j,y}$ by taking $\widehat{p}_{j,y}=\dsum_{i=1}^{r}\frac{f_{ij}}{n},$ that is, the total number of observations in column $j$ divided by the total number of observations in the data set. Then the expected number of observations in cell $(i,j)$ is&lt;/p&gt;
&lt;p&gt;$$e_{ij}=n\left( \widehat{p}_{i,x}\right) \left( \widehat{p}_{j,y}\right)$$&lt;/p&gt;
&lt;p&gt;Note the logic behind the test statistic is the same as for the goodness-of-fit test. What it essentially is doing is comparing the actual frequencies we have to what we should see if the null hypothesis were true. If $\chi^{2}$ is &amp;ldquo;too large,&amp;rdquo; this implies that the estimated distribution is far from the theorized distribution, and therefore we doubt that the theorized distribution is the one that gave rise to the data that we have.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Compute the &lt;strong&gt;p-value&lt;/strong&gt; or &lt;strong&gt;rejection region&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$p\text{-value}=P(\chi _{(r-1)(c-1)}^{2}&amp;gt;\chi^{2})$$&lt;/p&gt;
&lt;p&gt;The the degrees of freedom value here is found by taking $(r-1)(c-1)$, or the number of rows minus one times the number of columns minus one. Like the test for means, unless we have software, we usually go with the rejection region approach. The the rejection region (i.e., the &amp;ldquo;critical values&amp;rdquo;) for a test with the $\alpha$ level of significance would be computed as follows:&lt;/p&gt;
&lt;p&gt;$${\chi _{(r-1)(c-1)}^{2}:\chi _{(r-1)(c-1)}^{2}&amp;gt;\chi _{(r-1)(c-1),\alpha }^{2}}$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As with the ANOVA $F$ test, there is only one &amp;ldquo;guard&amp;rdquo; for the rejection region, since the test is one-sided. This reflects the nature of the test statistic above. If $\chi^{2}$ is small, we are inclined to believe that the observed distribution could have been produced by the theorized distribution. If $\chi^{2}$ is large, we would conclude the opposite. So we only look for &amp;ldquo;large&amp;rdquo; values.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make a conclusion&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the $p$-value $\leq \alpha$, we reject $H_{0}$. If the $p$-value $% &amp;gt;\alpha$, we fail to reject $H_{0}$. Equivalently, if $\chi^{2}$ lies within the rejection region, we reject $H_{0}$. If $\chi^{2}$ lies outside the rejection region, we fail to reject $H_{0}$.&lt;/p&gt;
&lt;p&gt;This test is known as an &lt;strong&gt;asymptotic test&lt;/strong&gt; because it is only valid if the sample size is &amp;ldquo;large enough.&amp;rdquo; and the $e_{i}$ are all &amp;ldquo;large enough.&amp;rdquo; There is some debate about how &amp;ldquo;large&amp;rdquo; these have to be in practice for the test to work properly, but the general recommendation is that all of the $e_{ij}$ must be above $5$ for small tables, or for large tables, $80%$ are above $5$. If this is not the case, we combine some categories.&lt;/p&gt;
&lt;p&gt;All of the above makes this test look a lot more complicated than it is. Let&amp;rsquo;s work an example putting all of this together.&lt;/p&gt;
&lt;p&gt;In the $2012$ General Social Survey, respondents were asked the question &amp;ldquo;In general, can people be trusted?&amp;rdquo; A researcher is interested in seeing if whether someone has been divorced influences their response to this question. The data are shown in the table below. Can we conclude at the $\alpha =0.01$ level that being divorced affects one&amp;rsquo;s ability to trust? To help organize your answer, fill out the &amp;ldquo;Expected&amp;rdquo; table.&lt;/p&gt;
&lt;p&gt;Refer to Figure 
&lt;a href=&#34;#two_inc_gen&#34;&gt;[two_inc_gen]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;two_inc_gen&amp;rdquo;}, the distribution of agreement with the question &amp;ldquo;Both men and women should contribute to household income. The data for the total sample size of $1,267$ is shown below. Can we conclude at the $\alpha =0.05$ level that gender and agreement on the two-income question are dependent? To help organize your answer, fill out the &amp;ldquo;Expected&amp;rdquo; table.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Matched Pairs</title>
      <link>/courses/bana3363/8-matched-pairs/</link>
      <pubDate>Tue, 28 Jul 2020 15:19:27 +0000</pubDate>
      <guid>/courses/bana3363/8-matched-pairs/</guid>
      <description>&lt;h1 id=&#34;confidence-interval-for-the-difference-in-means-matched-pairs&#34;&gt;Confidence Interval for the Difference in Means (Matched Pairs)&lt;/h1&gt;
&lt;p&gt;The confidence interval that we examined in the last handout applies when we are working with two &lt;em&gt;independent&lt;/em&gt; samples. Often, however, it makes sense to examine the effect of a treatment on samples that are in some sense related. These samples are called &lt;strong&gt;matched pairs.&lt;/strong&gt; One way a matched pair can form is if each unit that is sampled (a person or object) is studied before and after a treatment is applied. The two sets of data&amp;ndash;measurements before and after treatment&amp;ndash;will not be independent because the same entity is being measured both times. Another way to form matched pairs is to make groups that are related genetically (e.g. twins), by proximity to one another (e.g., neighbors), by time of measurement (e.g., months of the year), by relationship to one another (e.g., husband/wife), or in some other fashion (e.g., socioeconomic status).&lt;/p&gt;
&lt;p&gt;When we have a matched pairs design, the analysis is actually easier than for the two independent samples case. What we do, essentially, is make a new variable that represents the &lt;em&gt;differences&lt;/em&gt; between the groups on the measure of interest, and then perform one-sample techniques for $\mu_{d},$ the population mean difference. We need to calculate $\overline{x_{D}},$ the sample mean difference, and $s_{D},$ the standard deviation of the differences. Both of these are calculated using the number of pairs, $n_{d}.$ The formula for the confidence interval is given now.&lt;/p&gt;
&lt;p&gt;A $(1-\alpha )100%$ confidence interval for the mean difference between two related groups, denoted $\mu_{d},$ is&lt;/p&gt;
&lt;p&gt;$$\overline{x_{D}}\pm t_{\alpha /2,n_{d}}\frac{s_{D}}{\sqrt{n_{d}}}$$&lt;/p&gt;
&lt;p&gt;Here is an example.&lt;/p&gt;
&lt;p&gt;A video game design firm wants to determine if there is a difference in the accuracy of gamers that can be attributed to the assignment of player actions (shooting, jumping, changing weapons, etc.) to certain button layouts. A sample of 6 professional game testers each tried two types of layouts in a particular level under a particular attack scenario. The performance (measured as the percentage of enemies defeated) for each person is shown in Figure 
&lt;a href=&#34;#vidya_game&#34;&gt;[vidya_game]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;vidya_game&amp;rdquo;}. Calculate a $90%$ confidence interval for the true average difference in performance between the two layouts. What can you conclude?&lt;/p&gt;
&lt;p&gt;In a recent study, the effect that cell phone use had on reaction time while driving was studied. A sample of $20$ drivers was selected and each one was asked to drive through an obstacle course. Each person&amp;rsquo;s average reaction time (in seconds) to stimuli along the course was recorded both during a drive with no cell phone use and during a drive where the subject was engaged in a casual conversation with a friend. The data are summarized in Figure 
&lt;a href=&#34;#phone&#34;&gt;[phone]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;phone&amp;rdquo;} below.&lt;/p&gt;
&lt;p&gt;Why might a matched pairs design be a good choice for studying this question? Compute a $99%$ confidence interval for the true average difference in reaction time between using a cell phone and not using a cell phone.&lt;/p&gt;
&lt;p&gt;A consumer group wants to determine if husbands and wives spend differing amounts of money on each other on Valentine&amp;rsquo;s Day. A sample of seven married couples who had been married at least one year was taken, and the data (in dollars spent) are shown in the table below. Why might a matched pairs design be a good choice for studying this question? Compute a $95%$ confidence interval for the true average difference between husbands and wives in the amount of money spent on Valentine&amp;rsquo;s Day .&lt;/p&gt;
&lt;p&gt;How objectively do we view ourselves in terms of attractiveness? A website offers to give a person an attractiveness score (from $1$ to $10$) based on an uploaded photo. A random sample of five people uploaded a photo the website. Before their scores were given, they were asked to predict what the computer would say. The data are shown below. Compute a $90%$ confidence interval for the true average difference between a person&amp;rsquo;s predicted rating and their actual rating.&lt;/p&gt;
&lt;p&gt;The general four-step procedure for hypothesis testing still applies. What changes are the specifics.¬†Let&amp;rsquo;s see how.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Specify the hypothesis:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$H_{0}: \mu_{d}\leq d\text{ vs. }H_{1}:\mu_{d}&amp;gt;d$&lt;/li&gt;
&lt;li&gt;$H_{0}: \mu_{d}\geq d\text{ vs. }H_{1}:\mu_{d}&amp;lt;d$&lt;/li&gt;
&lt;li&gt;$H_{0}: \mu_{d}=d\text{ vs. }H_{1}:\mu_{d}\neq d$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calculate the &lt;strong&gt;test statistic&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will assume from now on that we never know the true standard     deviation of the population, so our statistic will be a $t.$&lt;/p&gt;
&lt;p&gt;$$t_{0}=\frac{\overline{x_{d}}-d}{\frac{s_{d}}{\sqrt{n_{d}}}}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the &lt;strong&gt;rejection region&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The the rejection region (i.e., the &amp;ldquo;critical values&amp;rdquo;) for a test     with the $\alpha$ level of significance would be computed as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$t_{n_{d}-1}: t_{n_{d}-1}&amp;gt;\text{ }t_{\alpha     ,_{n_{d}-1}}$&lt;/li&gt;
&lt;li&gt;$t_{n_{d}-1}: t_{n_{d}-1}&amp;lt;-t_{\alpha ,n_{d}-1}$&lt;/li&gt;
&lt;li&gt;$t_{n_{d}-1}: |t_{n_{d}-1}|&amp;gt;\text{ }t_{\alpha /2,n_{d}-1}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where $t_{n_{d}-1}$is a Student&amp;rsquo;s $t$ random variable with $n_{d}-1$     degrees of freedom.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make a conclusion&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the $p$-value $\leq \alpha ,$ we reject $H_{0}.$ If the $p$-value $% &amp;gt;\alpha ,$ we fail to reject $H_{0}.$ Equivalently, if $t_{0}$ lies within the rejection region, we reject $H_{0}$. If $t_{0}$ lies outside the rejection region, we fail to reject $H_{0}.$&lt;/p&gt;
&lt;p&gt;Perform a hypothesis test at the $\alpha =0.05$ level for all of the examples above.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intro to Regression</title>
      <link>/courses/bana3363/15-intro-to-regression/</link>
      <pubDate>Tue, 28 Jul 2020 15:19:27 +0000</pubDate>
      <guid>/courses/bana3363/15-intro-to-regression/</guid>
      <description>&lt;h2 id=&#34;dependence-between-numeric-variables-correlation&#34;&gt;Dependence Between Numeric Variables (Correlation)&lt;/h2&gt;
&lt;p&gt;There are an infinite number of possible relationships between numeric variables. There could be quadratic relationships, cubic relationships, logarithmic relationships, and so forth. In much of statistics, the focus is on the &lt;em&gt;linear relationship&lt;/em&gt; because it is generally easy to specify, it captures the notion of dependence in many real-world situations, and many other relationships can be at least approximated by a line in some small, focused area. The measure of linear relationship between two numeric variables is known as &lt;strong&gt;correlation.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;population correlation coefficient&lt;/strong&gt; $\rho$, pronounced &amp;ldquo;row,&amp;rdquo; describes the linear association between two random variables $X$ and $Y$. It is calculated as&lt;/p&gt;
&lt;p&gt;$$\rho =\frac{E(XY)-E(X)E(Y)}{\sqrt{V(X)}\sqrt{V(Y)}}$$&lt;/p&gt;
&lt;p&gt;where $E$ stands for &amp;ldquo;expected value&amp;rdquo; and $V$ stands for variance.&lt;/p&gt;
&lt;p&gt;The correlation coefficient $\rho$ is a &lt;em&gt;population parameter,&lt;/em&gt; like $\mu$ and $p$, and must therefore be estimated with data. This leads to the next definition.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;sample correlation coefficient&lt;/strong&gt; $r$ is a random variable that describes the relationship between two sets of numeric data that are considered in pairs $(y_{1},x_{1}),(y_{2},x_{2}),\ldots ,(y_{n},x_{n})$. It is given by&lt;/p&gt;
&lt;p&gt;$$r=\frac{\sum_{i}x_{i}y_{i}-n(\overline{x})(\overline{y})}{(n-1)s_{x}s_{y}}$$&lt;/p&gt;
&lt;p&gt;where $s_{x\text{ }}$ and $s_{y}$ are the standard deviations of $x$ and $y$ , respectively.&lt;/p&gt;
&lt;p&gt;The following are facts about correlation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Always lies between $-1$ and $1$ (inclusive)&lt;/li&gt;
&lt;li&gt;The strength of correlation is measured by $|r|$, the absolute value     of the correlation coefficient&lt;/li&gt;
&lt;li&gt;Correlations of $1$ (or $-1$) indicate perfect positive (or negative) linear relationship&lt;/li&gt;
&lt;li&gt;Correlation of $0$ indicates no linear relationship&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose we wanted to create a model of gasoline prices that factors in crude oil prices. The data would appear as shown in Figure 
&lt;a href=&#34;#data&#34;&gt;[data]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;data&amp;rdquo;} in a spreadsheet program. Using Excel&amp;rsquo;s chart feature, we could make a scatter plot as shown in Figure 
&lt;a href=&#34;#gas_scatter&#34;&gt;[gas_scatter]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;gas_scatter&amp;rdquo;}. The correlation between crude oil price and gasoline price can be shown to be $r=0.987$, which is an extremely strong positive linear relationship as can be seen.&lt;/p&gt;
&lt;p&gt;The concept of correlation plays a key role in estimating the simple linear regression model, as we will see in the next section.&lt;/p&gt;
&lt;h1 id=&#34;introduction-to-regression&#34;&gt;Introduction to Regression&lt;/h1&gt;
&lt;p&gt;The broad set of techniques that fall under the &amp;ldquo;regression analysis&amp;rdquo; title drive most practical research studies, from drug trials in the pharmaceutical industry to new product test marketing. Businesses face tough competition in the information age. Not only must they navigate an increasingly fragmented customer base, manage their reputations through traditional and social media channels, innovate with products and services at an increasing rate, and negotiate with many suppliers and distributors, but they must also make short- and long-term decisions about personnel, production, new markets to enter, and strategies to increase (or at least maintain) market share against competitors. Sites such as Kickstarter &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and Fundable&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; specialize in a type of online venture capital process called &amp;ldquo;crowdfunding,&amp;rdquo; where ordinary individuals can contribute to a project in exchange for a variety of rewards, from a simple acknowledgement as a supporter, to a functioning prototype of the company&amp;rsquo;s product, to a percentage of sales. You will no doubt hear a good deal more on this topic in your marketing and management classes.&lt;/p&gt;
&lt;p&gt;Managers must be able to make decisions that incorporate a number of factors. Increasingly, these decisions have been influenced by mathematical modeling. A new field called &lt;strong&gt;business analytics&lt;/strong&gt;&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; has arisen that seeks to use advanced mathematics and large data sets to discover patterns and relationships among consumer choices that can help optimize product and service delivery. If you have ever wondered why grocery stores have rewards cards, the answer is simple: it allows the store to track your purchases in order to present you with customized deals and to understand how your purchases relate to those of other customers. If you use Netflix to stream movies, you should know that everything you do&amp;ndash;the programs you search for, the number of searches you perform, the time you spend browsing your recommendations, the device you are using to watch the program, the genre of the programs you view most often, and more&amp;ndash;is being monitored&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. The data are fed into complex models designed by mathematicians and statisticians, and the patterns extracted from the data imply that, for instance, &lt;em&gt;Hot Tub Time Machine&lt;/em&gt; and &lt;em&gt;Les Miserables&lt;/em&gt; might appeal to the same user.&lt;/p&gt;
&lt;p&gt;Mathematical models can be divided into two three broad classes: &lt;strong&gt;deterministic,&lt;/strong&gt; &lt;strong&gt;probabilistic,&lt;/strong&gt; and &lt;strong&gt;regression&lt;/strong&gt; models. The last type is actually a blend of the first two. Let&amp;rsquo;s define these terms.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;deterministic model&lt;/strong&gt; is a mathematical equation for which, if a set of inputs $x$ are known, the output $y$ is known with certainty.&lt;/p&gt;
&lt;p&gt;Deterministic models are what we are most familiar with in elementary mathematics. They are why mathematics is seen as &amp;ldquo;objective,&amp;rdquo; in the sense that there is often only one &amp;ldquo;right answer&amp;rdquo; to a problem.¬†One simple example of a deterministic model that you should be familiar with is the formula for future value:$FV=P(1+r/n)^{nt}$. This formula is deterministic because, given a present value $P$, an interest rate $r$, the compounding frequency $n$, and the investment period $t$, we can calculate what the future value of the investment will be. That is, given $P,r,n$, and $t$, $FV$ is known with certainty. Another example of a deterministic model is the formula for a company&amp;rsquo;s total cost: $T=F+Vq$, where $F$ is a company&amp;rsquo;s fixed cost, $V$ is the variable cost per unit, and $q$ is the quantity produced. Given $F$, $V$, and $q$, we can compute the company&amp;rsquo;s total cost. Remembering the formula for distance ($D=st$, where $s=speed)$, the time it takes to drive $45$ miles to campus, if you drive $s$ miles per hour is $t=\frac{45}{s}$.&lt;/p&gt;
&lt;p&gt;These models are simple and leave much room for improvement. For instance, the future value formula assumes the interest rate remains fixed over time, which will rarely be the case. The total cost equation, meanwhile, ignores the fact that, for some level of production $q^{\ast }$, the company will need to expand and change its fixed costs to $F^{\ast }$ to keep up. More sophisticated models could be built to account for these factors. However, even when we incorporate these other variables, it is unlikely that we will be able to perfectly, to an infinite number of decimal places, the total cost of making a product or the future value of an investment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Variability&lt;/strong&gt; is a basic characteristic of the natural world. Every time you drive $45$ miles to get to campus, it takes you a different amount of time to arrive here because your speed fluctuates by small amounts over the course of your trip. Everyone lives to a different age. People spend different amounts of money on each shopping trip. Differing amounts of soda are in a can that states that it contains $12$ ounces. With deterministic models, variability is ignored, so they are technically wrong.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;probabilistic model&lt;/strong&gt; assumes that the values of a variable arise from a random sample from a probability distribution (for example, the normal distribution).&lt;/p&gt;
&lt;p&gt;These are the models that we have been working with all semester! For instance, if we were to calculate a confidence interval for the population mean amount of money that Americans give to charity each year, we¬†might have said:¬†&amp;ldquo;Assume you have a random sample of $n=1000$ people and their average yearly charitable contribution was $\overline{y}=1000$ and $s=300.&amp;ldquo;$ The histogram of the charitable contributions might appear as in Figure 
&lt;a href=&#34;#charity_cont&#34;&gt;[charity_cont]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;charity_cont&amp;rdquo;}.&lt;/p&gt;
&lt;p&gt;It is undeniable that the amount of money that people contribute to charity varies from person to person and even from time to time for the same person over different years. Assuming that &amp;ldquo;charitable contributions&amp;rdquo; is a random variable is a useful first approach to understanding the natural world, but we can do much better by bringing in additional information.&lt;/p&gt;
&lt;p&gt;It is sensible that charitable contributions depend on many factors, one of which is a person&amp;rsquo;s income (other variables might be education level, political affiliation, gender, etc.). We can model the idea that charitable contribution is a random variable, but one that is influenced by another (possibly nonrandom) variable. We can get at this idea by making a reasonable assumption:&lt;/p&gt;
&lt;p&gt;for any given income, there is an entire distribution of possible &amp;gt; charitable contributions that a person with that income could choose &amp;gt; to make.&lt;/p&gt;
&lt;p&gt;It would make sense that, as income increases, at least the mean contribution should increase, but how?&lt;/p&gt;
&lt;p&gt;One possibility is that the mean changes linearly with income. That is, if income is $x$ dollars per year, the average charitable contribution is $\beta_{0}+\beta_{1}x$, where $\beta_{0}$ (read¬†as &amp;ldquo;beta zero&amp;rdquo;) is the intercept of the line and $\beta_{1}$ (read as &amp;ldquo;beta one&amp;rdquo;) is the slope. These two $\beta$ values are **population parameters**, like $\mu$ and $p$, that we can use data to estimate. This is shown in Figure 
&lt;a href=&#34;#reg_model&#34;&gt;[reg_model]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;reg_model&amp;rdquo;}. The linear part that specifies how the mean changes with increasing income is **deterministic** (if we know $x$, $\beta_{0}$, and $\beta_{1}$, we know the mean with certainty), but the actual value is random, modeled as a draw from a normal distribution with mean $\beta_{0}+\beta_{1}x$. Now we have the next definition.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;regression model&lt;/strong&gt; is a blend of deterministic and probabilistic models that specifies how the distribution of some random variable $Y$ changes for a function of other (possibly nonrandom) variables $f(x_{1},x_{2},\ldots ,x_{p})$.&lt;/p&gt;
&lt;p&gt;How do we figure out what that function $f(x_{1},x_{2},\ldots ,x_{p})$ is? The best way is to simply graph the $\ Y$ variable with each of the $x$ variables using a **scatter plot**, a graph that plots each pair of points $(y_{i},x_{ij})$ in the ordinary $x-y$ plane. Looking at the scatter, you can see if a line might fit the data fairly well, or if another function such as a quadratic might work better. The simple linear regression model, which we now define, claims that the distribution of $Y$ depends only on one other variable, $x$.&lt;/p&gt;
&lt;p&gt;For a random sample of bivariate data $(Y_{1},x_{1}),(Y_{2},x_{2}),\ldots ,(Y_{n},x_{n})$, where the $Y_{i}$ are random variables and the $x_{i}$ are constants, the **simple linear regression model** relating $Y$ to $x$ is given by&lt;/p&gt;
&lt;p&gt;$Y_{i} =\beta_{0}+\beta_{1}x_{i}+\varepsilon_{i},\text{ }i=1,2,\ldots n\text{ [sample form]}$&lt;/p&gt;
&lt;p&gt;$Y =\beta_{0}+\beta_{1}x+\varepsilon \text{ [population form]}$&lt;/p&gt;
&lt;p&gt;where $\beta_{0}$¬†(read as &amp;ldquo;beta zero&amp;rdquo;) and $\beta_{1}$¬†(read as &amp;ldquo;beta one&amp;rdquo;) are fixed (but unknown) parameters and $\varepsilon_{i}$¬†(read as &amp;ldquo;epsilon&amp;rdquo;) are independent normally distributed random variables with mean $0$¬†and standard deviation $\sigma$¬†that are uncorrelated with $x$.&lt;/p&gt;
&lt;p&gt;In Figure 
&lt;a href=&#34;#reg_model&#34;&gt;[reg_model]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;reg_model&amp;rdquo;}, the only $x$ variable is income, but there are many other factors that affect charity that we could add to the model. What this equation says is that the actual value of $Y$ that we observe is made up of a deterministic (fixed and known) $x$ and a random (unknown) component $\varepsilon$. The $\varepsilon$ value is made up of the countless other factors that influence $Y$ that we have not explicitly included as an $x$ variable in the model. We require $\varepsilon$ to be in the model because we can never know $Y$ with absolute certainty. However, we hope that the relationship between $Y$ and $x$ is strong enough to account for a good bit of the change in $Y$, because we can quantify this change by estimating $\beta_{0}$ and $\beta_{1}$ (more on this below).&lt;/p&gt;
&lt;p&gt;The implication of equation 
&lt;a href=&#34;#pop_version&#34;&gt;[pop_version]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;pop_version&amp;rdquo;} is that the mean of $Y$ is $E(Y|x)=$ $\beta_{0}+\beta_{1}x$ because, by the rules of expected value¬†(see Handout $1$),&lt;/p&gt;
&lt;p&gt;$E(Y|x) = E(\beta_{0}+\beta_{1}x+\varepsilon )$&lt;/p&gt;
&lt;p&gt;$ = \beta_{0}+\beta_{1}x+E(\varepsilon )\text{ [since }\beta_{0}+\beta_{1}x\text{ is a constant]}$&lt;/p&gt;
&lt;p&gt;$ = \beta_{0}+\beta_{1}x\text{ [since we assume }E(\varepsilon )=0]$&lt;/p&gt;
&lt;p&gt;We put the $&amp;quot;|x&amp;quot;$ part in the expected value to emphasize that the mean of $Y$ depends on $x$. A shorthand notation for this is $\widehat{Y}$, read as $&amp;quot;Y$ hat.&amp;rdquo; We also have the following important fact:&lt;/p&gt;
&lt;p&gt;Because $\varepsilon$ is assumed to have a normal distribution, $Y$ also has a normal distribution, but with mean $\beta_{0}+\beta_{1}x.\label{simple_ols_theorem}$&lt;/p&gt;
&lt;p&gt;We can interpret $\beta_{0}$ and $\beta_{1}$ as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\beta_{0}$ is the mean of $Y$ when $x=0$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\beta_{1}$ is the increase in the mean of $Y$ for a one-unit increase in $x$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As stated above, the values of $\beta_{0}$ and $\beta_{1}$ are parameters **¬†**that we must use data to estimate. It is not immediately obvious what the &amp;ldquo;best&amp;rdquo; way to do this is, but a reasonable choice would be to find the estimated values $b_{0}$ and $b_{1}$ such that the line $b_{0}+b_{1}x$ goes through most of the data points. Consider Figure 
&lt;a href=&#34;#gas_scatter&#34;&gt;[gas_scatter]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;gas_scatter&amp;rdquo;}. It is fairly easy to imagine drawing a line through the scatterplot in such a way that the data fall close to the line. It&amp;rsquo;s a bit more challenging to do this with a data set such as the one shown in Figure 
&lt;a href=&#34;#c02&#34;&gt;[c02]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;c02&amp;rdquo;}, but it still appears as if a line might be an okay fit.&lt;/p&gt;
&lt;p&gt;Notice that, even though we know that gasoline is directly related to crude oil through refining, we cannot perfectly predict gasoline prices using crude oil prices. However, using crude oil prices will let us make a much better prediction than if we just worked with the gasoline data alone. Figure 
&lt;a href=&#34;#gasonoly&#34;&gt;[gasonoly]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;gasonoly&amp;rdquo;} displays a histogram of only the gas prices, which would (falsely) lead us to believe that our best prediction of gasoline price would be in the $$0.50-$1$ range, which is clearly not realistic.&lt;/p&gt;
&lt;p&gt;One way to fit a line is to choose two points, say, $(x_{1},y_{1})$ and $(x_{2},y_{2})$, out of all the points in the scatterplot and find the slope and $y$ intercept. But which points would you pick? Your choice of two points might not match mine, and ours might still be different from a third person&amp;rsquo;s. Clearly, we need a more objective way of fitting the line. The most common method used in practice is known as the**¬†method of ordinary least squares (OLS)**, which we now define.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;ordinary least squares (OLS) estimates,&lt;/strong&gt; $b_{0}$ and $b_{1}$, estimate $\beta_{0}$ and $\beta_{1}$ and are chosen such that the &amp;ldquo;sum of squared errors&amp;rdquo;, SSE, is made as small as possible. That is, we choose $b_{0}$ and $b_{1}$ so that&lt;/p&gt;
&lt;p&gt;$$SSE=\sum (y_{i}-b_{0}-b_{1}x_{i})^{2}$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;is minimized.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If you know about multivariable calculus, the approach is clear: find the partial derivatives of $SSE$ with respect to $b_{0}$ and $b_{1}$, set them equal to $0$, and solve for $b_{0}$ and $b_{1}$. In this course, we will simply state the solution as a theorem without proof.&lt;/p&gt;
&lt;p&gt;The values of $b_{0}$ and $b_{1}$ that minimize $\sum (y_{i}-b_{0}-b_{1}x_{i})^{2}$ are $b_{1}=r\frac{s_{y}}{s_{x}}$ and $b_{0}=\overline{y}-b_{1}(\overline{x})$, where $s_{y}$ and $s_{x}$ are the standard deviations of the observed data ${y_{i}}$ and ${x_{i}}$, respectively, and $r$ is the sample correlation coefficient between ${y_{i}}$ and ${x_{i}}$.&lt;/p&gt;
&lt;p&gt;Once we have estimated $b_{0}$ and $b_{1}$, we can estimate $y$ using the equation&lt;/p&gt;
&lt;p&gt;$$\widehat{y}=b_{0}+b_{1}x$$&lt;/p&gt;
&lt;p&gt;We can simply plug in any value of $x$ and we get an estimate of $y$. What happened to the error term $\varepsilon ?$ Since the error term is not known, we simply use its mean of $0$ to estimate it. What does our estimate of $y$ represent? $\widehat{y}$**¬†is the estimated mean of the random variable** $Y$**¬†at the value of** $x!$ Thus, through equation 
&lt;a href=&#34;#simple_model_eq&#34;&gt;[simple_model_eq]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;simple_model_eq&amp;rdquo;}, we have now defined a model that allows the distribution of $Y$ to change with $x$.&lt;/p&gt;
&lt;p&gt;The scatter plot of gasoline and oil prices with the least-square line is shown in Figure 
&lt;a href=&#34;#scatter_and_reg&#34;&gt;[scatter_and_reg]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;scatter_and_reg&amp;rdquo;}. The plot was made using Excel&amp;rsquo;s scatter plot and &amp;ldquo;Add trendline&amp;rdquo; options. The estimated equation is&lt;/p&gt;
&lt;p&gt;$$\widehat{y}=0.0268x+0.0315$$&lt;/p&gt;
&lt;p&gt;where $x$ is the price of oil per barrel. From this equation referring to 
&lt;a href=&#34;#simple_ols_theorem&#34;&gt;[simple_ols_theorem]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;simple_ols_theorem&amp;rdquo;}, we estimate that for each $1$ increase in the price of oil per barrel, the average price of gasoline increases by about $0.0268$, or about three cents. Further, looking at $b_{0}$, we can say (rather pointlessly) that the average price of gasoline when oil is $0$ per barrel is $0.0315$. Of course, oil will always cost something per barrel, so $b_{0}$ does not have a realistic interpretation here.&lt;/p&gt;
&lt;h1 id=&#34;using-the-simple-linear-regression-model&#34;&gt;Using the Simple Linear Regression Model&lt;/h1&gt;
&lt;p&gt;After the model has been fit, the next logical question is whether the model is useful or not. One way of assessing this is to determine if $x$ has significant predictive ability for $Y$. Since the effect of $x$ on $Y$ is measured by the unknown parameter $\beta_{1}$, the natural approach is to conduct a hypothesis test or construct a confidence interval for $\beta_{1}$ . First, we will address the confidence interval, since we can essentially define the test by determining whether a specified null hypothesis value of $\beta_{1}$ lies in the interval or not, much like we have done already. Before we can define the interval, we have to define a few terms related to the estimate $b_{1}$, which is what we use to construct the interval.&lt;/p&gt;
&lt;p&gt;The estimated value of $\beta_{1}$, known as $b_{1}$, is a random variable with a mean equal to $\beta_{1}$ and a standard deviation (called a **standard error)** of $\sqrt{\frac{\sigma ^{2}}{\sum (x_{i}-\overline{x})^{2}}}$. Since we rarely assume $\sigma ^{2}$ is known, we set $s.e.(b_{1})=\sqrt{\frac{SSE/(n-2)}{\sum (x_{i}-\overline{x})^{2}}}=\sqrt{\frac{SSE/(n-2)}{(n-1)s_{x}^{2}}}$, where $s_{x}$ is the standard deviation of the $x$ data.&lt;/p&gt;
&lt;p&gt;A consequence of this fact leads to the definition of the $(1-\alpha )100%$ confidence interval for $\beta_{1}$:&lt;/p&gt;
&lt;p&gt;A $(1-\alpha)100%$ confidence interval for $\beta_{1}$ is given by&lt;/p&gt;
&lt;p&gt;$$b_{1}\pm t_{\alpha /2,n-2}\sqrt{\frac{SSE/(n-2)}{(n-1)s_{x}^{2}}}$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;where&lt;/em&gt; $t_{n-2,\alpha /2}$¬†is the value of Student&amp;rsquo;s $t$ ¬†distribution with* $n-2$¬†degrees of freedom and* $\alpha/2$¬†probability to the right.&lt;/p&gt;
&lt;p&gt;We use $n-2$ because we had to estimate two values, $b_{0}$ and $b_{1}$, using the data. The hypothesis testing procedure follows a familiar pattern:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Specify the hypotheses&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The parameter of interest is $\beta_{1}$, so the hypotheses are statements about its value.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$H_{0}: \beta_{1}\leq c\text{ vs. }H_{0}:\beta_{1}&amp;gt;c$&lt;/li&gt;
&lt;li&gt;$H_{0}: \beta_{1}\geq c\text{ vs. }H_{0}:\beta_{1}&amp;lt;c$&lt;/li&gt;
&lt;li&gt;$H_{0} &amp;amp;:&amp;amp;\beta_{1}=c\text{ vs. }H_{0}:\beta_{1}\neq c$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In practice, $c$ is usually $0$, but it could be anything. In regression programs such as Excel, the hypothesis tested by default is&lt;/p&gt;
&lt;p&gt;$$H_{0}:\beta_{1}=0\text{ vs. }H_{0}:\beta_{1}\neq 0$$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Calculate the test statistic.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The test statistic is a $t$ statistic:&lt;/p&gt;
&lt;p&gt;$$t=\frac{b_{1}-c}{s.e.(b_{1})}$$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Determine the rejection region (or compute the $p$-value).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The test statistic is a Student&amp;rsquo;s $t$ statistic, so the critical values that mark the rejection regions are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$t_{n-2}: t_{n-2}&amp;gt;\text{ }t_{n-2,\alpha }}$&lt;/li&gt;
&lt;li&gt;$t_{n-2}: t_{n-2}&amp;lt;-\text{ }t_{n-2,\alpha }}$&lt;/li&gt;
&lt;li&gt;$t_{n-2} &amp;amp;:&amp;amp;|t_{n-2}|&amp;gt;\text{ }t_{n-2,\alpha /2}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Alternatively, with software we can calculate the $p$-value as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p = P(t_{n-2}&amp;gt;t)$&lt;/li&gt;
&lt;li&gt;$p &amp;amp;=&amp;amp;P(t_{n-2}&amp;lt;t)$&lt;/li&gt;
&lt;li&gt;$p &amp;amp;=&amp;amp;2\min [P(t_{n-2}&amp;lt;t_{0}),P(t_{n-2}&amp;gt;t)]$&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;Make a conclusion&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the $p$-value $\leq \alpha$, we reject $H_{0}$. If the $p$-value $&amp;gt;\alpha$, we fail to reject $H_{0}$. Equivalently, if $t$ lies within the rejection region, we reject $H_{0}$. If $t$ lies outside the rejection region, we fail to reject $H_{0}$.&lt;/p&gt;
&lt;p&gt;As was the case when we tested means from one and two populations, the following theorem is true:&lt;/p&gt;
&lt;p&gt;If the $(1-\alpha )100%$ confidence interval for $\beta_{1}$ **contains** $c$, then a test of $H_{0}:\beta_{1}=c$ vs. $H_{0}:\beta_{1}\neq c$ at the $\alpha$ level will **fail to reject** $H_{0}$.&lt;/p&gt;
&lt;p&gt;In practice, software programs such as Excel are used to fit regression models. Excel has a built-in tool to do this in the same location as the ANOVA program. The output even looks similar. As an example, let&amp;rsquo;s fit a simple linear regression model to the crude oil/gasoline price data which are graphed in Figure 
&lt;a href=&#34;#gas_scatter&#34;&gt;[gas_scatter]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;gas_scatter&amp;rdquo;}. Fitting the model using Excel gives output shown below in Figure 
&lt;a href=&#34;#reg_ouytpu&#34;&gt;[reg_ouytpu]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;reg_ouytpu&amp;rdquo;}. Let&amp;rsquo;s examine the output.&lt;/p&gt;
&lt;p&gt;Using the output above, answer the following questions:&lt;/p&gt;
&lt;p&gt;a). What is the estimated model?&lt;/p&gt;
&lt;p&gt;b). Interpret the coefficients $b_{0}$¬†and $b_{0}$ ¬†in context.&lt;/p&gt;
&lt;p&gt;c). What is the $95%$¬†confidence interval for $\beta_{1}?$&lt;/p&gt;
&lt;p&gt;d). What are the hypotheses that the $\mathit{p}$-value refers to?&lt;/p&gt;
&lt;p&gt;e). Based on the $p$-value and on the confidence interval, what conclusion can you make about the relationship between crude oil price and gasoline price?&lt;/p&gt;
&lt;p&gt;What is the relationship between the square footage of a house and its selling price? Figure 
&lt;a href=&#34;#housescatter&#34;&gt;[housescatter]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;housescatter&amp;rdquo;}¬†is a scatterplot of over $2,000$ selling prices of houses in Ames, Iowa, and their associated square footages. This plot was made using a free statistics program called R &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;. The regression output from Excel is shown in Figure&lt;/p&gt;
&lt;p&gt;a). What is the estimated model?&lt;/p&gt;
&lt;p&gt;b). Interpret the coefficients $b_{0}$¬†and $b_{0}$ ¬†in context.&lt;/p&gt;
&lt;p&gt;c). Calculate the $95%$¬†confidence interval for $\beta_{1}$.&lt;/p&gt;
&lt;p&gt;d). What are the hypotheses that the $p$-value refers to?&lt;/p&gt;
&lt;p&gt;e). Based on the $p$-value and on the confidence interval, what conclusion can you make about the relationship between the selling price of a house and its square footage?&lt;/p&gt;
&lt;p&gt;Does the amount of education that one&amp;rsquo;s father has influence the amount of education his children have? A sample of $20$ people were asked to report the number of years of formal schooling they had along with the number of years their fathers had. The data are shown in the table below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Dad_Schooling&lt;/th&gt;
&lt;th&gt;Child_Schooling&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Use Excel to fit the simple linear regression model to the data above.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Interpret the coefficients $b_{0}$¬†and $b_{0}$ ¬†in context.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calculate a $90%$¬†confidence interval for $\beta_{1}$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What are the hypotheses that the $p$-value refers to?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Based on the $p$-value and on the confidence interval, what conclusion can you make about the relationship between a child&amp;rsquo;s education level and his or her father&amp;rsquo;s education level?&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;evaluating-the-simple-linear-regression-model&#34;&gt;Evaluating the Simple Linear Regression Model&lt;/h2&gt;
&lt;p&gt;The essential components of regression analysis are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We assume that two variables, $x$ and $Y,$ are somehow related.&lt;/li&gt;
&lt;li&gt;Specifically, we assume the probability distribution of $Y$ (which is called the response or dependent variable) changes as $x$ (which is called the predictor or independent variable) changes.&lt;/li&gt;
&lt;li&gt;Specifically, we assume that the mean (or expected value) of ¬†$Y,$ denoted as $E(Y),$ changes as $x$ changes. To indicate this     we write $E(Y|x)$ read as &amp;ldquo;the expected value of $Y$ given $x.&amp;ldquo;$&lt;/li&gt;
&lt;li&gt;We can also indicate $E(Y|x)$ using the notation $\widehat{Y},$ read as $&amp;quot;y$ hat.&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Specifically, with simple regression, we assume that $Y=\beta_{0}+\beta_{1}x+\varepsilon$, where $\varepsilon$ has a normal     distribution with mean $0$ and standard deviation $\sigma $.&lt;/li&gt;
&lt;li&gt;Given a set of paired data points $(Y_{1},x_{1}),$ $(Y_{2},x_{2}),\ldots ,(Y_{n},x_{n}),$ we can fit the simple linear     regression model $\widehat{y}=b_{0}+b_{1}x$ by finding the values $b_{0}$ and $b_{1}$ that makes the &amp;ldquo;sum of squared errors&amp;rdquo; $SSE=$     $\sum (y_{i}-(b_{0}+b_{1}x_{i}))^{2}$ as small as possible. As you will recall, this is the **OLS (ordinary least squares) criterion.**&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once the model is fit, we should examine how good the fit is. Several items should be checked, but they fall under three general categories: $1)$. Does the model make sense? $2)$. Are the assumptions satisfied? and $3)$. Is the model practically useful? Statisticians have devised many ways to address these questions, some qualitative, some quantitative. Many of the quantitative methods are focused on examining $(2)$, since the validity of the predictions from the model rest upon the assumptions being at least reasonably satisfied. However, even if the assumptions are badly violated, the model could be made to be useful by making adjustments to the model-fitting process.&lt;/p&gt;
&lt;h2 id=&#34;assessing-the-fit-of-the-model&#34;&gt;Assessing the Fit of the Model&lt;/h2&gt;
&lt;p&gt;Whether the fitted model $\widehat{y}=b_{0}+b_{1}x$ makes sense or not depends on your knowledge of the subject you are trying to learn about, and specifically on your idea of how the two variables should be related. For example, it seems sensible that as income increases, the total amount of charitable contributions should increase as well. The relationship might be linear or non-linear. Two possible relationships are shown in Figures 
&lt;a href=&#34;#reg_model&#34;&gt;[reg_model]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;reg_model&amp;rdquo;} and 
&lt;a href=&#34;#quadreg&#34;&gt;[quadreg]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;quadreg&amp;rdquo;} below. However, other relationships are possible. A recent study has shown that the very wealthy are less generous in that they give a smaller percentage of their income to charity&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;The most important way to determine the relationship between variables is to simply graph the data. Scatter plots such as those in Figures 
&lt;a href=&#34;#gas_scatter&#34;&gt;[gas_scatter]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;gas_scatter&amp;rdquo;} and 
&lt;a href=&#34;#c02&#34;&gt;[c02]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;c02&amp;rdquo;} confirm what we suspect:¬†higher oil prices lead to higher gas prices, and the more cars on the road, the higher the CO$_{2}$ output. But what about the relationship between the square footage of a house and its selling price? Figure 
&lt;a href=&#34;#housescatter&#34;&gt;[housescatter]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;housescatter&amp;rdquo;}¬†is a scatterplot of over $2,000$ selling prices of houses in Ames, Iowa, and their associated square footage. Is the relationship linear or quadratic? In this case, it is a little harder to see.&lt;/p&gt;
&lt;h2 id=&#34;the-coefficient-of-determination&#34;&gt;The coefficient of determination&lt;/h2&gt;
&lt;p&gt;One way of measuring the fit of a model is known as $R^{2},$ read as $&amp;quot;R$ square,&amp;rdquo; which we now define.&lt;/p&gt;
&lt;p&gt;$R^2$, also known as &lt;strong&gt;the coefficient of determination&lt;/strong&gt; is the proportion of total variability in $Y$ that is accounted for, or &amp;ldquo;explained,&amp;rdquo; by the regression fit between $Y$ and $x$.&lt;/p&gt;
&lt;p&gt;The idea of &amp;ldquo;explained variability&amp;rdquo; should remind you of the ANOVA discussion. ANOVA is, in fact, a type of regression model where the $x$ variable is a categorical rather than a numeric variable. Remember that the &lt;strong&gt;fundamental assumption of statistics&lt;/strong&gt; is that variability exists in the world. It is inescapable. So the question is not whether we can eliminate variability,*¬†*but rather*¬†how we can account for it.* In the three data sets plotted above, we clearly see variability in the $Y$ variable. Some of that variability can potentially be &amp;ldquo;explained&amp;rdquo; by a regression line fit to the data. The rest we regard as &amp;ldquo;unexplained.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Figure 
&lt;a href=&#34;#rsq&#34;&gt;[rsq]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;rsq&amp;rdquo;} shows the idea. Panel (a) shows that the total variability in the data can be measured by the differences between each actual observation and the sample mean $\overline{y}$. We already know a measure for that:¬†the sample standard deviation $s$. However, the variability is really measured just by the numerator of that equation, which we defined as $&amp;quot;SSTOT,&amp;ldquo;$ which stands for &amp;ldquo;total sum of squares.&amp;rdquo;¬†This is defined as&lt;/p&gt;
&lt;p&gt;$$SSTOT=\sum (y_{i}-\overline{y})^{2}$$&lt;/p&gt;
&lt;p&gt;In panel (c), we see that some of this variability can be &amp;ldquo;explained&amp;rdquo; by the regression line through the differences between the values $\widehat{y_{i}}$ and the sample mean $\overline{y}$. This &amp;ldquo;explained&amp;rdquo; variability is measured by a sum of squares as well, which we call $SSR$, which stands for &amp;ldquo;sum of squares for regression.&amp;rdquo; This is defined as:&lt;/p&gt;
&lt;p&gt;$$SSR=\sum (\widehat{y_{i}}-\overline{y})^{2}$$&lt;/p&gt;
&lt;p&gt;The remaining variability is accounted for by the differences in the actual values, $y_{i},$ and the predicted values $\widehat{y_{i}}$. This is also a sum of squares, called , $SSE,$ which stands for &amp;ldquo;sum of squares for error.&amp;rdquo; It is defined as:&lt;/p&gt;
&lt;p&gt;$$SSE=\sum (y_{i}-\widehat{y_{i}})^{2}$$&lt;/p&gt;
&lt;p&gt;As implied in the figure, the following relationship holds:&lt;/p&gt;
&lt;p&gt;$$SSTOT=SSR+SSE$$&lt;/p&gt;
&lt;p&gt;If a &amp;ldquo;large&amp;rdquo; proportion of variability in the data can be accounted for by the model then we have evidence that the model is useful. The statistic that quantifies the proportion of variability accounted for by the model is known as $R^{2}$, which we now define.&lt;/p&gt;
&lt;p&gt;$R^{2},$ read as &amp;ldquo;r squared&amp;rdquo; and also known as &lt;strong&gt;the coefficient of determination&lt;/strong&gt;, measures the proportion of total variability in the $Y$ data that is accounted for by the regression fit. The formula is&lt;/p&gt;
&lt;p&gt;$$R^{2}=\frac{SSR}{SSTOT}=1-\frac{SSE}{SSTOT}$$&lt;/p&gt;
&lt;p&gt;Here is a useful fact relating $R^{2}$ to correlation.&lt;/p&gt;
&lt;p&gt;In the simple linear regression model $Y=\beta_{0}+\beta_{1}x+\varepsilon ,$ $R^{2}$ is equal to the square of the correlation coefficient $r$. That is $R^{2}=r^{2}$.&lt;/p&gt;
&lt;p&gt;How much variability is accounted for by the regression model is only one indication of its usefulness. It is possible to have a fairly large $R^{2}$ in a model that is not practically useful. A $95%$ confidence interval for $\beta_{1}$, for instance, could be too wide to make any reasonable judgment on the relationship between a unit increase in $x$ on the mean of $Y$. Another problem is that a high $R^{2}$ is possible when the underlying assumptions of the regression procedure are violated. Fortunately, in this case, adjustments to the model or transformations of the data are usually possible in order to make the assumptions more reasonably met. In the next section, we give an overview of some common regression assumption violations.&lt;/p&gt;
&lt;h2 id=&#34;violations-of-regression-assumptions&#34;&gt;Violations of Regression Assumptions&lt;/h2&gt;
&lt;p&gt;The basic ways that a simple linear regression model can go wrong are 1). the true $E(Y|x)$ is not linear, but some other function; 2). The error terms $\varepsilon_{i}$ are not independent of one another or of $x;$ 3). The error terms $\varepsilon_{i}$ do not have the same variance $V(\varepsilon )=$ $\sigma ^{2}$ for all values of $x$ ; 4). Several of these assumptions are violated. Let&amp;rsquo;s expand upon points¬†$(2)$ and $(3)$.&lt;/p&gt;
&lt;p&gt;The error terms $\varepsilon_{i}$ arise due to the imperfect fit between the regression line $\beta_{0}+\beta_{1}x$ and the actual value $Y$. For each value of $Y_{i}$ and $x_{i},$ there is an associated $\varepsilon_{i}$ that &amp;ldquo;absorbs&amp;rdquo; the effects of all of the variables that we didn&amp;rsquo;t include in the model. One of the key assumptions is that the variance of the error term $V(\varepsilon )$ *does not change* as $x$ changes. That is, $V(\varepsilon_{i})=V(\varepsilon_{j})$ for any $i$ and $\ j$. We further assume that the correlation between $\varepsilon_{i}$ and $\varepsilon_{j}$ is exactly $0,$ that is, the error in predicting $Y_{i}$ using $x_{i}$ is unrelated to the error in predicting $Y_{j}$ using $x_{j}$, for any $i$ and $j$. If these assumptions about the $\varepsilon_{i}$ are not satisfied, inferential statements we make using the fitted model¬†(for example, calculating a confidence interval or test for $\beta_{1}$, or predicting a new observation of $Y)$ are not strictly valid, and may thus be highly misleading.&lt;/p&gt;
&lt;p&gt;The most common method by which we evaluate the assumptions of the regression model is by examining what are called &amp;ldquo;residuals.&amp;rdquo; First, we need to define what a residual is.&lt;/p&gt;
&lt;p&gt;A regression &lt;strong&gt;residual&lt;/strong&gt; $e_{i}$**¬†**is the difference between the actual value $y_{i}$ and the fitted value $\widehat{y_{i}}$. For a fitted regression model, the residual is&lt;/p&gt;
&lt;p&gt;$$e_{i}=y_{i}-\widehat{y_{i}}$$&lt;/p&gt;
&lt;p&gt;In the case of the simple linear regression model, plotting the residuals by the $x$ variable is a natural choice. A scatterplot of $(e_{i},x_{i})$ is called a **residual plot**. Examining residual plots can help identify problems with the model fit. How should the residual plot look if all of the assumptions are reasonably met? The short answer is: *look for the absence of a pattern.* Figure 
&lt;a href=&#34;#good_resid&#34;&gt;[good_resid]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;good_resid&amp;rdquo;} illustrates this idea. Notice that the residuals seem to scatter around $0$ with no apparent pattern. We would say this model is a reasonably good fit to the data.&lt;/p&gt;
&lt;p&gt;A violation of the assumption of constant standard deviation (known formally as &lt;strong&gt;heteroscedasticity)&lt;/strong&gt; is shown in Figure 
&lt;a href=&#34;#hetero&#34;&gt;[hetero]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;hetero&amp;rdquo;}. Notice that as the $x$ variable (&amp;ldquo;time&amp;rdquo; in this case) gets larger, the residuals begin to &amp;ldquo;fan out&amp;rdquo; in a megaphone pattern, indicating an increase in the variability. In practice, an analyst would employ a &lt;strong&gt;variable transformation&lt;/strong&gt; to the data to correct for this. Discussion of how this transformation is accomplished is beyond the scope of this handout &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;An example of a model in which the assumption that $E(Y|x)$ is linear is not met is shown next in Figure 
&lt;a href=&#34;#misspesified&#34;&gt;[misspesified]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;misspesified&amp;rdquo;}. Such a model is called a &lt;strong&gt;misspecified model,&lt;/strong&gt; and many times such models will result in error terms that are correlated. Moreover, misspecified models are problematic in their own right simply because any predictions we get from using the model will be wrong because the wrong relationship was postulated between $Y$ and $x$. In the example below, it can be shown that fitting a parabola $E(Y|x)=\beta_{0}+\beta_{1}x+\beta_{2}x^{2}$ rather than a line will lead to a residual plot that looks like Figure 
&lt;a href=&#34;#good_resid&#34;&gt;[good_resid]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;good_resid&amp;rdquo;} above.&lt;/p&gt;
&lt;p&gt;For many of the real-world data sets we will use, the assumptions of the simple model will be violated. For instance, Figure 
&lt;a href=&#34;#crude_resid&#34;&gt;[crude_resid]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;crude_resid&amp;rdquo;} indicates that the standard deviation of the residual terms increases as the price of oil increases. The pattern is that of the &amp;ldquo;megaphone&amp;rdquo; we saw earlier. It turns out that for this model, the fitted values $\widehat{y}%_{i}$ (i.e., the predictions of $y$ for each price per barrel of oil) don&amp;rsquo;t differ by much when the problem of non-constant variability is corrected¬†(this correction isn&amp;rsquo;t shown here and is beyond the scope of this handout.) Therefore, &lt;em&gt;we will proceed in the examples as if the model assumptions are reasonably met&lt;/em&gt;, although in &amp;ldquo;real life,&amp;rdquo; as you should always do in any situation, you will need to check assumptions.&lt;/p&gt;
&lt;p&gt;Once you have fit the model, there are two things you might want to do with it: make an inference about $E(Y|x)$ for a specific $x$ and predict a new value of $Y,$ call it $Y^{new}$. We will discuss these two issues in the next section.&lt;/p&gt;
&lt;h2 id=&#34;estimation-of-mean-of-y-and-prediction-of-new-y&#34;&gt;Estimation of Mean of Y and Prediction of New Y&lt;/h2&gt;
&lt;p&gt;It turns out that estimating the mean of $Y$ for a specific value of $x$ and predicting a new value of $Y$ for a specific value of $x$ are done the same way: plug the specific value of $x$, call it $x_{p}$, into the fitted model. ¬†What differs is constructing the interval. In general, the confidence interval for $E(Y|x_{p})$ will be narrower than the **prediction interval** for the value of $Y^{new}$. This fact makes sense because estimating a specific value of a distribution should logically be less precise than estimating its mean.&lt;/p&gt;
&lt;p&gt;Here are the two formulas&lt;/p&gt;
&lt;p&gt;A $(1-\alpha )100%$ &lt;strong&gt;confidence interval&lt;/strong&gt; for $E(Y|x_{p})$ for the simple linear regression model is&lt;/p&gt;
&lt;p&gt;$$\widehat{y}\pm t_{n-2,\alpha /2}\sqrt{\frac{SSE}{n-2}\left[ \frac{1}{n}+\frac{(x_{p}-\overline{x})^{2}}{(n-1)s_{x}^{2}}\right] }$$&lt;/p&gt;
&lt;p&gt;where $x_{p}$ is the specific value of $x$ for which we want to estimate the mean of $Y$.&lt;/p&gt;
&lt;p&gt;A $(1-\alpha )100%$ &lt;strong&gt;prediction interval&lt;/strong&gt; ¬†for a new observation $Y^{new}$ at a specific value $x_{p}$, is&lt;/p&gt;
&lt;p&gt;$$\widehat{y}\pm t_{n-2,\alpha /2}\sqrt{\frac{SSE}{n-2}\left[ 1+\frac{1}{n}+\frac{(x_{p}-\overline{x})^{2}}{(n-1)s_{x}^{2}}\right] }$$&lt;/p&gt;
&lt;p&gt;Notice that the only difference between the two is the addition of the $&amp;quot;1&amp;quot;$ in the prediction interval formula.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s work some examples to put all of the concepts of this handout into practice.&lt;/p&gt;
&lt;p&gt;For the following values of SSR, SSE, and SSTOT, calculate $R^{2}$.&lt;/p&gt;
&lt;p&gt;a). $SSE=159$, $SSTOT=309.3$&lt;/p&gt;
&lt;p&gt;b). $SSR=1682,$¬†$SSTOT=1982$&lt;/p&gt;
&lt;p&gt;c). $SSR=149,$¬†$SSE=592$&lt;/p&gt;
&lt;p&gt;The fitted regression model relating motor vehicles per $1000$ people to $CO_{2}$ output for a sample of countries using the data displayed in Figure 
&lt;a href=&#34;#c02&#34;&gt;[c02]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;c02&amp;rdquo;} is shown below$.$.The average number of motor vehicles per $1000$ people $\overline{x}=260.1$ with a standard deviation $s_{x}=236$.&lt;/p&gt;
&lt;p&gt;a). Show how $R^{2}$¬†was calculated.&lt;/p&gt;
&lt;p&gt;b). Interpret $R^{2}$¬†in the context of the problem.&lt;/p&gt;
&lt;p&gt;c). Compute a $99%$¬†confidence interval for the true mean CO$_{2}$ output in a country when it has $400$¬†motor vehicles per $1000$¬†people.&lt;/p&gt;
&lt;p&gt;d). Compute a $99%$¬†confidence interval for the true mean CO$_{2}$¬†output in a country when it has $400$¬†motor vehicles per $1000$¬†people.&lt;/p&gt;
&lt;p&gt;An economist is studying the nature of street vendors in Mexico. She has gathered the following data on $15$ vendors:¬†age, number of hours worked per day, and annual earnings. Using Excel, fit a simple linear regression model to predict annual earnings using hours worked per day as the $x$ variable.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Earnings&lt;/th&gt;
&lt;th&gt;Age&lt;/th&gt;
&lt;th&gt;Hours&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2841&lt;/td&gt;
&lt;td&gt;29&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1876&lt;/td&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2934&lt;/td&gt;
&lt;td&gt;62&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1552&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3065&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3670&lt;/td&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2005&lt;/td&gt;
&lt;td&gt;65&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3215&lt;/td&gt;
&lt;td&gt;44&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1930&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2010&lt;/td&gt;
&lt;td&gt;70&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3111&lt;/td&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2882&lt;/td&gt;
&lt;td&gt;29&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1683&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1817&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4066&lt;/td&gt;
&lt;td&gt;33&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;a). Show how $R^{2}$¬†was calculated.&lt;/p&gt;
&lt;p&gt;b). Interpret $R^{2}$¬†in the context of the problem&lt;/p&gt;
&lt;p&gt;c). Compute a $95%$¬†confidence interval for the true mean earnings of a vendor who works $\mathit{6}$¬†hours per day using the fact that the average work hours $\overline{x}=$¬†$8.7$ with a standard deviation $s_{x}=2.3$.¬†&lt;/p&gt;
&lt;p&gt;d).Compute a $95%$¬†prediction interval for the earnings of a street vendor who works $\mathit{6}$¬†hours per day.&lt;/p&gt;
&lt;p&gt;Use the output in Figure 
&lt;a href=&#34;#house_example&#34;&gt;[house_example]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;house_example&amp;rdquo;} ¬†and the fact that the average age of a house is $36.4$ years with a standard deviation of $30.3$ years to answer the following questions.&lt;/p&gt;
&lt;p&gt;a). Show how $R^{2}$¬†was calculated.&lt;/p&gt;
&lt;p&gt;b). Interpret $R^{2}$¬†in the context of the problem.&lt;/p&gt;
&lt;p&gt;c). Compute a $99%$¬†confidence interval for the average price of a $10$-year-old home.&lt;/p&gt;
&lt;p&gt;d).Compute a $99%$¬†prediction interval for the price of a $10$-year-old home.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;www.kickstarter.com/&#34;&gt;www.kickstarter.com/&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;www.fundable.com/&#34;&gt;www.fundable.com/&lt;/a&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://www.indeed.com/jobs?q=business+analytics&amp;amp;l&#34;&gt;http://www.indeed.com/jobs?q=business+analytics&amp;amp;l&lt;/a&gt; &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://www.wired.com/underwire/2013/08/qq_netflix-algorithm/?mbid=social10558894&#34;&gt;http://www.wired.com/underwire/2013/08/qq_netflix-algorithm/?mbid=social10558894&lt;/a&gt; &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Available here: 
&lt;a href=&#34;www.r-project.org&#34;&gt;www.r-project.org&lt;/a&gt; &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Multiple Linear Regression</title>
      <link>/courses/bana3363/18-multiple-linear-regression/</link>
      <pubDate>Tue, 28 Jul 2020 15:19:27 +0000</pubDate>
      <guid>/courses/bana3363/18-multiple-linear-regression/</guid>
      <description>&lt;h2 id=&#34;multiple-linear-regression&#34;&gt;Multiple Linear Regression&lt;/h2&gt;
&lt;p&gt;The point of regression analysis is to use the relationship between a random variable $Y$ and another variable $x$ to make better predictions about $Y$ than could be made by simply using the overall average $\overline{y}.$ In the &amp;ldquo;real world,&amp;rdquo; variables are related to one another, and thus a model that allows the distribution of one variable to change with another is one way to model this relationship. However, it would be rather silly to conclude that $Y$ is influenced only by a single variable $x.$ Rather, it is much more sensible that $Y$ is influenced by a collection of $x$ variables, say $\mathbf{x=(}x_{1},x_{2},\ldots ,x_{p})$ where $p&amp;gt;2.$&lt;/p&gt;
&lt;p&gt;For example, if $Y$ is a person&amp;rsquo;s cholesterol level, a person&amp;rsquo;s age would be a logical choice for an $x$ variable. We might call it $x_{1}.$ But cholesterol might also be related to gender $(x_{2})$, weight $(x_{3})$, blood pressure $(x_{4})$, and many other variables. Being able to model $Y$ as a function of all of these variables simultaneously would allow doctors and researchers to examine the effects of each of these variables on cholesterol level in the presence of the other variables, and make predictions that could help patients make decisions about their lifestyles.&lt;/p&gt;
&lt;p&gt;Fortunately, regression analysis extends naturally from one independent variable to many independent variables. The basic concept is essentially the same, except we add more $x$ variables to the mix:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We assume the probability distribution of $Y$ changes as the collection of $x$ variables $\mathbf{x=(}x_{1},x_{2},\ldots ,x_{p})$ changes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Specifically, we assume that the mean (or expected value) of ¬†$Y$, denoted as $E(Y)$, changes as $\mathbf{x}$ changes. To indicate this we write $E(Y|\mathbf{x})$ read as &amp;ldquo;the expected value of $Y$ given $\mathbf{x}$.&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When multiple $x$ variables are involved, however, we can define many types of functional relationships between $Y$ and $\mathbf{x.}$ The simplest one is known as the¬†&amp;ldquo;first-order multiple linear regression model,&amp;rdquo; which we now define:&lt;/p&gt;
&lt;p&gt;A first-order multiple linear regression model is specified by&lt;/p&gt;
&lt;p&gt;$$Y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\ldots +\beta_{p}x_{p}+\varepsilon$$&lt;/p&gt;
&lt;p&gt;where the $\beta$ terms are fixed (but unknown) parameters and $\varepsilon$ has a normal distribution with mean $0$ and standard deviation $\sigma$.&lt;/p&gt;
&lt;p&gt;The implication of the multiple linear regression model is that the mean of $% Y$ is $E(Y|\mathbf{x})=\widehat{Y}=$ $\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\ldots +\beta_{p}x_{p}$ because, by the rules of expected value¬†(see Handout $1$),&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} E(Y|\mathbf{x}) &amp;amp;=&amp;amp;E(\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\ldots +\beta_{p}x_{p}+\varepsilon ) \ &amp;amp;=&amp;amp;\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\ldots +\beta_{p}x_{p}+E(\varepsilon )\text{ [since }\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\ldots +\beta_{p}x_{p}\text{ is a constant]} \ &amp;amp;=&amp;amp;\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\ldots +\beta_{p}x_{p}\text{ [since we assume }E(\varepsilon )=0]\end{aligned}$$&lt;/p&gt;
&lt;p&gt;We can interpret the parameters in a similar manner as with the simple linear regression model, except that we must now be mindful of the other $x$ variables in the model. Here are the general interpretations.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\beta_{0}$ is the mean of $Y$ when all the $x$ variables are $0$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\beta_{1}$ is the increase in the mean of $Y$ for a one-unit increase in $x_{1}$, holding the other $x$ variables fixed$.$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In general, $\beta_{j}$ is the increase in the mean of $Y$ for a one-unit increase in $x_{j}$, holding the other $x$ variables fixed.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The parameters are estimated with &lt;strong&gt;ordinary least squares (OLS)&lt;/strong&gt; just as with the simple linear regression model. However, in the case of multiple $x$ variables, the formulas are not simple to write out by hand. In fact, the values that minimize $SSE=\sum (y_{i}-b_{0}-b_{1}x_{i1}-b_{2}x_{i2}-\ldots -b_{p}x_{ip})^{2}$ are found using matrix algebra, which is beyond the scope of this course. Suffice it to say that you will never have to find these values by hand. Multiple regression is nearly always carried out using specialized software such as Excel.&lt;/p&gt;
&lt;p&gt;With the simple linear regression model, the fitted function $\widehat{y}% =b_{0}+b_{1}x_{1}$ was a line. With a multiple linear regression model, the fitted function $\widehat{y}=b_{0}+b_{1}x_{1}+b_{2}x_{2}+\ldots +b_{p}x_{p}$ is a **plane** (flat surface) in $p+1$ dimensions. For instance if we were to fit a model with two $x$ variables, we would obtain the following function in Figure 
&lt;a href=&#34;#regplane&#34;&gt;[regplane]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;regplane&amp;rdquo;}, a plane in three dimensions. With more than two $x$ variables, we obtain a **hyperplane**, a figure that we can no longer easily visualize.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now discuss how we might use assess and use the multiple linear regression model.&lt;/p&gt;
&lt;h1 id=&#34;using-the-multiple-linear-regression-model&#34;&gt;Using the Multiple Linear Regression Model&lt;/h1&gt;
&lt;p&gt;After the model has been fit, the next logical question is whether the model is useful or not. One way of assessing this is to determine if there is an &lt;strong&gt;overall regression relationship&lt;/strong&gt; between $Y$ and the $x$ variables. We can examine the overall regression relationship by examining $R^{2}$ and by conducting the &lt;strong&gt;model F test&lt;/strong&gt;. Let&amp;rsquo;s discuss the overall test first.&lt;/p&gt;
&lt;h2 id=&#34;model-f-test&#34;&gt;Model F Test&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Specify the hypothesis. The hypothesis is very similar to the ANOVA hypothesis of Chapter 14.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\begin{aligned} H_{0} &amp;amp;:&amp;amp;\beta_{1}=\beta_{2}=\ldots =\beta_{p}=0 \ H_{1} &amp;amp;:&amp;amp;\text{ Not all }\beta ^{\prime }s\text{ equal 0 [or, at least one }% \beta \text{ is not equal to }0]\end{aligned}$$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Calculate the test statistic.&lt;/p&gt;
&lt;p&gt;The test uses an $F$ statistic&lt;/p&gt;
&lt;p&gt;$$f=\frac{SSR/p}{SSE/(n-(p+1))}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the $p$-value&lt;/p&gt;
&lt;p&gt;$$p=P(F_{p,n-(p+1)}&amp;gt;f)$$&lt;/p&gt;
&lt;p&gt;Alternatively, we can define the rejection region as follows:&lt;/p&gt;
&lt;p&gt;$${\text{ }f_{p,n-(p+1)}:f_{p,n-(p+1)}&amp;gt;f_{p,n-(p+1)},_{\alpha }}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make a conclusion&lt;/p&gt;
&lt;p&gt;If the $p$-value $\leq \alpha $, we reject $H_{0}.$ If the $p$-value $% &amp;gt;\alpha $, we fail to reject $H_{0}.$ Equivalently, if $f$ lies within the rejection region, we reject $H_{0}$. If $f$ lies outside the rejection region, we fail to reject $H_{0}.$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In general, to construct confidence intervals or tests for regression coefficients, we need the standard error and the appropriate value of $t$ from the Student&amp;rsquo;s t distribution. The test for an individual regression coefficient $\beta_{j}$ is as follows.&lt;/p&gt;
&lt;h2 id=&#34;test-and-confidence-interval-for-individual-beta_j&#34;&gt;Test and Confidence Interval for Individual $\beta_{j}$&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Specify the hypotheses.&lt;/p&gt;
&lt;p&gt;The parameter of interest is $\beta_{j}$, so the hypotheses are statements about its value.&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} \text{(a) }H_{0} &amp;amp;:&amp;amp;\beta_{j}\leq c\text{ vs. }H_{0}:\beta_{j}&amp;gt;c \ \text{(b) }H_{0} &amp;amp;:&amp;amp;\beta_{j}\geq c\text{ vs. }H_{0}:\beta_{j}&amp;lt;c \ \text{(c) }H_{0} &amp;amp;:&amp;amp;\beta_{j}=c\text{ vs. }H_{0}:\beta_{j}\neq c\end{aligned}$$&lt;/p&gt;
&lt;p&gt;In practice, $c$ is usually $0$, but it could be anything. In regression programs such as Excel, the hypothesis tested by default is&lt;/p&gt;
&lt;p&gt;$$H_{0}:\beta_{j}=0\text{ vs. }H_{0}:\beta_{j}\neq 0$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calculate the test statistic.&lt;/p&gt;
&lt;p&gt;The test statistic is a $t$ statistic:&lt;/p&gt;
&lt;p&gt;$$t_{0}=\frac{b_{j}-c}{s.e.(b_{j})}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the p-value&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} \text{(a) }p &amp;amp;=&amp;amp;P(t_{n-(p+1)}&amp;gt;t_{0}) \ \text{(b) }p &amp;amp;=&amp;amp;P(t_{n-(p+1)}&amp;lt;t_{0}) \ \text{(c) }p &amp;amp;=&amp;amp;2\min [P(t_{n-(p+1)}&amp;lt;t_{0}),P(t_{n-(p+1)}&amp;gt;t_{0})]\end{aligned}$$&lt;/p&gt;
&lt;p&gt;Alternatively, we can define the rejection region as follows:&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} \text{(a)}{\text{ }t_{n-(p+1)} &amp;amp;:&amp;amp;t_{n-(p+1)}&amp;gt;t_{n-(p+1),\alpha }} \ \text{(b)}{\text{ }t_{n-(p+1)} &amp;amp;:&amp;amp;t_{n-(p+1)}&amp;lt;-t_{n-(p+1),\alpha }} \ \text{(c)}{t_{n-(p+1)} &amp;amp;:&amp;amp;|t_{n-(p+1)}|&amp;gt;t_{n-(p+1),\alpha /2}}\end{aligned}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make a conclusion&lt;/p&gt;
&lt;p&gt;If the $p$-value $\leq \alpha $, we reject $H_{0}.$ If the $p$-value $% &amp;gt;\alpha $, we fail to reject $H_{0}.$ Equivalently, if $t_{0}$ lies within the rejection region, we reject $H_{0}$. If $t_{0}$ lies outside the rejection region, we fail to reject $H_{0}.$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;An important issue arises in multiple regression with respect to $R^{2}$ that we summarize in the following theorem.&lt;/p&gt;
&lt;p&gt;The confidence interval for an individual coefficient is given in the following definition.&lt;/p&gt;
&lt;p&gt;A $(1-\alpha )100%$ confidence interval for $\beta_{j}$ is given by&lt;/p&gt;
&lt;p&gt;$$b_{j}\pm t_{n-(p+1),\alpha /2}s.e.(b_{j})$$&lt;/p&gt;
&lt;p&gt;where $t_{n-(p+1),\alpha /2}$ is the value of Student&amp;rsquo;s $t$ distribution with $n-(p+1)$ degrees of freedom and $\alpha /2$ probability to the right, and $s.e.(b_{j})$ is the standard error of $b_{j}.$&lt;/p&gt;
&lt;p&gt;The interpretation of a confidence interval in the multiple regression context must mention the other variables being held constant.&lt;/p&gt;
&lt;p&gt;An important issue arises in multiple regression with respect to $R^{2}$ that we summarize in the following theorem.&lt;/p&gt;
&lt;p&gt;$R^{2}$ will never decrease with the addition of $x$ variables. In other words, it is possible to &amp;ldquo;force&amp;rdquo; $R^{2}$ to $1$ by adding additional $x$ variables, whether they are useful for predicting $Y$ or not.&lt;/p&gt;
&lt;p&gt;Therefore, what is needed is a measure of fit that penalizes us for adding additional variables that don&amp;rsquo;t really help in predicting $Y.$ Statisticians have defined many such measures, but the one most used in practice (and which is reported in Excel) is called &lt;strong&gt;adjusted&lt;/strong&gt; $R^{2}.$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Adjusted&lt;/strong&gt; $R^{2}$ (denoted as $R_{adj}^{2})$ is a penalizing measure of fit for a multiple regression model defined by&lt;/p&gt;
&lt;p&gt;$$R_{adj}^{2}=1-\left( \frac{n-1}{n-(p+1)}\right) (1-R^{2})$$&lt;/p&gt;
&lt;p&gt;Some facts about $R_{adj}^{2}$ that make it useful for assessing model fit are:¬†1). it is possible for $R_{adj}^{2}$ to decrease if the addition of an independent variable is not useful for predicting $Y$, and 2). it is possible for $R_{adj}^{2}$ to be negative. When fitting a multiple regression model, $R_{adj}^{2}$ is the preferred measure of fit over $R^{2}.$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s work an example or two using Excel.&lt;/p&gt;
&lt;p&gt;We can think of many factors that might influence the final sale of a house, such as age, square footage, # of bathrooms, # of bedrooms, amenities, etc. Let&amp;rsquo;s fit a model to¬†age, square footage, and # of bathrooms using the Ames housing data set from earlier handouts. ¬†Using the excel output in Figure 
&lt;a href=&#34;#reg_output&#34;&gt;[reg_output]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;reg_output&amp;rdquo;}, answer the following questions.&lt;/p&gt;
&lt;p&gt;a). What would be the estimated selling price of a house that is $10$¬†years old, and for which there are $2$¬†bathrooms and $% 1400$¬†square feet?&lt;/p&gt;
&lt;p&gt;b). How would you interpret the coefficients?&lt;/p&gt;
&lt;p&gt;c). How would you interpret the coefficient for Bath?&lt;/p&gt;
&lt;p&gt;d). Show how adjusted $R^{2}$¬†was calculated. Why should we look at this value rather than $R^{2}?$&lt;/p&gt;
&lt;p&gt;e). Conduct the overall $F$¬†test (specify hypotheses, report test statistic, report rejection region, report conclusion).&lt;/p&gt;
&lt;p&gt;f). Report and interpret the confidence interval for Bath.&lt;/p&gt;
&lt;p&gt;Data on $4,137$ college students was obtained and a first-order multiple linear regression model was fit in order to determine the effects of various factors on college GPA, measured on the usual four-point scale. The results are shown in the Excel output below in Figure 
&lt;a href=&#34;#col_gpa&#34;&gt;[col_gpa]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;col_gpa&amp;rdquo;}.&lt;/p&gt;
&lt;p&gt;a). Write down the estimated model.&lt;/p&gt;
&lt;p&gt;b). What would be the estimated GPA of a student with an SAT score of $1190$¬†who graduated $3^{rd\text{ \ }}$ in a class of $542$¬†students?&lt;/p&gt;
&lt;p&gt;c). How would you interpret the coefficients?&lt;/p&gt;
&lt;p&gt;d). Calculate and interpret a $95%$¬†confidence interval for $\beta_{3}$,¬†the coefficient for high school rank.&lt;/p&gt;
&lt;p&gt;e). Test the hypothesis that $\beta_{2}\neq 0$¬†using the four-step procedure.&lt;/p&gt;
&lt;p&gt;An economist is studying the nature of street vending in Mexico. She has gathered the following data for $15$ vendors: age, hours worked per day, and annual earnings. Fit a multiple linear regression model using the data below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Earnings&lt;/th&gt;
&lt;th&gt;Age&lt;/th&gt;
&lt;th&gt;Hours&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2841&lt;/td&gt;
&lt;td&gt;29&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1876&lt;/td&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2934&lt;/td&gt;
&lt;td&gt;62&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1552&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3065&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3670&lt;/td&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2005&lt;/td&gt;
&lt;td&gt;65&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3215&lt;/td&gt;
&lt;td&gt;44&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1930&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2010&lt;/td&gt;
&lt;td&gt;70&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3111&lt;/td&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2882&lt;/td&gt;
&lt;td&gt;29&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1683&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1817&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4066&lt;/td&gt;
&lt;td&gt;33&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;a). Write down the estimated model.&lt;/p&gt;
&lt;p&gt;b). What would be the estimated average earnings for a $19$ -year-old vendor working $5$¬†hours per day?&lt;/p&gt;
&lt;p&gt;c). How would you interpret the coefficients?&lt;/p&gt;
&lt;p&gt;d). Test the hypothesis that $\beta_{1}$,¬†the coefficient for age, is not equal to $0$¬†using the four-step procedure.&lt;/p&gt;
&lt;p&gt;e). Calculate and interpret a $99%$¬†confidence interval for $\beta_{2}$,¬†the coefficient for hours.&lt;/p&gt;
&lt;h2 id=&#34;multiple-linear-regression-1&#34;&gt;Multiple Linear Regression&lt;/h2&gt;
&lt;p&gt;The point of regression analysis is to use the relationship between a random variable $Y$ and another variable $x$ to make better predictions about $Y$ than could be made by simply using the overall average $\overline{y}.$ In the &amp;ldquo;real world,&amp;rdquo; variables are related to one another, and thus a model that allows the distribution of one variable to change with another is one way to model this relationship. However, it would be rather silly to conclude that $Y$ is influenced only by a single variable $x.$ Rather, it is much more sensible that $Y$ is influenced by a collection of $x$ variables, say $\mathbf{x=(}x_{1},x_{2},\ldots ,x_{p})$ where $p&amp;gt;2.$ For example, if $Y$ is a person&amp;rsquo;s cholesterol level, a person&amp;rsquo;s age would be a logical choice for an $x$ variable. We might call it $x_{1}.$ But cholesterol might also be related to gender $(x_{2})$, weight $(x_{3})$, blood pressure $(x_{4})$, and many other variables. Being able to model $Y$ as a function of all of these variables simultaneously would allow doctors and researchers to examine the effects of each of these variables on cholesterol level in the presence of the other variables, and make predictions that could help patients make decisions about their lifestyles.&lt;/p&gt;
&lt;p&gt;Fortunately, regression analysis extends naturally from one independent variable to many independent variables. The basic concept is essentially the same, except we add more $x$ variables to the mix:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We assume the probability distribution of $Y$ changes as the collection of $x$ variables $\mathbf{x=(}x_{1},x_{2},\ldots ,x_{p})$ changes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Specifically, we assume that the mean (or expected value) of ¬†$Y$, denoted as $E(Y)$, changes as $\mathbf{x}$ changes. To indicate this we write $E(Y|\mathbf{x})$ read as &amp;ldquo;the expected value of $Y$ given $\mathbf{x}.&amp;ldquo;$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When multiple $x$ variables are involved, however, we can define many types of functional relationships between $Y$ and $\mathbf{x.}$ The simplest one is known as the¬†&amp;ldquo;first-order multiple linear regression model,&amp;rdquo; which we now define:&lt;/p&gt;
&lt;p&gt;A first-order multiple linear regression model is specified by&lt;/p&gt;
&lt;p&gt;$$Y=\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\ldots +\beta_{p}x_{p}+\varepsilon$$&lt;/p&gt;
&lt;p&gt;where the $\beta$ terms are fixed (but unknown) parameters and $\varepsilon$ has a normal distribution with mean $0$ and standard deviation $\sigma$.&lt;/p&gt;
&lt;p&gt;The implication of the multiple linear regression model is that the mean of $% Y$ is $E(Y|\mathbf{x})=\widehat{Y}=$ $\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\ldots +\beta_{p}x_{p}$ because, by the rules of expected value¬†(see Handout $1$),&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} E(Y|\mathbf{x}) &amp;amp;=&amp;amp;E(\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\ldots +\beta_{p}x_{p}+\varepsilon ) \ &amp;amp;=&amp;amp;\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\ldots +\beta_{p}x_{p}+E(\varepsilon )\text{ [since }\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\ldots +\beta_{p}x_{p}\text{ is a constant]} \ &amp;amp;=&amp;amp;\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}+\ldots +\beta_{p}x_{p}\text{ [since we assume }E(\varepsilon )=0]\end{aligned}$$&lt;/p&gt;
&lt;p&gt;We can interpret the parameters in a similar manner as with the simple linear regression model, except that we must now be mindful of the other $x$ variables in the model. Here are the general interpretations.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$\beta_{0}$ is the mean of $Y$ when all the $x$ variables are $0$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\beta_{1}$ is the increase in the mean of $Y$ for a one-unit increase in $x_{1}$, holding the other $x$ variables fixed$.$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In general, $\beta_{j}$ is the increase in the mean of $Y$ for a one-unit increase in $x_{j}$, holding the other $x$ variables fixed.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The parameters are estimated with ordinary least squares (OLS) just as with the simple linear regression model. However, in the case of multiple $x$ variables, the formulas are not simple to write out by hand. In fact, the values that minimize $SSE=\sum (y_{i}-b_{0}-b_{1}x_{i1}-b_{2}x_{i2}-\ldots -b_{p}x_{ip})^{2}$ are found using matrix algebra, which is beyond the scope of this course. Suffice it to say that you will never have to find these values by hand. Multiple regression is nearly always carried out using specialized software such as Excel.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now discuss how we might use assess and use the multiple linear regression model.&lt;/p&gt;
&lt;h2 id=&#34;using-the-multiple-linear-regression-model-1&#34;&gt;Using the Multiple Linear Regression Model&lt;/h2&gt;
&lt;p&gt;After the model has been fit, the next logical question is whether the model is useful or not. One way of assessing this is to determine if there is an &lt;strong&gt;overall regression relationship&lt;/strong&gt; between $Y$ and the $x$ variables. We can examine the overall regression relationship by examining $R^{2}$ and by conducting the &lt;strong&gt;model F test&lt;/strong&gt;. Let&amp;rsquo;s discuss the overall test first.&lt;/p&gt;
&lt;h3 id=&#34;model-f-test-1&#34;&gt;Model F Test&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Specify the hypothesis. The hypothesis is very similar to the ANOVA hypothesis of Chapter 14.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\begin{aligned} H_{0} &amp;amp;:&amp;amp;\beta_{1}=\beta_{2}=\ldots =\beta_{p}=0 \ H_{1} &amp;amp;:&amp;amp;\text{ Not all }\beta ^{\prime }s\text{ equal 0 [or, at least one }% \beta \text{ is not equal to }0]\end{aligned}$$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Calculate the test statistic.&lt;/p&gt;
&lt;p&gt;The test uses an $F$ statistic&lt;/p&gt;
&lt;p&gt;$$f=\frac{SSR/p}{SSE/(n-(p+1))}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the $p$-value&lt;/p&gt;
&lt;p&gt;$$p=P(F_{p,n-(p+1)}&amp;gt;f)$$&lt;/p&gt;
&lt;p&gt;Alternatively, we can define the rejection region as follows:&lt;/p&gt;
&lt;p&gt;$${\text{ }f_{p,n-(p+1)}:f_{p,n-(p+1)}&amp;gt;f_{p,n-(p+1)},_{\alpha }}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make a conclusion&lt;/p&gt;
&lt;p&gt;If the $p$-value $\leq \alpha $, we reject $H_{0}.$ If the $p$-value $% &amp;gt;\alpha $, we fail to reject $H_{0}.$ Equivalently, if $f$ lies within the rejection region, we reject $H_{0}$. If $f$ lies outside the rejection region, we fail to reject $H_{0}.$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In general, to construct confidence intervals or tests for regression coefficients, we need the standard error and the appropriate value of $t$ from the Student&amp;rsquo;s t distribution. The test for an individual regression coefficient $\beta_{j}$ is as follows.&lt;/p&gt;
&lt;h3 id=&#34;test-and-confidence-interval-for-individual-beta_j-1&#34;&gt;Test and Confidence Interval for Individual $\beta_{j}$&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Specify the hypotheses.&lt;/p&gt;
&lt;p&gt;The parameter of interest is $\beta_{j}$, so the hypotheses are statements about its value.&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} \text{(a) }H_{0} &amp;amp;:&amp;amp;\beta_{j}\leq c\text{ vs. }H_{0}:\beta_{j}&amp;gt;c \ \text{(b) }H_{0} &amp;amp;:&amp;amp;\beta_{j}\geq c\text{ vs. }H_{0}:\beta_{j}&amp;lt;c \ \text{(c) }H_{0} &amp;amp;:&amp;amp;\beta_{j}=c\text{ vs. }H_{0}:\beta_{j}\neq c\end{aligned}$$&lt;/p&gt;
&lt;p&gt;In practice, $c$ is usually $0$, but it could be anything. In regression programs such as Excel, the hypothesis tested by default is&lt;/p&gt;
&lt;p&gt;$$H_{0}:\beta_{j}=0\text{ vs. }H_{0}:\beta_{j}\neq 0$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calculate the test statistic.&lt;/p&gt;
&lt;p&gt;The test statistic is a $t$ statistic:&lt;/p&gt;
&lt;p&gt;$$t_{0}=\frac{b_{j}-c}{s.e.(b_{j})}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Compute the p-value&lt;/p&gt;
&lt;p&gt;a) $p =P(t_{n-(p+1)}&amp;gt;t_{0})$
b) $p =P(t_{n-(p+1)}&amp;lt;t_{0})$
c) $p =2\min [P(t_{n-(p+1)}&amp;lt;t_{0}),P(t_{n-(p+1)}&amp;gt;t_{0})]$&lt;/p&gt;
&lt;p&gt;Alternatively, we can define the rejection region as follows:&lt;/p&gt;
&lt;p&gt;a) $t_{n-(p+1)} &amp;amp;:&amp;amp;t_{n-(p+1)}&amp;gt;t_{n-(p+1),\alpha }}$
b) $t_{n-(p+1)} &amp;amp;:&amp;amp;t_{n-(p+1)}&amp;lt;-t_{n-(p+1),\alpha }}$
c) $t_{n-(p+1)} &amp;amp;:&amp;amp;|t_{n-(p+1)}|&amp;gt;t_{n-(p+1),\alpha /2}}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make a conclusion&lt;/p&gt;
&lt;p&gt;If the $p$-value $\leq \alpha$, we reject $H_{0}.$ If the $p$-value $% &amp;gt;\alpha$, we fail to reject $H_{0}$. Equivalently, if $t_{0}$ lies within the rejection region, we reject $H_{0}$. If $t_{0}$ lies outside the rejection region, we fail to reject $H_{0}.$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;An important issue arises in multiple regression with respect to $R^{2}$ that we summarize in the following theorem.&lt;/p&gt;
&lt;p&gt;The confidence interval for an individual coefficient is given in the following definition.&lt;/p&gt;
&lt;p&gt;A $(1-\alpha )100%$ confidence interval for $\beta_{j}$ is given by&lt;/p&gt;
&lt;p&gt;$$b_{j}\pm t_{n-(p+1),\alpha /2}s.e.(b_{j})$$&lt;/p&gt;
&lt;p&gt;where $t_{n-(p+1),\alpha /2}$ is the value of Student&amp;rsquo;s $t$ distribution with $n-(p+1)$ degrees of freedom and $\alpha /2$ probability to the right, and $s.e.(b_{j})$ is the standard error of $b_{j}.$&lt;/p&gt;
&lt;p&gt;The interpretation of a confidence interval in the multiple regression context must mention the other variables being held constant.&lt;/p&gt;
&lt;p&gt;An important issue arises in multiple regression with respect to $R^{2}$ that we summarize in the following theorem.&lt;/p&gt;
&lt;p&gt;$R^{2}$ will never decrease with the addition of $x$ variables. In other words, it is possible to &amp;ldquo;force&amp;rdquo; $R^{2}$ to $1$ by adding additional $x$ variables, whether they are useful for predicting $Y$ or not.&lt;/p&gt;
&lt;p&gt;Therefore, what is needed is a measure of fit that penalizes us for adding additional variables that don&amp;rsquo;t really help in predicting $Y.$ Statisticians have defined many such measures, but the one most used in practice (and which is reported in Excel) is called &lt;strong&gt;adjusted&lt;/strong&gt; $R^{2}.$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Adjusted&lt;/strong&gt; $R^{2}$ (denoted as $R_{adj}^{2})$ is a penalizing measure of fit for a multiple regression model defined by&lt;/p&gt;
&lt;p&gt;$$R_{adj}^{2}=1-\left( \frac{n-1}{n-(p+1)}\right) (1-R^{2})$$&lt;/p&gt;
&lt;p&gt;Some facts about $R_{adj}^{2}$ that make it useful for assessing model fit are:¬†1). it is possible for $R_{adj}^{2}$ to decrease if the addition of an independent variable is not useful for predicting $Y$, and 2). it is possible for $R_{adj}^{2}$ to be negative. When fitting a multiple regression model, $R_{adj}^{2}$ is the preferred measure of fit over $R^{2}.$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s work an example or two using Excel.&lt;/p&gt;
&lt;p&gt;We can think of many factors that might influence the final sale of a house, such as age, square footage, # of bathrooms, # of bedrooms, amenities, etc. Let&amp;rsquo;s fit a model to¬†age, square footage, and # of bathrooms using the Ames housing data set from earlier handouts. ¬†Using the excel output in Figure 
&lt;a href=&#34;#reg_output&#34;&gt;[reg_output]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;reg_output&amp;rdquo;}, answer the following questions.&lt;/p&gt;
&lt;p&gt;a). What would be the estimated selling price of a house that is $10$¬†years old, and for which there are $2$¬†bathrooms and $1400$¬†square feet?&lt;/p&gt;
&lt;p&gt;b). How would you interpret the coefficients?&lt;/p&gt;
&lt;p&gt;c). How would you interpret the coefficient for Bath?&lt;/p&gt;
&lt;p&gt;d). Show how adjusted $R^{2}$¬†was calculated. Why should we look at this value rather than $R^{2}?$&lt;/p&gt;
&lt;p&gt;e). Conduct the overall $F$¬†test (specify hypotheses, report test statistic, report rejection region, report conclusion).&lt;/p&gt;
&lt;p&gt;f). Report and interpret the confidence interval for Bath.&lt;/p&gt;
&lt;p&gt;Data on $4,137$ college students was obtained and a first-order multiple linear regression model was fit in order to determine the effects of various factors on college GPA, measured on the usual four-point scale. The results are shown in the Excel output below in Figure 
&lt;a href=&#34;#col_gpa&#34;&gt;[col_gpa]&lt;/a&gt;{reference-type=&amp;quot;ref&amp;rdquo; reference=&amp;quot;col_gpa&amp;rdquo;}.&lt;/p&gt;
&lt;p&gt;a). Write down the estimated model.&lt;/p&gt;
&lt;p&gt;b). What would be the estimated GPA of a student with an SAT score of $1190$¬†who graduated third in a class of $542$¬†students?&lt;/p&gt;
&lt;p&gt;c). How would you interpret the coefficients?&lt;/p&gt;
&lt;p&gt;d). Calculate and interpret a $95%$¬†confidence interval for $\beta_{3}$,¬†the coefficient for high school rank.&lt;/p&gt;
&lt;p&gt;e). Test the hypothesis that $\beta_{2}\neq 0$¬†using the four-step procedure.&lt;/p&gt;
&lt;p&gt;An economist is studying the nature of street vending in Mexico. She has gathered the following data for $15$ vendors: age, hours worked per day, and annual earnings. Fit a multiple linear regression model using the data below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Earnings&lt;/th&gt;
&lt;th&gt;Age&lt;/th&gt;
&lt;th&gt;Hours&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2841&lt;/td&gt;
&lt;td&gt;29&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1876&lt;/td&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2934&lt;/td&gt;
&lt;td&gt;62&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1552&lt;/td&gt;
&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3065&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3670&lt;/td&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2005&lt;/td&gt;
&lt;td&gt;65&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3215&lt;/td&gt;
&lt;td&gt;44&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1930&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2010&lt;/td&gt;
&lt;td&gt;70&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3111&lt;/td&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2882&lt;/td&gt;
&lt;td&gt;29&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1683&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1817&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4066&lt;/td&gt;
&lt;td&gt;33&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;a). Write down the estimated model.&lt;/p&gt;
&lt;p&gt;b). What would be the estimated average earnings for a $19$ -year-old vendor working $5$¬†hours per day?&lt;/p&gt;
&lt;p&gt;c). How would you interpret the coefficients?&lt;/p&gt;
&lt;p&gt;d). Test the hypothesis that $\beta_{1}$,¬†the coefficient for age, is not equal to $0$¬†using the four-step procedure.&lt;/p&gt;
&lt;p&gt;e). Calculate and interpret a $99%$¬†confidence interval for $\beta_{2}$,¬†the coefficient for hours.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
